-Image November 27, 2025 Z-Image: An Efficient Image Generation
Foundation Model with Single-Stream Diffusion Transformer Z-Image Team,
Alibaba Group Abstract The landscape of high-performance image
generation models is currently dominated by pro- prietary systems, such
as Nano Banana Pro \[ 27\] and Seedream 4.0 \[ 64\]. Leading open-source
alternatives, including Qwen-Image \[ 76\], Hunyuan-Image-3.0 \[ 8\] and
FLUX.2 \[ 36\], are charac- terized by massive parameter counts (20B to
80B), making them impractical for inference, and fine-tuning on
consumer-grade hardware. To address this gap, we proposeZ-Image, an
efficient 6B-parameterfoundation generative model built upon a Scalable
Single-Stream Diffusion Trans- former (S3-DiT) architecture
thatchallenges the "scale-at-all-costs" paradigm. By systematically
optimizing the entire model lifecycle -- from a curated data
infrastructure to a streamlined training curriculum -- we complete the
full training workflow injust 314K H800 GPU hours (approx. \$630K). Our
few-step distillation scheme with reward post-training further yields
Z-Image-T urbo, offering bothsub-second inference latencyon an
enterprise-grade H800 GPU and compatibility with consumer-grade hardware
(\<16GB VRAM). Additionally, our omni-pre-training paradigm also enables
efficient training ofZ-Image-Edit, an editing model with impressive
instruction-following capabilities. Both qualitative and quantitative
experiments demonstrate that our model achieves performance comparable
to or surpassing that of leading competitors across various dimensions.
Most notably, Z-Image exhibitsexceptional capabilities in photorealistic
image generation and bilingual text rendering, delivering results that
rival top-tier commercial models, thereby demonstrating
thatstate-of-the-art results are achievable with significantly reduced
computational overhead. We publicly release our code, weights, and
online demo to foster the development of accessible, budget-friendly,
yet state-of-the-art generative models.
GitHubhttps://github.com/Tongyi-MAI/Z-Image ModelScope
Modelhttps://modelscope.cn/models/Tongyi-MAI/Z-Image-Turbo HuggingFace
Modelhttps://huggingface.co/Tongyi-MAI/Z-Image-Turbo ModelScope
DemoOnline Demo (ModelScope) HuggingFace DemoOnline Demo (HuggingFace)
Image GalleryOnline Gallery Offline Gallery

Figure 1\|Showcases of Z-Image-Turbo in photo-realistic image
generation. 2

Figure 2\|Showcases of Z-Image-Turbo in bilingual text-rendering. 3

Figure 3\|Showcases of Z-Image-Edit in various image-to-image tasks.
Each arrow represents an edit from the input to output images. 4

Under a sudden iPhone flash, she stands just where the surf and sand
meet, the midi-skirt and fluid blouse catching the gentle seaside
breeze, woven folds reflecting faint glimmers of distant city lights and
moon's shy glow. Her bare feet nestle softly in the warm, textured sand,
illuminated subtly by distant streetlamps that drift behind her, palms
and skyscrapers whispering quiet stories to the night. Damp tendrils of
wavy hair move gently, framing a serene, turned face softened by
reflective air, while the expansive darkness spills across two-thirds of
the frame in velvety repose. A delicate blur trails her skirt's hem,
contrasting with the crisp focus of her poised stance, embodying
timeless grace and effortless charm---flash-lit elegant beach snapshot,
captured on iPhone. Qwen-Image HunyuanImage3.0 Imagen 4 Ultra
Seedream4.0 Nano Banana Pro Z-Image-TurboInput prompt FLUX 2
\[dev\]Seedream3.0 Lumina-Image 2.0Figure 4\|Showcases of comparison
between Z-Image-Turbo and currently state-of-the-art models \[ 58,76, 8,
27, 36, 64, 21, 26\]. Z-Image-Turbo shows conspicuous photo-realistic
generation capacity. 5

Contents 1 Introduction 7 2 Data Infrastructure 8 2.1 Data Profiling
Engine . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . 9 2.2 Cross-modal Vector Engine . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . 10 2.3 World Knowledge
Topological Graph . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . 11 2.4 Active Curation Engine . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . 11 2.5 Efficient Construction
of Editing Pairs with Graphical Representation . . . . . . . . . . . .
12 3 Image Captioner 13 3.1 Detailed Caption with OCR Information . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . 14 3.2 Multi-Level
Caption with World Knowledge . . . . . . . . . . . . . . . . . . . . . .
. . . . 14 3.3 Difference Caption for Image Editing . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . 15 4 Model Training 15 4.1
Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . 15 4.2 Training Efficiency Optimization .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 4.3
Pre-training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . 17 4.4 Supervised Fine-Tuning (SFT) . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 4.5
Few-Step Distillation . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . 19 4.5.1 Decoupled DMD: Resolving Detail
and Color Degradation . . . . . . . . . . . . . . 20 4.5.2 DMDR:
Enhancing Capacity with RL and Regularization . . . . . . . . . . . . .
. . 21 4.5.3 Results and Analysis . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . 21 4.6 Reinforcement Learning with
Human Feedback (RLHF) . . . . . . . . . . . . . . . . . . . . 21 4.6.1
Reward Annotation and Training . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . 21 4.6.2 Stage 1: Offline Alignment with DPO on
Objective Dimensions . . . . . . . . . . . 21 4.6.3 Stage 2: Online
Refinement with GRPO . . . . . . . . . . . . . . . . . . . . . . . . .
22 4.7 Continued Training for Image Editing . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . 23 4.8 Prompt Enhancer with Reasoning
Chain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 5
Performance Evaluation 24 5.1 Elo-based Human Preference Evaluation . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . 24 5.2
Quantitative Evaluation . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . 25 5.2.1 Text-to-Image Generation . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 5.2.2
Instruction-based Image Editing . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . 28 5.3 Qualitative Evaluation . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . 29 5.3.1 Superior
Photorealistic Generation . . . . . . . . . . . . . . . . . . . . . . .
. . . . . 30 5.3.2 Outstanding Bilingual Text Rendering . . . . . . . .
. . . . . . . . . . . . . . . . . . 30 5.3.3 Instruction-following
Editing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
5.3.4 Enhanced Reasoning Capacity and World Knowledge through Prompt
Enhancer . 30 5.3.5 Emerging Multi-lingual and Multi-cultural
Understanding Capacity . . . . . . . . 31 6 Conclusion 46 7 Authors 46
7.1 Core Contributors . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . 46 7.2 Contributors . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
46 References 47 A Prompts Used in the Report 53 A.1 Figure 1 . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . 53 A.2 Figure 2 . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . 57 A.3 Figure 3 .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . 63 6

1.  Introduction The field of text-to-image (T2I) generation has
    witnessed remarkable advancements in recent years, evolving from
    generating rudimentary textures to producing photorealistic imagery
    with complex se- mantic adherence \[ 57,18,35,76,64,8,4\]. However,
    as the capabilities of these models have scaled, their development
    and accessibility face significant barriers. The current landscape
    is increasingly charac- terized by two divergent trends: on one
    side, state-of-the-art commercial closed-source models -- such as
    Nano Banana Pro \[ 27\] and Seedream 4.0 \[ 64\] -- remain enclosed
    within "black boxes", offering high performance but limited
    transparency or reproducibility. On the other side, open-source
    models, while fostering democratization, often resort to massive
    parameter scaling -- approaching tens of billions of parameters
    (e.g., Qwen-Image \[ 76\] (20B), FLUX.2 \[ 36\] (32B) and
    Hunyuan-Image-3.0 \[ 8\] (80B) -- imposing prohibitive computational
    costs for both training and inference. In this context, distilling
    synthetic data from proprietary models has emerged as an appealing
    shortcut to train high-performing models at lower cost, becoming a
    prevalent approach for resource-constrained academic research \[
    13,20\]. However, this strategy risks creating a closed feedback
    loop that may lead to error accumulation and data homogeniza- tion,
    potentially hindering the emergence of novel visual capabilities
    beyond those already present in the teacher models. In this work, we
    presentZ-Image, a powerful diffusion transformer model that
    challenges both the "scale-at-all-costs" paradigm and the reliance
    on synthetic data distillation. We demonstrate that neither approach
    is necessary to develop a top-tier image generation model. Instead,
    we introduce the first comprehensive end-to-end solution that
    systematically optimizes every stage of the model lifecycle -- from
    data curation and architecture design to training strategies and
    inference acceleration -- enabling efficient, low-cost development
    onpurely real-world data without distilling results from other
    models. Most notably, this methodological efficiency allows us to
    complete the entire training workflow with remarkably low
    computational overhead. As detailed in Table 1, the complete
    training pipeline for Z-Image requires only314K H800 GPU hours,
    translating to approximately\$628Kat current market rates (about \$2
    per GPU hour \[ 38\]). In a landscape where leading models often
    demand orders of magnitude more resources, this modest investment
    demonstrates that principled design can effectively rival
    brute-force scaling. Table 1\|Training costs of Z-Image, assuming
    the rental price of H800 is about \$2 per GPU hour. The rental price
    refers from \[38\]. Training Costs Low-res. Pre-Training
    Omni-Pre-Training Post-Training Total in H800 GPU Hours 147.5K
    142.5K 24K 314K in USD \$295K \$285K \$48K \$628K This breakthrough
    in cost-efficiency is underpinned by a systematic methodology built
    on four pillars: â€¢Efficient Data Infrastructure:In
    resource-constrained scenarios, an efficient data infrastructure is
    pivotal; it serves to maximize the rate of knowledge acquisition per
    unit of time -- thereby accelerating training efficiency -- while
    simultaneously establishing the upper bound of model capabilities.
    To achieve this, we introduce a comprehensive Data Infrastructure
    composed of four synergistic modules: aData Profiling Enginefor
    multi-dimensional feature extraction, aCross-modal Vector Enginefor
    semantic deduplication and targeted retrieval, aWorld Knowledge
    Topological Graph for structured concept organization, and anActive
    Curation Enginefor closed-loop refinement. By granularly profiling
    data attributes and orchestrating the training distribution, we
    ensure that the "right data" is aligned with the "right stage" of
    model development. This infrastructure maximizes the utility of
    real-world data streams, effectively eliminating computational waste
    arising from redundant or low-quality samples. â€¢Efficient
    Architecture:Inspired by the remarkable scalability of decoder-only
    architectures in large language models \[ 6\], we propose aScalable
    Single-Stream Multi-Modal Diffusion Transformer (S3-DiT). Unlike
    dual-stream architectures that process text and image modalities in
    isolation, our design facilitates dense cross-modal interaction at
    every layer. This high parameter efficiency enables Z-Image to
    achieve superior performance within a compact 6B parameter size,
    significantly 7

lowering the hardware requirements for both training and deployment. The
compact model size is also made possible in part by our use of a prompt
enhancer (PE) to augment the model's complex world knowledge
comprehension and prompt understanding capabilities, further mitigating
the limitations of the modest parameter count. Furthermore, this
early-fusion transformer architecture ensures superior versatility by
treating tokens from different modalities uniformly -- including text
tokens, image VAE tokens, and image semantic tokens -- enabling seamless
handling of diverse tasks such as text-to-image generation and
image-to-image editing within a unified framework. â€¢Efficient Training
Strategy:We design a progressive training curriculum composed of three
strate- gic phases: (1)Low-resolution Pre-training, which bootstraps the
model to acquire foundational visual-semantic alignment and synthesis
knowledge at a fixed 2562resolution. (2)Omni-pre- training, a unified
multi-task stage that consolidates arbitrary-resolution generation,
text-to-image synthesis, and image-to-image manipulation. By amortizing
the heavy pre-training budget across these diverse capabilities, we
eliminate the need for separate, resource-intensive stages. (3)PE- aware
Supervised Fine-tuning, a joint optimization paradigm where Z-Image is
fine-tuned using PE-enhanced captions. This ensures seamless synergy
between the Prompt Enhancement module and the diffusion backbone without
incurring additional LLM training costs, thereby maximizing the overall
development efficiency of the Z-Image system. â€¢Efficient Inference:We
presentZ-Image-T urbo, which delivers exceptional aesthetic alignment
and high-fidelity visual quality in only 8 Number of Function
Evaluations (NFEs). This performance is unlocked by the synergy of two
key innovations:Decoupled DMD\[ 46\], which explicitly disentangles the
quality-enhancing and training-stabilizing roles of the distillation
process, andDMDR\[ 32\], which integrates Reinforcement Learning by
employing the distribution matching term as an intrinsic regularizer.
Together, these techniques enable highly efficient generation without
the typical trade-off between speed and quality. Building upon this
robust foundation and efficient workflow, we have successfully derived
two spe- cialized variants that address distinct application needs.
First, our few-shot distillation scheme with reinforcement learning
yieldsZ-Image-T urbo, an accelerated model that achieves exceptional
aesthetic alignment in just 8 NFEs. It offerssub-second
inference1latency on enterprise GPUsandfits within the memory
constraints of consumer-grade hardware (\<16GB VRAM). Second, leveraging
the multi-task nature of our omni-pre-training, we
introduceZ-Image-Edit, a model specialized for precise instruction-
following image editing. Extensive qualitative and quantitative
experiments demonstrate the superiority of the Z-Image family. As
illustrated in Figure 1 and Figure 2, Z-Image delivers strong
capabilities of photorealistic generation and exceptional bilingual
(Chinese/English) text rendering, matching the visual fidelity of much
larger models. Figure 3 showcases the capabilities of Z-Image-Edit,
highlighting its precise adherence to editing instructions. Furthermore,
qualitative comparisons in Figure 4 and Section 5.3 reveal that our
model rivals top-tier commercial systems, proving thatstate-of-the-art
results are achievable with significantly reduced computational
overhead. We publicly release our code, weights, and online demo to
foster the development of accessible, budget-friendly generative models.
2. Data Infrastructure While the remarkable capabilities of
state-of-the-art text-to-image models are underpinned by large-scale
training data, achieving optimal performance under constrained
computational resources necessitates a paradigm shift from data quantity
to data efficiency. Simply scaling the dataset size often leads to
diminishing returns; instead, an efficient training pipeline requires a
data infrastructure that maximizes the information gain per computing
unit. To this end, an ideal data system must be strictly curated to
beconceptually broadyetnon-redundant, exhibitrobust multilingual
text-image alignment, and crucially, bestructured for dynamic curriculum
learning, ensuring that the data composition evolves to match the
model's training stages. To realize this, we have designed and
implemented an integrated Efficient Data Infrastructure. Far from a
static repository, this system operates as a dynamic engine architected
to maximize the rate of knowledge acquisition within a fixed training
budget. As the cornerstone of our pipeline, this infrastructure is
composed of four core, synergistic modules: 1FlashAttention-3 \[65\]
andtorch.compile\[1\] is necessary for achieving sub-second inference
latency. 8

1.Data Profiling Engine:This module serves as the quantitative
foundation for our data strategy. It extracts and computes a rich set of
multi-dimensional features from raw data, spanning low- level physical
attributes (e.g.image metadata, clarity metrics) to high-level semantic
properties (e.g., anomaly detection, textual description). These
computed profiles are not merely for basic filtering; they are the
essential signals used to quantify data complexity and quality, enabling
the programmatic construction of curricula for our dynamic learning
stages. 2.Cross-modal Vector Engine:Built on billions of embeddings,
this module is the engine for ensuring efficiency and diversity. It
directly supports our goal of anon-redundantdataset through large-scale
semantic deduplication. Furthermore, its cross-modal search capabilities
are critical for diagnosing and remediating model failures. This allows
us to pinpoint and prune data responsible for specific failure cases and
strategically sample to fill conceptual gaps. 3.World Knowledge
Topological Graph:This structured knowledge graph provides the semantic
backbone for the entire infrastructure. It directly underpins our goal
ofconceptual breadthby organizing knowledge hierarchically. Crucially,
this topology functions as a semantic compass for data curation. It
allows us to identify and fill conceptual voids in our dataset by
traversing the graph to find underrepresented entities. Furthermore, it
provides the structured framework needed to precisely rebalance the data
distribution across different concepts during training, ensuring a more
efficient and comprehensive learning process. 4.Active Curation
Engine:This module operationalizes our infrastructure into a truly
dynamic, self- improving system. It serves two primary, synergistic
functions. First, it acts as a frontier exploration engine, employing
automatic sampling to identify concepts on which the model performs
poorly or lacks knowledge ("hard cases"). Second, it drives a
closed-loop data annotation pipeline. This ensures that every iteration
not only expandsconceptual breadthof the dataset with high-value
knowledge but also continuously refines the data quality, maximizing the
learning efficiency of the entire training process. Collectively, these
components forge a robust data infrastructure that not only fuels the
training of text- to-image models but also establishes a versatile
infrastructure for broader multimodal model training. Leveraging this
system, we successfully facilitate the training of various critical
components, including captioners, reward models, and our image editing
model (i.e., Z-Image-Edit). In particular, we construct a dedicated data
pipeline specifically for Z-Image-Edit upon this infrastructure, the
details of which are elaborated in Section 2.5. 2.1. Data Profiling
Engine The Data Profiling Engine is engineered to systematically process
a massive, uncurated data pool, comprising large-scale internal
copyrighted collections. It computes a comprehensive suite of multi-
dimensional features for each image-text pair, enabling principled data
curation. Recognizing that different data sources exhibit unique biases,
our engine supports source-specific heuristics and sampling strategies
to ensure a balanced and high-quality training corpus. The profiling
process is structured across several key dimensions: Image Metadata.We
begin by caching fundamental properties for each image. This includes
elementary metadata like resolution (width and height) and file size,
which facilitate efficient filtering based on resolution and aspect
ratio. Simultaneously, we compute a perceptual hash (pHash) from the
image's byte stream. This hash acts as a compact visual fingerprint,
enabling rapid and effective low-level deduplication to remove identical
or near-identical images. Together, these pre-computed attributes form
the first layer of data selection. Technical Quality Assessment.The
technical quality of an image is a critical determinant of model
performance. Our engine employs a multi-faceted approach to quantify and
filter out low-quality assets: â€¢Compression Artifacts:To identify
over-compressed images, we calculate the ratio of the ideal uncompressed
file size (derived from resolution and bit depth) to the actual file
size. A low ratio indicates potential quality degradation due to
excessive compression. â€¢Visual Degradations:We utilize an in-house
trained quality assessment model to score images 9

on a range of degradation factors, including color cast, blurriness,
perceptible watermarks, and excessive noise. â€¢Information Entropy:To
maximize the density of meaningful content seen during training, we
filter out low-entropy images. This is achieved through two
complementary methods: (1) analyzing the variance of border pixels to
detect images with large, uniform-color backgrounds or frames, and (2)
performing a transient JPEG re-encoding and using the resulting
bytes-per-pixel (BPP) as a proxy for image complexity. Semantic and
Aesthetic Content.Beyond technical quality, we profile the high-level
semantic and aesthetic properties of images: â€¢Aesthetic Quality:We
leverage an aesthetics scoring model, trained on labels from
professional annotators, to quantify the visual appeal of each image.
â€¢AIGC Content Detection:Following the findings of Imagen 3 \[ 3\], we
trained a dedicated classifier to detect and filter out AI-generated
content. This step is crucial for preventing degradation in the model's
output quality and physical realism. â€¢High-Level Semantic Tagging:We
have trained a specialized Vision-Language Model (VLM) to generate rich
semantic tags. These tags include general object categories,
human-centric attributes (e.g.number of people), and culturally specific
concepts, with a particular focus on elements relevant to Chinese
culture. The same model also performs safety assessment by assigning
Not-Safe-for-Work (NSFW) scores, allowing for the unified filtering of
both semantically irrelevant and inappropriate content. Cross-Modal
Consistency and Captioning.The alignment between an image and its
textual description is paramount. â€¢Text-Image Correlation:We use CN-CLIP
\[ 86\] to compute the alignment score between an image and its
associated alt caption. Pairs with low correlation scores are discarded
to ensure the relevance of textual supervision. â€¢Multi-Level
Captioning:For all images selected for pre-training, we generate a
structured set of captions, including concise tags, short phrases, and
detailed long-form descriptions. Notably, diverging from prior works \[
21,64,76\] that use separate modules for Optical Character Recognition
(OCR) and watermark detection, our approach leverages the powerful
inherent capabilities of our VLM. We explicitly prompt the VLM to
describe any visible text or watermarks within the image, seamlessly
integrating this information into the final caption. This unified
strategy not only streamlines the data processing pipeline but also
enriches the textual descriptions with critical visual details, as
further elaborated in Section 3. 2.2. Cross-modal Vector Engine We
enhance the de-duplication method proposed in Stable Diffusion 3 \[
18\], reformulating it as a scalable, graph-based community detection
task. Addressing the severe scalability bottleneck of the original
ð‘Ÿð‘Žð‘›ð‘”ð‘’_ð‘ ð‘’ð‘Žð‘Ÿð‘â„Ž function, we substitute it with a highly efficient
k-nearest neighbor (k-NN) ð‘ ð‘’ð‘Žð‘Ÿð‘â„Ž function. We construct a proximity
graph from the k-NN distances and subsequently apply the community
detection algorithm \[ 68\]. This methodology closely approximates the
original algorithm's output for a sufficiently large k while drastically
reducing time complexity. Our fully GPU-accelerated \[ 60\] pipeline
achieves a processing rate of approximately 8 hours per 1 billion items
on 8 H800s, encompassing index construction and 100-NN querying. This
approach not only ensures a non-redundant dataset by identifying dense
clusters for effective de-duplication but also extracts semantic
structures via modularity levels, facilitating fine-grained data
balancing. Furthermore, we constructed an efficient retrieval pipeline
leveraging multimodal features \[ 86\] combined with a state-of-the-art
index algorithm \[ 54\]. This system's cross-modal search capabilities
are critical for both data curation and active model remediation. Beyond
identifying distributional voids for strategically sampling to fill
conceptual gaps -- thereby enabling targeted augmentation for a balanced
pre-training distribution -- this engine is instrumental in diagnosing
model failures. By querying the 10

system with failure cases (e.g., problematic generated images or text
prompts), we can pinpoint and prune the underlying data clusters
responsible for the erroneous behavior. This iterative refinement
process, targeting both data gaps and model failures, ensures dataset
robustness and is pivotal for sourcing high-quality candidates for
complex downstream tasks. 2.3. World Knowledge Topological Graph The
construction of our knowledge graph follows a three-stage process.
Initially, we build a compre- hensive but redundant knowledge graph from
all Wikipedia entities and their hyperlink structures. To refine this
graph, we employ a two-pronged pruning strategy: first, centrality-based
filtering removes nodes with exceptionally low PageRank \[ 56\] scores,
which represent isolated or seldom-referenced concepts; second, visual
generatability filtering uses a VLM to discard abstract or ambiguous
concepts that cannot be coherently visualized. Subsequently, to address
the limited conceptual coverage of the pruned graph, we augment it by
leveraging a large-scale internal dataset of captioned images. We
extract tags and corresponding text embeddings from all available
captions. Inspired by \[ 71\], we then perform an automatic hierarchical
strategy on these embeddings. Each parent node is named by using a VLM
to summarize its child nodes. This not only supplements the graph with
new concept nodes but also organizes them into a structured taxonomic
tree, significantly enhancing the structural integrity of the graph. In
the final stage, we perform weight assignment and dynamic expansion to
align the graph with practical applications. This involves manually
curating and up-weighting high-frequency concepts from user prompts, and
proactively integrating novel, trending concepts not yet present in our
data pool to maintain the relevance and timeliness of the graph. In
application, this graph underpins our semantic-level balanced sampling
strategy. We map the tags within each training caption to their
corresponding nodes in the knowledge graph. By considering both the BM25
\[ 62\] score of a tag and its hierarchical relationships (i.e.,
parent-child links) within the graph, we compute a semantic-level
sampling weight for each data point. This weight then guides our data
engine to perform principled, staged sampling from the data pool,
enabling fine-grained control over the training data distribution.
Z-ImageDiagnosisContinualPretrainingAugmented Curated Data
æ¾é¼ é³œé±¼Deduplication Rule-basedFiltering Image Embedding Text
Embedding...Retrieval ...Long-tail Conceptæ¾é¼ é³œé±¼Uncurated Data Figure
5\|Overview of the Active Curation Engine. The pipeline refines
uncurated data through cross- modal embedding, deduplication, and
rule-based filtering to construct a high-quality augmented dataset. A
feedback mechanism leverages the Z-Image model to diagnose long-tail
distribution deficiencies, dynamically guiding cross-modal retrieval to
reinforce the data collection process. The "Squirrel Fish" (æ¾é¼ é³œé±¼ )
case illustrates a classic long-tail challenge: it is actually the name
of a Chinese cuisine but the model lacks the specific concept for this
dish and may rely on compositional reasoning (combining "Squirrel" (
æ¾é¼  ) and "Fish" ( é³œé±¼ )), leading to erroneous generations absent of
domain-specific training data. 2.4. Active Curation Engine To
systematically elevate data quality and address long-tail distribution
challenges, we deploy a compre- hensive Active Curation Engine (Figure
5). This framework incorporates a filtering tool and Z-Image as 11

Media Pool Topology Graph ConceptBalanceQualityBalancePseudo-labeled
Data ä¸€å¼ ç²¾è‡´çš„è‡ªæ‹...åŠ¨æ¼«å¥³æ€§, å®¤å†…...an doodle painting...Score:
2Score: 7Score: 8Propose Scores/CaptionsReward / Captioner
ä¸€å¼ ç²¾è‡´çš„è‡ªæ‹...åŠ¨æ¼«å¥³æ€§, å®¤å†…...Score: 7Score: 8Human VerifierAI
VerifierRewardfail HumanCorrect ä¸€å¼ å¹³å¹³æ— å¥‡çš„è‡ªæ‹...åŠ¨æ¼«å¥³æ€§, å®¤å†…...an
doodle painting...Score: 2Score: 9Score: 4pass Figure 6\|Illustration of
the Human-in-the-Loop Active Learning Cycle. Data sampled from the media
pool undergoes concept and quality balancing before being assigned
pseudo-labels . A dual-verifier system (Human and AI) filters these
proposals: approved samples pass directly, while rejected cases trigger
a manual correction phase . This feedback loop iteratively refines the
annotations and updates the topology graph to ensure high-precision
alignment. a diagnostic generative prior. The pipeline begins by
processing uncurated data through cross-modal embedding and
deduplication, followed by rule-based filtering to eliminate low-quality
samples. To support the continuous evolution of Z-Image, we establish a
human-in-the-loop active learning cycle (Figure 6) where the reward
model and captioner are progressively optimized. In this pipeline, we
first employ the topology graph (Section 2.3) and the initial reward
model to curate a balanced subset from the unlabeled media pool. The
current captioner and reward model then assign pseudo-labels to these
samples. A hybrid verification mechanism -- comprising both human and AI
verifiers -- verifies these proposals; rejected samples trigger a manual
correction phase by human experts to refine captions or scores. This
high-quality annotated data is then used to retrain the captioner and
reward model, thereby creating a virtuous cycle of our whole data
infrastructure enhancement. 2.5. Efficient Construction of Editing Pairs
with Graphical Representation Change the text inside the blue box
to'æˆ‘â¼ˆæ±Ÿçœ‹', and simultaneously change the text inside the brown box to
'æ²¡æ˜¯'.Modifications based on the original courtyard: Replace all the
orange circular stepping stones on the central path with5 dark
rectangular flagstones, giving them a wet, slightly reflective texture
like after rain. In the planting strip on the left, remove the original
small tree andreplace it with a low pine tree with branches leaning
towards the path; meanwhile, significantly increase the density of the
undergrowth by adding a large amount of fresh green ferns and moss
ground cover, paired with larger volcanic rocks, and use neat white
pebbles to form a border. In the right foreground, remove the three
square stone pillars with cloud patterns andreplace them with a
rectangular stone bench/stone trough...213456 2:Add a rainbowspanning
across the image over a waterfall background, and place a brown cowboy
hatwith stitching on the character's head. (a) Graphical Representation
(b) Paired Image from Videos (c) Rendering for Text EditingInput Image
Edited Images Figure 7\|Data construction for image editing using
different strategies: (a) arbitrarily permuting and combining different
edited versions of the same input image where the green arrow represents
the pair constructed by task-specfic expert models and the red arrow
denotes the pair generated by combination and permutation, (b)
collecting images with inherent relationship from video frames, and (c)
controllable text rendering system for text editing. Collecting editing
pairs that exhibits precise instruction following is challenging, owing
to the requirement of consistency maintaining and the diverse and
complex nature of editing operations. Through scalable 12

and controllable strategies as shown in Figure 7, we construct a
large-scale training corpus from diverse sources. Mixed Editing with
Expert Models.To guarantee broad task coverage, we begin by curating a
diverse taxonomy of editing tasks, and then leverage task-specific
expert models to synthesize high-quality training data for each
category. To improve the training efficiency, we construct mixed-editing
data, where multiple editing actions are integrated into one editing
pair. Thus, the model can enhance its ability in multiple editing tasks
from only a single composite pair, instead of relying on multiple ones.
Efficient Graphical Representation.For an input image, we synthesize
multiple edited versions corre- sponding to different editing tasks,
enabling us to further scale the training data at zero cost through
arbitrary pairwise combination \[ 42\] (e.g., 2 ð‘+1 2pairs are
constructed from one input image and its ð‘ edited versions). Apart from
scaling the quantity, this strategy 1) creates mixed-editing training
data by combining two edited versions to enhance the training
efficiency, and 2) yields inverse pairs to improve data quality,i.e.,
transforming a real, undistorted input image to an output image. Paired
Images from Videos.Constructing image editing pairs from predefined
tasks suffers from limited diversity. To overcome this issue, we
leverage naturally grouped images collected from a large scale video
frames in our media pool. These images, by sharing inherent relatedness
(e.g., common subjects, scenes, or styles), implicitly define complex
editing relationships among themselves. Building on this, we refine the
data by calculating the cosine similarity between image embeddings using
CN-CLIP \[ 86\], allowing us to filter for pairs with high semantic
relevance within each image group. The resulting dataset of video frame
pairs offers three key advantages: 1) high task diversity, 2) inherent
coupling of multiple edit types (e.g., simultaneous changes in human
pose and background), and 3) superior scalability. Rendering for Text
Editing.The acquisition of high-quality training data for text editing
presents substantial challenges, where natural images suffer from the
scarcity and imbalance of textual content, and text editing requires
paired samples with precise operation annotations. To address these
challenges, we develop a controllable text rendering system \[ 76\] that
grants us precise control over not only the textual content but also its
visual attributes, such as font, color, size, and position. This
approach enables us to systematically generate a large-scale dataset of
paired images, where the ground-truth editing instruction are known by
the rendering operation, thereby directly overcoming the aforementioned
data limitations. 3. Image Captioner Single Image Image PairZ-Captioner
Model World Knowledge OCR AugmentationTagging CaptionShort CaptionLong
CaptionText-to-Image Captions Step1: CaptionStep2: AnalysisStep3:
Instruction Image Editing Instructions Figure 8\|Pipeline for generating
text-to-image captions and image editing instructions. OCR results
(obtained through CoT) and world knowledge (from meta information) are
explicitly included into the captions. We build an all-in-one image
captioner, Z-Captioner, by incorporating multiple types of image
caption. 13

Tagging Caption: 1 girl, single subject focus, young woman, another
person in the background, East Asian, black hair, bangs, white T-shirt,
red apron, Hangzhou food festival, holding a plate, using chopsticks,
Longjing shrimp, West Lake, Hangzhou, China, LeifengPagoda, EUROSTAR,
magazine cover, outdoors, daytime, lakeside, water, tour boat, willow
trees, displayed food, osmanthus cake, tea leaves, glass jars, orange
triangular flags, white tents, photography, half-body shot, white
border, 'Food', 'Culture', 'Design Travel', 'ND', '2025', 'THE
MAGAZINE', 'F BIODI', 'FOOD', 'EUROSTAR', 'SPECIAL', 'TASTY HANGZHOU',
'ç¾Žâ»èŠ‚æ”»ç•¥', 'HANGZOU FOOD FESTIV AL 2025', 'éª‘â¾è·¯çº¿', '2025
æ­å·žç¾Žâ»èŠ‚'Short Caption: A promotional magazine cover themed around
Hangzhou Cuisine, featuring a young woman at a West Lake food stall
during the 2025 Hangzhou Food Festival, holding a plate and selling
Longjing shrimp.Long Caption: A "Food ND" magazine cover, a special
issue about "HANGZHOU FOOD FESTIV AL 2025."The scene takes place on the
shore of West Lake, one of Hangzhou's iconic landmarks, captured in a
vibrant, sunny daytime setting.Atthe center of the image is a young East
Asian woman with black hair, wearing a white T-shirt and a bright red
apron. She is holding a white plate of the famous local dish Longjing
Shrimp and lifting a piece with a pair of green chopsticks. The red
apron is printed with white text reading "2025 æ­å·žç¾Žâ»èŠ‚". To her right
is a food stall under a white tent, decorated with a string of orange
triangular flags. On the table are various local delicacies: two jars of
Longjing tea leaves, a wooden tray with square white cakes sprinkled
with osmanthus, several jars of pickled vegetables, and an orange
hanging tag with vertical black text reading "ä¸­â¾¬â¾¹". In the background
is the expansive view of West Lake, its surface shimmering in the
sunlight, with several traditional-style boats gliding across the water.
On the opposite shore rise lush green hills, and the famous
LeifengPagoda stands clearly on the right-side hilltop... source
imagetargetimageStep1 Caption: Source image: A high-quality close-up
portrait of a domestic cat. It has white fur on its chest and around its
mouth and nose, and striking green eyes that stare directly into the
camera with a neutral expression. The lighting is soft and even,
highlighting the details of its fur and whiskers...Target image: An
anthropomorphic cat is walking on a tropical sandy beach. The cat's head
is exactly the same as in the original image, but its body is now
dressed in a formal navy-blue pinstripe suit with a white shirt and a
black bow tie. It stands on its hind legs, holding a glass of
amber-colored liquid in its right paw...Step2 : AnalysisSubject
modification: The cat's body has been replaced with an upright,
anthropomorphic body dressed in a navy-blue pinstripe suit, a white
dress shirt, and a black bow tie.Element addition: A glass containing an
amber-colored liquid has been added to the cat's right paw.Scene change:
The entire background has been replaced...Step3 : InstructionPlace this
cat on a tropical beach. Replace its body with an upright,
anthropomorphic one dressed in a navy-blue pinstripe suit, a white
shirt, and a black bow tie. As it walks toward the camera, have it hold
a glass of wine in its right paw.Figure 9\|Single image caption and
difference caption examples. Left: for single image, we have captions of
different types and lengths, and notably, OCR results (all the texts
transcribed in their original languages) and world knowledge (explicitly
and correctly recognizing the famous beauty spot, West Lake, Hangzhou,
China, in this example) is included. Right: difference captions are
composed step-by-step. As revealed in previous works \[ 49\], different
captioning tasks can benefit each other as they share the same goal of
understanding and depicting images. Our model is designed not only to
describe visual elements, but also to leverage extensive world knowledge
to interpret the semantic context of the image. The integration of world
knowledge is particularly critical for the downstream text-to-image
synthesis task, as it enables the model to accurately render images
involving specific named entities. Figure 8 shows our pipeline for
generating text-to-image captions and image editing instructions. 3.1.
Detailed Caption with OCR Information First, we specially emphasize that
according to our experiments, including explicit OCR information in
image captions is inextricably bound with accurate text rendering in the
generated images. Therefore, we employ a way that shares the same spirit
as Chain-of-Thought (CoT) \[ 73\], by first explicitly recognizing all
optical characters in the image and then generating a caption based on
the OCR results. This effectively mitigates missing texts compared to
directly generating a caption that encapsulates everything, especially
for the cases where texts are very long/dense. In addition, we force the
OCR results to remain in their original languages without any
translation, avoiding them being falsely rendered in their translated
languages. 3.2. Multi-Level Caption with World Knowledge We design five
different types of image captions in total, including long, medium and
short captions, as well as tags and simulated user prompts. With the
data infrastructure in Section 2, we include world knowledge in all five
types of captions by performing image captioning conditioned on meta
information. This significantly alleviates hallucinations when our
captioner identifies and names specific entities such as public figures,
famous landmarks, or known events. To be specific, for relatively long
captions, we include very dense information of the images, in order that
the model could learn a mapping from the text to the image as accurate
as possible. These captions contain full OCR results as mentioned above,
along with subjects, objects, background, location information, et al.
14

We deliberately adopt a plain and objective linguistic style for our
descriptions, strictly confining them to factual information observable
in the image. By inhibiting subjective interpretations and imaginative
associations, our purpose is to enhance data efficiency for the image
generation task by eliminating non-essential information. On the other
hand, short captions, tags and simulated user prompts are designed for
the model to adapt to real user prompts (which are usually short and
unspecific) for better user experience. Notably, most of the simulated
user instructions are incomplete prompts. They differ from short
captions in that a short caption provides a relatively complete and
comprehensive description of the entire image. In contrast, a short
simulated prompt may mimic user behavior by focusing only on specific
parts of interest to the user, while making no mention of the rest of
the image. 3.3. Difference Caption for Image Editing Difference caption
is a concise editing instruction specifying the transformation from a
source to a target image. To generate this, we employ a three-step CoT
process that systematically breaks down the comparative task \[100\].
1.Step1: Detailed Captioning.We first generate a comprehensive,
OCR-inclusive caption for both the source and target images
respectively. This step provides a structured, detailed representation
of each image's content. 2.Step2: Difference Analysis.The model then
performs a comparative analysis, leveraging both the raw images and
their generated captions, to tell all discrepancies from visual and
textual perspectives. 3.Step3: Instruction Synthesis.Finally, the model
generates a concise editing instruction based on the identified
differences. This step -by-step process helps the model create clear and
useful instructions by moving from under- standing, to comparing, and
finally to generating the instructions. 4. Model Training This section
presents the complete training pipeline of Z-Image and Z-Image-Edit. We
begin by intro- ducing our Scalable Single-Stream Diffusion Transformer
(S3-DiT) architecture (Section 4.1) and training efficiency
optimizations (Section 4.2), followed by a multi-stage training process:
pre-training (Section 4.3), supervised fine-tuning (Section 4.4),
few-step distillation (Section 4.5), and reinforcement learning with
human feedback (Section 4.6). Finally, we describe the continued
training strategy for image editing capabilities (Section 4.7) and our
reasoning-enhanced prompt enhancer (Section 4.8). The overall training
pipeline is summarized in Figure 11. And in Figure 12, we present
intermediate generation results throughout Z-Image's training process to
demonstrate the benefits contributed by each stage. 4.1. Architecture
Efficiency and stability are the core objectives guiding the design of
Z-Image. To achieve this, we employ the lightweight Qwen3-4B \[ 85\] as
the text encoder, leveraging its bilingual proficiency to align complex
in- structions with visual content. For image tokenization, we utilize
the Flux VAE \[ 35\] selected for its proven reconstruction quality.
Exclusively for editing tasks, we augment the architecture with SigLIP 2
\[ 69\] to capture abstract visual semantics from reference images.
Inspired by the scaling success of decoder-only models, we adopt a
Single-Stream Multi-Modal Diffusion Transformer (MM-DiT) paradigm \[
18\]. In this setup, text, visual semantic tokens, and VAE image tokens
are concatenated at the sequence level to serve as a unified input
stream, maximizing parameter efficiency compared to dual-stream
approaches \[ 18,76\]. We employ 3D Unified RoPE \[ 58,78\] to model
this mixed sequence, wherein image tokens expand across spatial
dimensions and text tokens increment along the temporal dimension.
Crucially, for editing tasks, the reference image tokens and target
image tokens are assigned aligned spatial RoPE coordinates but are
separated by a unit interval offset in the temporal dimension.
Additionally, different time-conditioning values are applied to the
reference and target images to distinguish between clean and noisy
images. 15

Text ProcessorPredictedVelocityOutputProjection... Qwen3-4B
Embedding....Image ProcessorNoised V AE Embedding ...#
Single-StreamAttention Block TimestepConditionCCÃ—N.......#
Single-StreamFFN BlockC EmbedSemantic ProcessorSigLip-2 Embedding....
Image ProcessorVA E Embedding ... TimestepConditionCC.......C
Embed......ConditioningConcatenationMultiplicationAddition.CÃ—+ RMS
NormScaleRMS NormFeedForword# Single-StreamFFNBlockZero-init. GateC...
QueryKeyValueMulti-head Self-AttentionZero-init. Gate ScaleK-NormRMS
NormQ-Norm \# Single-StreamAttentionBlockRMS NormU-RoPE A charming white
kitten lounging on a striped sofa, heartwarming scene.t = \[0, 1\]Repose
the cat on the striped sofa: make it lie down with its head resting.t =
\[0, 1\] t = 1# Z-Image# Z-Image-EditC... C... C...Ã—Ã—+ +Figure
10\|Architecture overview of the Z-Image series. The S3-DiT consists of
single-stream FFN blocks and single-stream attention blocks. It
processes inputs from different modalities through lightweight
modality-specific processors, then concatenates them into a unified
input sequence. This modality- agnostic architecture maximizes
cross-modal parameter reuse to ensure parameter efficiency, while
providing flexible compatibility for varying input configurations in
both Z-Image and Z-Image-Edit. As illustrated in Figure 10, the specific
architecture of our S3-DiT (Scalable Single-Stream DiT) commences with
lightweight modality-specific processors, each composed of two
transformer blocks for initial modal alignment. Subsequently, tokens
enter the unified single-stream backbone. To ensure training stability,
we implement QK-Norm to regulate attention activations \[ 33,50,24,53\]
and Sandwich-Norm to constrain signal amplitudes at the input and output
of each attention / FFN blocks \[ 16,99\]. For conditional information
injection, input condition vectors are projected into scale and gate
parameters to modulate the normalized inputs and outputs of both
Attention and FFN layers. To reduce parameter overhead, this projection
is decomposed into a low-rank pair: a shared, layer-agnostic
down-projection layer followed by layer-specific up-projection layers.
Finally, RMSNorm \[ 91\] is uniformly utilized for all the
aforementioned normalization operations. Table 2\|Architecture
Configurations of S3-DiT. Configuration S3-DiT Total Parameters 6.15B
Number of Layers 30 Hidden Dimension 3840 Number of Attention Heads 32
FFN Intermediate Dimension 10240 (ð‘‘ð‘¡,ð‘‘â„Ž,ð‘‘ð‘¤) (32, 48, 48) 4.2. Training
Efficiency Optimization To optimize training efficiency, we implemented
a multi-faceted strategy targeting both computational and memory
overheads. 16

Low-ResolutionPre-trainingOmniPre-trainingSupervisedFine-tuningReinforcement
Learning with Human
FeedbackContinuedPre-trainingForEditingSupervisedFine-tuningForEditing#Z-Image#Z-Image-EditFew-stepDistillationFigure
11\|The training pipeline of Z-Image and Z-Image-Edit. The
low-resolution pre-training and omni-pre-training stages provide a
suitable initialization for image generation and editing tasks, after
which separate post-training processes yield the Z-Image and
Z-Image-Edit models respectively. For distributed training, we employed
a hybrid parallelization strategy. We applied standard Data Parallelism
(DP) to the VAE and Text Encoder, as they remain frozen and incur
minimal memory footprint. In contrast, for the large DiT model, where
optimizer states and gradients consume substantial memory, we utilized
FSDP2 \[ 96\] to effectively shard these overheads across GPUs.
Furthermore, we implemented gradient checkpointing across all DiT
layers. This technique trades an acceptable increase in computational
cost for significant memory savings, enabling larger batch sizes and
improved overall throughput. To further accelerate computation and
optimize memory usage, the DiT blocks were compiled usingtorch.compile,
a just-in-time (JIT) compiler \[1\]. In addition to system-level
optimizations, we addressed inefficiencies arising from mixed-resolution
training. Grouping samples with significantly different sequence lengths
into a single batch typically results in excessive padding, which
significantly impedes overall training speed. To mitigate this, we
designed a sequence length-aware batch construction strategy. Prior to
training, we estimate the sequence length of each sample based on the
resolution (height and width) recorded in the metadata. The sampler then
groups samples with similar sequence lengths into the same batch,
thereby minimizing computational waste. Crucially, we additionally
employ a dynamic batch sizing mechanism: smaller batch sizes are
assigned to long-sequence batches to prevent Out-Of-Memory (OOM) errors,
while larger batch sizes are used for short sequences to avoid resource
vacancy. This approach ensures maximal hardware utilization across
varying resolutions. 4.3. Pre-training Z-Image is trained using the flow
matching objective \[ 45,48\], where noised inputs are first constructed
through linear interpolation between Gaussian noise ð‘¥0and the original
image ð‘¥1,i.e.,ð‘¥ð‘¡=ð‘¡Â·ð‘¥ 1+(1âˆ’ð‘¡)Â·ð‘¥ 0. The model is then trained to predict
the velocity of the vector field that defines the path between them,
i.e.,ð‘£ð‘¡=ð‘¥1âˆ’ð‘¥0. The training objective can be formulated as:
L=Eð‘¡,ð‘¥0,ð‘¥1,ð‘¦\[âˆ¥ð‘¢(ð‘¥ð‘¡,ð‘¦,ð‘¡;ðœƒ)âˆ’(ð‘¥ 1âˆ’ð‘¥0)âˆ¥2\], (1) Whereðœƒas the learnable
parameters and ð‘¦as the conditional embedding. Following SD3 \[ 18\], we
employ the logit-normal noise sampler to concentrate the training
process on intermediate timesteps. Additionally, to account for the
variations in Signal-to-Noise Ratio (SNR) arising from our
multi-resolution training setup, we adopt the dynamic time shifting
strategy as used in Flux \[ 35\]. This ensures that the noise level is
appropriately scaled for different image resolutions, leading to more
effective training. The pre-training of Z-Image can be broadly divided
into two phases: low-resolution pre-training and omni-pre-training.
Low-resolution Pre-training. This phase consists of a single stage,
conducted exclusively at a 2562 resolution on the text-to-image
generation task. The primary emphasis of this stage is on efficient
cross- modal alignment and knowledge injection -- equipping the model
with the capability to generate a diverse range of concepts, styles, and
compositions, which is consistent with the initial stage of conventional
multi-stage training protocols. As shown in Figure 1, this phase
accounts for over half of our total pre- 17

training compute. This allocation is based on the rationale that the
majority of the model's foundational visual knowledge (e.g., Chinese
text rendering) is acquired during this low-resolution training stage.
Omni-pre-training. The "omni" here signifies three key aspects:
â€¢Arbitrary-Resolution Training: We design an arbitrary-resolution
training strategy in which the original image resolution is mapped to a
predefined training resolution range through a resolution- mapping
function. The model is then trained on images with diverse resolutions
and aspect ratios. This enables the learning of cross-scale visual
information, mitigates information loss caused by downsampling to a
fixed resolution, and improves overall data efficiency. â€¢Joint
Text-to-Image and Image-to-Image Training: We integrate the
image-to-image task into the pre-training framework. By leveraging the
substantial compute budget available during pre- training, we can
effectively exploit large-scale, naturally occurring, and weakly aligned
image pairs, as discussed in Section 2.5. Learning the relationships
between natural image pairs provides a strong initialization for
downstream tasks such as image editing. Importantly, we observe that
this joint pre-training scheme does not introduce any noticeable
performance degradation on the text-to-image task. â€¢Multi-level and
Bilingual Caption Training: It is widely recognized that high-quality
captions are crucial for training text-to-image models \[ 4\]. To ensure
both bilingual understanding and strong native prompt-following
capability, we employ Z-Captioner to generate bilingual, multi-level
synthetic captions (including long, medium, and short descriptions, as
well as tags and simulated user prompts). In addition, the original
textual metadata associated with each image is incorporated with a small
probability to further enhance the model's acquisition of world
knowledge. The use of captions at different granularities and from
diverse perspectives provides broad mode coverage, which is beneficial
for subsequent stages of training. Moreover, for image-to-image tasks,
we randomly sample either the target image's caption or the pairwise
difference caption with a certain probability, corresponding to
reference-guided image generation and multi-task image editing,
respectively. Working with our data infrastructure, the
omni-pre-training phase is conducted in multiple stages. Upon completion
of the final stage, the model becomes capable of generating images at
arbitrary resolutions up to the 1k-1.5k range and can condition its
output on both image and text inputs. This provides a suitable starting
point for the subsequent training of Z-Image and Z-Image-Edit. 4.4.
Supervised Fine-T uning (SFT) Distribution Narrowing via High-Quality
Alignment.While the omni-pre-training stage establishes broad world
understanding and mode coverage, the resulting distribution inevitably
exhibits high variance, reflecting the noisy nature of web-scale data.
Consequently, the primary objective of Supervised Fine-Tuning (SFT) is
not merely to correct local artifacts, but to narrow the generation
distribution towards a focused, high-fidelity sub-manifold\[ 67\]. This
phase aims for rapid convergence to a fixed distribution characterized
by consistent visual aesthetics and precise instruction following. To
achieve this, we transition from the noisy supervision of pre-training
to a curriculum dominated by highly curated images filtering by our data
infrastructure andsuper detailed, grounded captions. This rigorous
supervision acts as an anchor, forcing the model to discard low-quality
modes (e.g., unstable stylization or inconsistent rendering) and align
strictly with detailed textual descriptions, shifting the model from a
diversity-maximizing regime to a quality-maximizing operating point.
Concept Balancing with Tagged Resampling.A critical challenge in
narrowing the distribution is the risk of catastrophic forgetting,
particularly for long-tail concepts that are prone to being overshadowed
by dominant modes during convergence. To address this, we enforce
strictclass balancingthroughout the SFT phase. We employ a dynamic
resampling strategy guided by world knowledge topological graph in
Section 2. Specifically, we maintain a target prior over concepts and
utilize BM25-based retrieval to compute rarity scores for training
samples on the fly. Mini-batches are constructed by up-weighting
under-represented concepts -- such as rare entities or specific artistic
styles -- while down-weighting over-represented ones. This mechanism
ensures that while the model converges to the target high-quality 18

Figure 12\|Intermediate generation results throughout Z-Image-Turbo's
training process, echoing our analysis of each stage's contribution.
distribution, the marginal distribution over concepts remains uniform,
effectively preserving the semantic diversity of the pre-trained model.
Robustness via Model Merging.Despite balanced training, SFT on specific
high-quality datasets can introduce subtle biases or trade-offs between
capabilities (e.g., photorealism vs.Â stylistic flexibility). To achieve
a Pareto-optimal solution without complex inference routing, we
employModel Merging\[ 75,93\] as the final refinement step. We fine-tune
multiple SFT variants initialized from the same backbone, each slightly
biased towards different capability dimensions (e.g., strict instruction
following or aesthetic rendering). We then perform a linear
interpolation of their weights in the parameter space: ðœƒfinal=Ã ð‘–ð›¼ð‘–ðœƒð‘–.
This lightweight merging strategy effectively smooths the loss
landscape, neutralizing individual biases and resulting in a final model
that exhibits superior stability and robustness across diverse prompts
compared to any single SFT checkpoint. 4.5. Few-Step Distillation The
goal of the Few-Step Distillation stage is to reduce the inference time
of our foundational SFT model, achieving the efficiency demanded by
real-world applications and large-scale deployment. While our 6B
foundational model represents a significant leap in efficiency compared
to larger counterparts, the inference cost remains non-negligible. Due
to the inherent iterative nature of diffusion models, our standard SFT
model requires approximately 100 Number of Function Evaluations (NFEs)
to generate high-quality samples using Classifier-Free Guidance (CFG) \[
29\]. To bridge the gap between generation quality and interactive
latency, we implemented a few-step distillation strategy. Fundamentally,
the distillation process involves teaching a student model to mimic the
teacher's denois- ing dynamics across fewer timesteps along its sampling
trajectory. The core challenge lies in reducing the inherent uncertainty
of this trajectory, allowing the student to "collapse" its probabilistic
path into a deter- ministic and highly efficient inference process.
Therefore, the key to enable a stable few-step integrator is to
meticulously control the distillation process. We initially selected the
Distribution Matching Distillation (DMD) \[ 88,89\] paradigm due to its
promising performance in academic works. However, in practice, we
encountered persistent artifacts such as the loss of high-frequency
details and noticeable color shifts -- issues that have been
increasingly documented by the community. These observations signaled a
need for algorithmic refinement. Through a deeper exploration of the
distillation mechanism, we gained new 19

insights into the underlying dynamics of DMD, leading to two key
technical advancements:Decoupled DMD\[ 46\] andDMDR\[ 32\]. We refer
interested readers to the respective academic papers for full technical
details. Below, we introduce the practical application of these
techniques in buildingZ-Image-T urbo.
(b)DMD(c)D-DMD(d)D-DMD+DMDR(a)SFTé»„æ˜æ—¶åˆ†ï¼Œåœ¨ä¸€é—´å †æ»¡å¥‡çå¼‚å®çš„æ‚ä¹±å¤è‘£åº—é‡Œï¼Œä¸€ä½ç¥žæƒ…è‹¥æœ‰æ‰€æ€çš„å¹´è½»å¥³å­è½»è½»æŠšæ‘¸ç€ä¸€ä¸ªå¤è€ç¡•å¤§å¤©çƒé¥±ç»é£Žéœœçš„è¡¨é¢ã€‚Atdusk,inaclutteredantiqueshopfilledwithcuriosities,ayoungwomanwithathoughtfulexpressiongentlytouchestheweatheredsurfaceofalarge,ancientcelestialglobe.Prompt
ä¸€åªå¼ å¼€çš„æ‰‹çš„ç‰¹å†™ï¼Œæ‰‹å¿ƒå‘ä¸Šï¼ŒæŽŒçº¹å’ŒæŒ‡å…³èŠ‚çš„ç»†èŠ‚æ¸…æ™°å¯è§ã€‚A close-upof
anopen hand, palm up, withthedetails of thepalm
linesandknucklesclearlyvisible.
ä¸€å¹…è¾¾èŠ¬å¥‡é£Žæ ¼çš„æ–‡è‰ºå¤å…´æ—¶æœŸæ²¹ç”»ï¼Œæç»˜äº†åœ£æ¯çŽ›åˆ©äºšæ€€æŠ±åœ£å­è€¶ç¨£ï¼Œäººç‰©æž„æˆäº†ç¨³å®šçš„é‡‘å­—å¡”å½¢ï¼ŒèƒŒæ™¯æ˜¯æœ¦èƒ§çš„é£Žæ™¯ã€‚A
Renaissance oilpaintingin thestyle of Leonardo daVinci
depictstheVirginMary holding theinfantJesus.
Thefiguresformastablepyramidshapeagainstahazylandscapebackground. Figure
13\|Few-Step Distillation visualization results across different
distillation strategies: (a) the original SFT model; (b) Standard DMD;
(c)Decoupled DMD (D-DMD); and (d)D-DMD+DMDR(Z-Image-T urbo). The
proposed approach achieves real-time 8-step inference while attaining
superior perceived quality and aesthetic appeal. 4.5.1. Decoupled DMD:
Resolving Detail and Color Degradation Our investigation revealed a core
insight: the effectiveness of existing DMD methods is not a monolithic
phenomenon but the result of two independent, collaborating mechanisms:
â€¢CFG-Augmentation (CA):This acts as the primary engine driving the
distillation process, efficiently building up the few-step generation
capabilities of the student model. Despite its dominant role, this
factor has been largely overlooked in previous literature. â€¢Distribution
Matching (DM):This functions primarily as a powerful regularizer,
ensuring the stability of the training process and removing the emerging
artifacts. By recognizing and decoupling these two mechanisms, we were
able to study and optimize them in isolation. This motivation led to the
development ofDecoupled DMD, an improved distillation framework that
features a decoupled application of renoising schedules tailored
specifically for the CA and DM terms. In practice,Decoupled
DMDeffectively addresses the pain points of traditional DMD, ensuring
sharp detail preservation and color fidelity. Notably, the resulting
distilled model not only matches the original multi-step teacher but
even surpasses it in terms of photorealism and visual impact. 20

4.5.2. DMDR: Enhancing Capacity with RL and Regularization To further
push the performance boundaries of our few-step model, we incorporate
Reinforcement Learning (RL) into the few-step distillation process.
Applying RL to generative models typically faces the risk of "reward
hacking", where the model exploits the reward function to generate
high-scoring but visually nonsensical images. To mitigate this, external
regularization is usually required. Our insight from Decoupled DMD
provides a natural solution: since we established that the Distribution
Matching (DM) term functions as a high-quality regularizer, it can be
organically combined with RL objectives. This synthesis gives rise
toDMDR(Distribution Matching Distillation meets Reinforcement Learning)
\[ 32\]. In this framework, RL unlocks the student model's capacity to
align with human preferences, while the DM term acts as a robust
constraint, effectively preventing reward hacking. This synergy
allowsZ-Image-T urboto achieve superior aesthetic alignment and semantic
faithfulness while maintaining strict generative stability. 4.5.3.
Results and Analysis The efficacy of ourDecoupled DMDandDMDRdistillation
strategy is visualized in Figure 13. The original SFT model (a) sets a
high baseline but suffers from high latency. Standard DMD (b), while
fast, exhibits characteristic degradation: blurred textures and shifted
color tones. OurDecoupled DMD(c) successfully resolves these artifacts,
restoring sharp details and accurate colors. Finally,Z-Image-T urbo(d),
refined via a combination ofDecoupled DMDandDMDR, represents the optimal
convergence of speed and quality. It achieves 8-step inference that is
not only indistinguishable from the 100-step teacher but frequently
surpasses it in perceived quality and aesthetic appeal. In summary, our
Few-Step Distillation framework resolves the long-standing tension
between inference speed and visual fidelity. 4.6. Reinforcement Learning
with Human Feedback (RLHF) Following the previous stages, the model has
acquired strong foundational capabilities but may still exhibit
inconsistencies in aligning with nuanced human preferences. To bridge
this gap, we introduce a comprehensive post-training framework
leveraging Reinforcement Learning with Human Feedback (RLHF). This
framework hinges on a powerful, multi-dimensional reward model, which
provides targeted feedback for online optimization. Guided by these
feedback signals, our approach is structured into two sequential stages:
an initial offline alignment phase using Direct Preference Optimization
(DPO) \[ 59\], followed by an online refinement phase with Group
Relative Policy Optimization (GRPO) \[ 66\]. This two-stage strategy
allows us to first efficiently instill robust adherence to objective
standards and then leverage the fine-grained signals from our reward
modelfor optimizing more subjective qualities. As illustrated in Figure
14, this comprehensive process yields substantial improvements in
photorealism, aesthetic quality, and instruction following. 4.6.1.
Reward Annotation and Training As an indispensable and critical
component of the RLHF pipeline, our reward model is designed to evaluate
the model's performance along three key dimensions:
instruction-following capability, AI- Content Detection perception, and
aesthetic quality. The reward model is then trained specifically to
provide targeted feedback along these axes. For instruction following,
we perform syntactic and semantic decomposition of the prompt into a
structured hierarchy that includes (i) core subject entities, (ii)
attribute specifications, (iii) action or interaction requirements, (iv)
spatial or compositional constraints, and (v) stylistic or rendering
conditions. During annotation, human raters simply click on the elements
that are not satisfied by the model's output. We then compute the ratio
of satisfied elements to obtain the final instruction-following score,
which is used as the target reward. 4.6.2. Stage 1: Offline Alignment
with DPO on Objective Dimensions While manually curating preference
pairs for DPO is feasible for capturing human aesthetic judgments,
scaling this process to a large, high-quality dataset presents a
significant bottleneck in real practice. Sourcing consistently
informative preference pairs across subjective dimensions (e.g.,
aesthetics, style) 21

é¦™è•‰é•¿å‡ºçŒ´å­çš„è„‘è¢‹
è¿‘æ™¯ç‰¹å†™ä¸€åªäººç±»å¼‚è‰²çœ¼çœ¸ï¼Œè“è‰²çž³å­”åœ¨ä¸€ä¸ªé˜³å…‰æ˜Žåªšçš„å¤æ—¥ï¼Œä¸€ä¸ªç¾Žä¸½è€Œå¤©çœŸçš„18å²å¥³å­©åœ¨å°æºªé‡ŒçŽ©æ°´ï¼Œç¬‘å®¹ç¿çƒ‚
FSD RLHFä¸€ä¸ªç‹®å­éª‘çš„è¢‹é¼ åœ¨å–œé©¬æ‹‰é›…å±±è„‰ä¸Žè¿åŠ¨å‘˜å¥”è·‘çš„ç”»é¢ Figure
14\|Visual comparison between Few-Step Distillation (FSD, top row) and
RLHF (bottom row). Building upon the strong foundation of the FSD model,
RLHF further enhancesphotorealism,aesthetic quality, andinstruction
following. is slow and requires extensive expert annotation. To address
this scalability challenge and enhance annotation efficiency, our DPO
strategy pivots to focus exclusively on objective, verifiable
dimensions. These dimensions, such astext renderingandobject counting,
offer clear and binary correctness criteria that are highly amenable to
automated evaluation by modern Vision-Language Models (VLMs). For
instance, given a prompt requiring specific text, an image with
accurately rendered characters is designated as the positive sample
('chosen'), while an image with typographical errors becomes the
negative sample ('rejected'). We leverage VLMs to programmatically
generate a large corpus of such candidate preference pairs. This
VLM-generated dataset is then subjected to a streamlined human
verification and cleaning process, ensuring high fidelity. This hybrid
VLM-human pipeline dramatically increases annotation throughput and
consistency compared to purely human manual curation. Furthermore, to
smooth the learning curve, we implement a curriculum learning strategy
for DPO training. The process begins with prompts of low complexity
(e.g., rendering a single word, generating a small number of objects)
and progressively advances to more challenging instructions involving
multiple elements, complex layouts, or difficult styles. During this
process, we also optimized our pair selection strategy. We observed that
DPO's convergence is sensitive to the differentiation between positive
and negative samples. To maximize training efficiency, our curriculum
initially prioritizes pairs with moderate differentiation and gradually
introduces more challenging pairs exhibiting larger or more subtle
differences, which we found accelerate convergence and improve the final
performance. 4.6.3. Stage 2: Online Refinement with GRPO Building upon
the robust foundation established by DPO, the second stage employs
online reinforcement learning with GRPO. Guided by our reward model,
this stage is designed to significantly enhance the model's capability
forphotorealistic image generation, alongside improvingaesthetic
qualityand nuancedinstruction-following. During the GRPO training loop,
we compute a composite advantage function by aggregating the scores from
our reward model (e.g., realism, aesthetics, instruction following,
etc.). This multi-faceted feedback mechanism enables targeted,
fine-grained optimization \[ 84\]. By providing distinct signals for
different aspects of the generation, GRPO can simultaneously enhance
photorealistic image generation, aesthetic quality, improve semantic
accuracy, and reduce undesirable artifacts. This integrated approach
proved to be significantly more effective than optimizing against a
single reward, allowing the model to achieve a 22

better balance across multiple, often competing, quality dimensions.
4.7. Continued Training for Image Editing Starting from the base model,
the continued pre-training for image editing consists of two stages, as
shown in Figure 10. In the continued pre-training stage, we train the
model with our constructed editing pairs (see Section 2.5), together
with our text-to-image SFT data to ensure high image quality. We first
train the whole amount of editing data in resolution of 5122for a few
thousand steps for quick adaptation to editing tasks, and then increase
the image resolution to 10242for high generation quality. Because image
editing data pairs are expensive and difficult to acquire, their total
volume is significantly smaller and far less diverse than that of
text-to-image data. Therefore, we suggest a relatively higher ratio of
text-to-image data (e.g., text-to-image:image-to-image =4:1) to avoid
performance degradation during training. In the following SFT stage, a
task-balanced, high-quality subset of the training data is manually con-
structed to further improve the model's overall performance, especially
its instruction following ability. However, synthetic data (e.g., the
rendered text data for text editing), though easy-to-acquire and guaran-
teed to be 100% accurate in terms of instruction following, are far from
the distribution of real-world user input, and are thus heavily
downsampled in this final training stage. 4.8. Prompt Enhancer with
Reasoning Chain Z-Image TurboPE w/o reasoningPE w/ reasoning
ç”Ÿæˆä¸€ä¸ªä½ç½®åœ¨30Â°9 '36 "N, 120Â°7' 12" Eçš„ç…§ç‰‡Reasoning
chain:...åæ ‡30Â°09'36"N, 120Â°07'12"E
åœ¨å“ªé‡Œï¼ŸæŸ¥ä¸€ä¸‹ï¼šè¿™æ˜¯ä¸­å›½æµ™æ±Ÿçœæ­å·žå¸‚è¥¿æ¹–åŒºé™„è¿‘ï¼Œå…·ä½“æ˜¯æ­å·žè¥¿æ¹–ä¸€å¸¦ã€‚è¥¿æ¹–æ˜¯ä¸–ç•Œæ–‡åŒ–é—äº§ï¼Œè‘—åæ™¯ç‚¹ã€‚æ‰€ä»¥ï¼Œæˆ‘åº”è¯¥æž„æƒ³ä¸€ä¸ªåŸºäºŽè¥¿æ¹–çš„è§†è§‰åœºæ™¯ã€‚...
Reasoning
chain:...æˆ‘éœ€è¦å®šä¹‰æ­¥éª¤çš„æ–‡å­—ã€‚æ ‡å‡†æ³¡æ™®æ´±èŒ¶æ­¥éª¤ï¼š1.èŒ¶å…·ï¼šç´«ç ‚å£¶ã€å…¬é“æ¯ã€èŒ¶æ¯ã€‚2.å–èŒ¶å¶ï¼šç”¨èŒ¶åˆ™å–5-7å…‹æ™®æ´±èŒ¶ã€‚3.æ´—èŒ¶ï¼šæ²¸æ°´å†²æ´—èŒ¶å¶åŽå€’æŽ‰ã€‚4.å†²æ³¡ï¼šæ³¨å…¥æ²¸æ°´ï¼Œæµ¸æ³¡30ç§’ã€‚5.å€’èŒ¶ï¼šå°†èŒ¶æ±¤åˆ†å…¥èŒ¶æ¯ã€‚...æ‰‹å¸é£Žæ ¼ï¼Œæ³¡æ™®æ´±èŒ¶çš„æ­¥éª¤ï¼Œç”¨ä¸­æ–‡è¯´æ˜Ž
Five key habits to improve your child's gradesReasoning chain:...Content
Filling (Hallucination): Since the user did not provide specific habits,
I must generate these five content points based on common sense logic to
ensure the image possesses concrete details:1.Consistent Daily Study
Schedule2.Adequate Sleep Routine3.Organized Workspace Setup4.Active
Reading Engagement5.Positive Feedback Practice... Figure 15\|PE
visualization. We compare generation results between PE without
reasoning (middle column) and PE with reasoning (right column). As shown
in the top row, the reasoning chain enables the model to decipher raw
coordinates into a specific scenic context (e.g., West Lake) rather than
simply rendering the coordinate text. In the second row, the reasoning
module plans detailed steps for "brewing Pu-erh tea," allowing the model
to generate specific illustrations for each step instead of a generic
list. This demonstrates that the reasoning chain effectively injects
world knowledge and provides fine-grained content planning for complex
user prompts. Due to limited model size (6B parameters), Z-Image
exhibits limitations in world knowledge, intent 23

understanding, and complex reasoning. However, it serves as a powerful
text decoder capable of translating detailed prompts into realistic
images. To address the cognitive gaps, we propose equipping Z-Image with
a Prompt Enhancer (PE), powered by system prompt and a pretrained VLM,
to improve its reasoning and knowledge capabilities. Distinct from other
methods, we keep the large VLM fixed during alignment. Instead, we
process all input prompts (and input images for Z-Image-Edit) through
our PE model during the Supervised Fine-Tuning (SFT) stage. This
strategy ensures that Z-Image aligns effectively with the Prompt
Enhancer during SFT. Furthermore, we identify the structured reasoning
chain as a key factor for injecting reasoning and world knowledge. As
shown in Figure 15, without reasoning, the PE merely renders coordinate
text onto the image when given geolocation data; with reasoning, it
infers the location (e.g., West Lake) to generate the correct scene.
Similarly, in generating journal-style instructions, the lack of
reasoning leads to monotonous outputs, whereas the reasoning-enhanced
model enriches the result by generating specific illustrations for each
step. 5. Performance Evaluation 5.1. Elo-based Human Preference
Evaluation Table 3\|Elo rankings of Text-to-Image models from AI Arena.
Z-Image-Turbo achieves 4th globally and 1st among open-source models.
Rank Model Name Company Type 95% CI Elo Score Win Rate 1 Imagen 4 Ultra
Preview 0606 Google Closed-source -16/+16 1048 48% 2
gemini-2.5-flash-image-preview Google Closed-source -16/+14 1046 47% 3
Seedream 4.0 ByteDance Closed-source -17/+16 1039 46% 4Z-Image-T
urboAlibaba Open-source (6B) -15/+17 1025 45% 5 Seedream 3.0 ByteDance
Closed-source -15/+19 1012 41% 6 Qwen-Image Alibaba Open-source (20B)
-16/+16 1008 41% 7 GPT Image 1 OpenAI Closed-source -14/+17 986 38% 8
FLUX.1 Kontext Pro Black Forest Labs Closed-source -15/+14 950 32% 9
Ideogram 3.0 Ideogram Closed-source -15/+16 936 29% To rigorously
benchmark Z-Image-Turbo's capabilities against the competitive landscape
of generative models, we participated in AI Arena2, a public-facing,
independent benchmarking platform powered by large-scale human judgment.
Unlike automated metrics that frequently misalign with human perception,
AI Arena provides dynamic and unbiased model rankings based on thousands
of pairwise comparisons, making it an ideal venue for objective
performance assessment. The evaluation protocol is built upon the Elo
rating system -- a well-established method for ranking competitors based
on head-to-head outcomes. In each round, two images generated from the
same text prompt by different models are displayed side-by-side with
identities hidden. Evaluators are asked to select the image they
perceive as superior in terms of visual coherence, detail rendering,
prompt alignment, and artistic quality. Each vote updates the global Elo
leaderboard dynamically, ensuring that rankings reflect collective human
judgment over time. In the evaluation3, Z-Image-Turbo -- our
high-efficiency diffusion architecture with 6B parameters and low
inference cost of 8 NFEs -- competed against 8 leading models, including
top-tier closed-source systems: Imagen 4 Ultra Preview 0606 \[ 26\],
gemini-2.5-flash-image-preview \[ 25\] (both Google), Seedream 4.0
(ByteDance) \[ 64\], GPT Image 1 \[High\] (OpenAI) \[ 55\], FLUX.1
Kontext \[Pro\] (Black Forest Labs) \[ 37\], and Ideogram 3.0 \[31\],
along with the open-source baseline Qwen-Image \[76\]. As shown in Table
3, Z-Image-Turbo attained an Elo score of 1025, achieving 4th place
globally and ranking 1st among open models on the leaderboard. Despite
its efficient design, it outperforms multiple industry-leading
closed-source models and surpasses Qwen-Image while operating at
significantly lower computational cost. With a 45% win rate across all
matchups, Z-Image-Turbo demonstrates
2https://aiarena.alibaba-inc.com/corpora/arena/leaderboard?arenaType=T2I
3Snapshot taken on November 26, 2025 24

strong competitiveness against the entire field, including top-tier
proprietary systems. This high level of performance positions it firmly
within the elite tier of generative models, affirming its status as one
of the most advanced open-source systems available. These results
establish Z-Image-Turbo as one of the leading open text-to-image models
in terms of both quality and efficiency. More than a high-performing
generator, it represents a new baseline for efficiency- oriented
architecture design, demonstrating that compact models can achieve
elite-level performance without compromising usability. This combination
of speed, fidelity, and openness enables deployment in
resource-constrained environments, interactive applications, and
community-driven innovation. 5.2. Quantitative Evaluation To
comprehensively evaluate the generation and editing capabilities
ofZ-Imageand its variants (Z- Image-T urboandZ-Image-Edit), we conducted
extensive experiments across multiple authoritative benchmarks. These
evaluations cover general image generation, fine-grained instruction
following, text rendering in both English and Chinese, and
instruction-based image editing. 5.2.1. Text-to-Image Generation
CVTG-2K.To evaluate our model's performance on text rendering tasks, we
conduct quantitative experiments on the CVTG-2K benchmark \[ 17\].
CVTG-2K is a specialized benchmark designed for Com- plex Visual Text
Generation, encompassing diverse scenarios with varying numbers of text
regions. As presented in Table 4, our model achieves superior
performance on CVTG-2K across all evaluation metrics. Specifically,
Z-Image attains the highest average Word Accuracy score of 0.8671,
outperforming compet- itive baselines such as GPT-Image-1 \[ 55\]
(0.8569) and Qwen-Image \[ 76\] (0.8288). Notably, our model
demonstrates robust performance across varying levels of complexity,
maintaining consistent accuracy even as the number of text regions
increases from 2 to 5. Furthermore, Z-Image-Turbo, our efficient
variant, achieves the highest CLIP Score of 0.8048 among all models
while maintaining competitive text accuracy (0.8585 average Word
Accuracy), striking an effective balance between generation quality and
inference efficiency. These results demonstrate the effectiveness of our
approach in complex visual text generation scenarios. Table
4\|Quantitative evaluation results of English text rendering on CVTG-2K
\[17\]. Rank Model NED CLIPScoreWord Accuracy 2 regions 3 regions 4
regions 5 regions averageâ†‘ 1Z-Image 0.9367 0.79690.9006
0.87220.86520.8512 0.8671 2Z-Image-T urbo 0.92810.80480.8872 0.8662
0.8628 0.8347 0.8585 3 GPT Image 1 \[High\] \[55\] 0.94780.7982 0.8779
0.86590.87310.8218 0.8569 4 Qwen-Image \[76\] 0.9116 0.8017 0.8370
0.8364 0.8313 0.8158 0.8288 5 TextCrafter \[17\] 0.8679 0.7868 0.7628
0.7628 0.7406 0.6977 0.7370 6 SD3.5 Large \[18\] 0.8470 0.7797 0.7293
0.6825 0.6574 0.5940 0.6548 7 Seedream 3.0 \[21\] 0.8537 0.7821 0.6282
0.5962 0.6043 0.5610 0.5924 8 FLUX.1 \[dev\] \[37\] 0.6879 0.7401 0.6089
0.5531 0.4661 0.4316 0.4965 9 3DIS \[98\] 0.6505 0.7767 0.4495 0.3959
0.3880 0.3303 0.3813 10 RAG-Diffusion \[40\] 0.4498 0.7797 0.4388 0.3316
0.2116 0.1910 0.2648 11 TextDiffuser-2 \[10\] 0.4353 0.6765 0.5322
0.3255 0.1787 0.0809 0.2326 12 AnyText \[70\] 0.4675 0.7432 0.0513
0.1739 0.1948 0.2249 0.1804 LongText-Bench.To further assess our model's
capability in rendering longer texts, we evaluate its performance on
LongText-Bench \[ 22\], a specialized benchmark focusing on evaluating
the performance of rendering longer texts in both English and Chinese.
As shown in Table 5, our models demonstrate strong and consistent
performance across both language settings. On LongText-Bench-EN, Z-Image
achieves a competitive score of 0.935, ranking third among all evaluated
models, while on LongText-Bench-ZH, it attains a score of 0.936,
securing second place. Z-Image-Turbo also delivers impressive results,
scoring 0.917 on the English benchmark and 0.926 on the Chinese
benchmark, demonstrating strong efficiency- performance trade-offs. This
consistent performance across both languages highlights our model's
robust bilingual text rendering capability. 25

Table 5\|Quantitative evaluation results on LongText-Bench \[22\]. Rank
Model LongText-Bench-ENâ†‘LongText-Bench-ZHâ†‘ 1 Qwen-Image \[76\]
0.9430.946 2Z-Image 0.935 0.936 3Z-Image-T urbo 0.917 0.926 4 Seedream
3.0 \[21\] 0.896 0.878 5 X-Omni \[22\] 0.900 0.814 6 GPT Image 1
\[High\] \[55\] 0.9560.619 7 Kolors 2.0 \[34\] 0.258 0.329 8 BAGEL
\[15\] 0.373 0.310 9 OmniGen2 \[78\] 0.561 0.059 10 HiDream-I1-Full
\[7\] 0.543 0.024 11 BLIP3-o \[11\] 0.021 0.018 12 Janus-Pro \[14\]
0.019 0.006 13 FLUX.1 \[Dev\] \[37\] 0.607 0.005 OneIG.We utilize the
OneIG benchmark \[ 9\] to assess fine-grained alignment. As reported in
Tables 6 and 7, Z-Image achieves the highest overall score (0.546) on
the English track, surpassing Qwen-Image (0.539) and GPT Image 1
\[High\] (0.533). Notably, Z-Image sets a new state-of-the-art in text
rendering reliability with an EnglishTextscore of 0.987 and a
ChineseTextscore of 0.988, significantly outperforming competitors. On
the Chinese track, Z-Image ranks second overall (0.535), confirming its
multi-lingual robustness. Additionally, our distilled version,
Z-Image-Turbo, demonstrates impressive efficiency, maintaining strong
performance with only a marginal decrease compared to the base model.
Table 6\|Quantitative evaluation results on OneIG-EN \[ 9\]. The overall
score is the average of the five dimensions. Rank Model Alignment Text
Reasoning Style Diversity Overallâ†‘ 1Z-Image 0.881 0.987 0.280 0.387
0.194 0.546 2 Qwen-Image \[76\] 0.8820.891 0.306 0.418 0.197 0.539 3 GPT
Image 1 \[High\] \[55\] 0.851 0.8570.345 0.4620.151 0.533 4 Seedream 3.0
\[21\] 0.818 0.865 0.275 0.413 0.277 0.530 5Z-Image-T urbo
0.8400.9940.298 0.368 0.139 0.528 6 Imagen 4 \[26\] 0.857 0.805 0.338
0.377 0.199 0.515 7 Recraft V3 \[61\] 0.810 0.795 0.323 0.378 0.205
0.502 8 HiDream-I1-Full \[7\] 0.829 0.707 0.317 0.347 0.186 0.477 9
OmniGen2 \[78\] 0.804 0.680 0.271 0.377 0.242 0.475 10 SD3.5 Large
\[18\] 0.809 0.629 0.294 0.353 0.225 0.462 11 CogView4 \[97\] 0.786
0.641 0.246 0.353 0.205 0.446 12 FLUX.1 \[Dev\] \[37\] 0.786 0.523 0.253
0.368 0.238 0.434 13 Kolors 2.0 \[34\] 0.820 0.427 0.262 0.360 0.300
0.434 14 Imagen 3 \[3\] 0.843 0.343 0.313 0.359 0.188 0.409 15 BAGEL
\[15\] 0.769 0.244 0.173 0.367 0.251 0.361 16 Lumina-Image 2.0 \[58\]
0.819 0.106 0.270 0.354 0.216 0.353 17 SANA-1.5-4.8B \[81\] 0.765 0.069
0.217 0.401 0.216 0.334 18 SANA-1.5-1.6B \[81\] 0.762 0.054 0.209 0.387
0.222 0.327 19 BAGEL+CoT \[15\] 0.793 0.020 0.206 0.390 0.209 0.324 20
SD 1.5 \[63\] 0.565 0.010 0.207 0.3830.429 0.319 21 SDXL \[57\] 0.688
0.029 0.237 0.332 0.296 0.316 22 Show-o2-7B \[83\] 0.817 0.002 0.226
0.317 0.177 0.308 23 BLIP3-o \[11\] 0.711 0.013 0.223 0.361 0.229 0.307
24 Show-o2-1.5B \[83\] 0.798 0.002 0.219 0.317 0.186 0.304 25 Janus-Pro
\[14\] 0.553 0.001 0.139 0.276 0.365 0.267 GenEval.As shown in Table 8,
we evaluate object-centric generation using GenEval \[ 23\]. Z-Image
achieves an overall score of 0.84, securing a three-way tie for second
place alongside Seedream 3.0 \[ 21\] and GPT Image 1 \[High\] \[ 55\],
trailing only Qwen-Image \[ 76\] (0.87). Notably, Z-Image-Turbo delivers
highly competitive performance with an overall score of 0.82,
maintaining only a 2-point gap from the base model. These results
indicate that our model possesses a robust capability for generating
accurate and distinct entities. 26

Table 7\|Quantitative evaluation results on OneIG-ZH \[ 9\]. The overall
score is the average of the five dimensions. Rank Model Alignment Text
Reasoning Style Diversity Overallâ†‘ 1 Qwen-Image \[76\] 0.8250.963 0.267
0.405 0.279 0.548 2Z-Image 0.7930.9880.266 0.386 0.243 0.535 3 Seedream
3.0 \[21\] 0.793 0.928 0.281 0.397 0.243 0.528 4Z-Image-T urbo 0.782
0.982 0.276 0.361 0.134 0.507 5 GPT Image 1 \[High\] \[55\] 0.812
0.6500.300 0.4490.159 0.474 6 Kolors 2.0 \[34\] 0.738 0.502 0.226 0.331
0.333 0.426 7 BAGEL \[15\] 0.672 0.365 0.186 0.357 0.268 0.370 8
Cogview4 \[97\] 0.700 0.193 0.236 0.348 0.214 0.338 9 HiDream-I1-Full
\[7\] 0.620 0.205 0.256 0.304 0.300 0.337 10 Lumina-Image 2.0 \[58\]
0.731 0.136 0.221 0.343 0.240 0.334 11 BAGEL+CoT \[15\] 0.719 0.127
0.219 0.385 0.197 0.329 12 BLIP3-o \[11\] 0.608 0.092 0.213 0.369 0.233
0.303 13 Janus-Pro \[14\] 0.324 0.148 0.104 0.2640.358 0.240 Table
8\|Quantitative Evaluation results on GenEval \[23\]. Rank Model Single
Object Two Object Counting Colors Position Attribute Binding Overallâ†‘ 1
Qwen-Image \[76\] 0.99 0.92 0.89 0.88 0.76 0.77 0.87 2Z-Image 1.000.94
0.780.930.62 0.77 0.84 2 Seedream 3.0 \[21\] 0.99 0.960.91 0.930.470.80
0.84 2 GPT Image 1 \[High\] \[55\] 0.99 0.92 0.85 0.92 0.75 0.61 0.84 5
HiDream-I1-Full \[7\] 1.00 0.980.79 0.91 0.60 0.72 0.83 6Z-Image-T urbo
1.000.95 0.77 0.89 0.65 0.68 0.82 7 Janus-Pro-7B \[14\] 0.99 0.89 0.59
0.900.790.66 0.80 8 Lumina-Image 2.0 \[58\] - 0.87 0.67 - - 0.62 0.73 9
SD3.5-Large \[18\] 0.98 0.89 0.73 0.83 0.34 0.47 0.71 10 FLUX.1 \[Dev\]
\[37\] 0.98 0.81 0.74 0.79 0.22 0.45 0.66 11 JanusFlow \[51\] 0.97 0.59
0.45 0.83 0.53 0.42 0.63 12 SD3 Medium \[18\] 0.98 0.74 0.63 0.67 0.34
0.36 0.62 13 Emu3-Gen \[72\] 0.98 0.71 0.34 0.81 0.17 0.21 0.54 14
Show-o \[82\] 0.95 0.52 0.49 0.82 0.11 0.28 0.53 15 PixArt-ð›¼\[13\] 0.98
0.50 0.44 0.80 0.08 0.07 0.48 DPG-Bench.Table 9 presents the comparison
on the DPG-Bench benchmark \[ 30\], which evaluates the ability of
prompt following in dense prompts. Z-Image achieves a strong global
performance, ranking third overall with a score of 88.14, closely
trailing Seedream 3.0 \[ 21\] and Qwen-Image \[ 76\]. Notably, our model
demonstrates robust performance in theAttributedimension (93.16),
surpassing the leading Qwen- Image (92.02) and Seedream 3.0 (91.36).
Furthermore, our 8-step distillation model (Z-Image-Turbo), maintains
competitive performance while achieving high efficiency. TIIF.Table 10
details the performance on the TIIF benchmark testmini \[ 74\], which
systematically evaluates instruction-following capabilities. Z-Image and
Z-Image-Turbo achieve the 4th and 5th ranks, respectively. These results
demonstrate that both the base and distilled versions possess
exceptional capabilities in interpreting and executing complex user
instructions across diverse categories. PRISM-Bench.We evaluate our
models on PRISM-Bench \[ 19\], a VLM-powered benchmark assessing
reasoning and aesthetics across seven tracks. On the English track
(Table 11), Z-Image-Turbo achieves the 3rd rank (77.4), outperforming
the base model and Qwen-Image, which highlights its superior efficiency
and generation quality. On the Chinese track (Table 12), Z-Image ranks
2nd (75.3), demonstrating robust multi-lingual performance with
exceptional scores inText Rendering(83.4) andComposition(88.6). 27

Table 9\|Quantitative evaluation results on DPG \[30\]. Rank Model
Global Entity Attribute Relation Other Overallâ†‘ 1 Qwen-Image \[76\]
91.32 91.56 92.02 94.3192.73 88.32 2 Seedream 3.0 \[21\] 94.31
92.6591.36 92.78 88.24 88.27 3Z-Image 93.39 91.2293.1692.22 91.52 88.14
4 Lumina-Image 2.0 \[58\] - 91.97 90.2094.85- 87.20 5 HiDream-I1-Full
\[7\] 76.44 90.22 89.48 93.74 91.83 85.89 6 GPT Image 1 \[High\] \[55\]
88.89 88.94 89.84 92.63 90.96 85.15 7Z-Image-T urbo 91.29 89.59 90.14
92.16 88.68 84.86 8 Janus-Pro-7B \[14\] 86.90 88.90 89.40 89.32 89.48
84.19 9 SD3 Medium \[18\] 87.90 91.01 88.83 80.70 88.68 84.08 10 FLUX.1
\[Dev\] \[37\] 74.35 90.00 88.96 90.87 88.33 83.84 11 DALL-E 3 \[4\]
90.97 89.61 88.39 90.58 89.83 83.50 12 Janus-Pro-1B \[14\] 87.58 88.63
88.17 88.98 88.30 82.63 13 Emu3-Gen \[72\] 85.21 86.68 86.84 90.22 83.15
80.60 14 PixArt-Î£\[12\] 86.89 82.89 88.94 86.59 87.68 80.54 15 Janus
\[77\] 82.33 87.38 87.70 85.46 86.41 79.68 16 Hunyuan-DiT \[41\] 84.59
80.59 88.01 74.36 86.41 78.87 17 Playground v2.5 \[39\] 83.06 82.59
81.20 84.08 83.50 75.47 18 SDXL \[57\] 83.27 82.43 80.91 86.76 80.41
74.65 19 Lumina-Next \[99\] 82.82 88.65 86.44 80.53 81.82 74.63 20
PixArt-ð›¼\[13\] 74.97 79.32 78.60 82.57 76.96 71.11 21 SD1.5 \[63\] 74.63
74.23 75.39 73.49 67.81 63.18 Table 10\|Quantitative evaluation results
on TIIF Bench testmini \[74\]. Rank ModelOverallâ†‘Basic Following
Advanced Following Designer Avg Attribute Relation Reasoning Avg
Attr.+Rela. Attr.+Reas. Rela.+Reas. Style Text Real World short long
short long short long short long short long short long short long short
long short long short long short long short long 1 GPT Image 1 \[High\]
\[55\] 89.15 88.29 90.75 89.66 91.3387.08 84.57 84.5796.32 97.32 88.55
88.35 87.07 89.44 87.22 83.96 85.59 83.2190.00 93.33 89.83 86.83
89.7393.46 2 Qwen-Image \[76\] 86.14 86.83 90.18 87.22
90.5091.5088.2290.7879.81 79.38 79.30 80.88 79.21 78.94 78.85 81.69
75.57 78.59100.00 100.0092.76 89.1490.3091.42 3 Seedream 3.0 \[21\]
86.02 84.31 87.07 84.93 90.50 90.0089.8585.94 80.86 78.86 79.16 80.60
79.76 81.82 77.23 78.85 75.64 78.64100.0093.3397.1787.78 83.21 83.58
4Z-Image 80.20 83.04 78.36 82.79 79.50 86.50 80.45 79.94 75.13 81.94
72.89 77.02 72.91 77.56 66.99 73.82 73.89 75.62 90.00 93.33
94.8493.2188.06 85.45 5Z-Image-T urbo 77.73 80.05 81.85 81.59 86.50
87.00 82.88 79.99 76.17 77.77 68.32 74.69 72.04 75.24 60.22 73.33 68.90
71.92 83.33 93.33 83.71 84.62 85.82 77.24 6 DALL-E 3 \[4\] 74.96 70.81
78.72 78.50 79.50 79.83 80.82 78.82 75.82 76.82 73.39 67.27 73.45 67.20
72.01 71.34 63.59 60.72 89.66 86.67 66.83 54.83 72.93 60.99 7 FLUX.1
\[dev\] \[37\] 71.09 71.78 83.12 78.65 87.05 83.17 87.25 80.39 75.01
72.39 65.79 68.54 67.07 73.69 73.84 73.34 69.09 71.59 66.67 66.67 43.83
52.83 70.72 71.47 8 FLUX.1 \[Pro\] \[37\] 67.32 69.89 79.08 78.91 78.83
81.33 82.82 83.82 75.57 71.57 61.10 65.37 62.32 65.57 69.84 71.47 65.96
67.72 63.00 63.00 35.83 55.83 71.80 68.80 9 Midjourney V7 \[52\] 68.74
65.69 77.41 76.00 77.58 81.83 82.07 76.82 72.57 69.32 64.66 60.53 67.20
62.70 81.22 71.59 60.72 64.59 83.33 80.00 24.83 20.83 68.83 63.61 10 SD
3 \[18\] 67.46 66.09 78.32 77.75 83.33 79.83 82.07 78.82 71.07 74.07
61.46 59.56 61.07 64.07 68.84 70.34 50.96 57.84 66.67 76.67 59.83 20.83
63.23 67.34 11 SANA 1.5 \[81\] 67.15 65.73 79.66 77.08 79.83 77.83 85.57
83.57 73.57 69.82 61.50 60.67 65.32 56.57 69.96 73.09 62.96 65.84 80.00
80.00 17.83 15.83 71.07 68.83 12 Janus-Pro-7B \[14\] 66.50 65.02 79.33
78.25 79.33 82.33 78.32 73.32 80.32 79.07 59.71 58.82 66.07 56.20 70.46
70.84 67.22 59.97 60.00 70.00 28.83 33.83 65.84 60.25 13 Infinity \[28\]
62.07 62.32 73.08 75.41 74.33 76.83 72.82 77.57 72.07 71.82 56.64 54.98
60.44 55.57 74.22 64.71 60.22 59.71 80.00 73.33 10.83 23.83 54.28 56.89
14 PixArt-Î£\[12\] 62.00 58.12 70.66 75.25 69.33 78.83 75.07 77.32 67.57
69.57 57.65 49.50 65.20 56.57 66.96 61.72 66.59 54.59 83.33 70.00 1.83
1.83 62.11 52.41 15 Show-o \[82\] 59.72 58.86 73.08 75.83 74.83 79.83
78.82 78.32 65.57 69.32 53.67 50.38 60.95 56.82 68.59 68.96 66.46 56.22
63.33 66.67 3.83 2.83 55.02 50.92 16 LightGen \[79\] 53.22 43.41 66.58
47.91 55.83 47.33 74.82 45.82 69.07 50.57 46.74 41.53 62.44 40.82 61.71
50.47 50.34 45.34 53.33 53.33 0.00 6.83 50.92 50.55 17 Hunyuan-DiT
\[41\] 51.38 53.28 69.33 69.00 65.83 69.83 78.07 73.82 64.07 63.32 42.62
45.45 50.20 41.57 59.22 61.84 47.84 51.09 56.67 73.33 0.00 0.83 40.10
44.20 18 Lumina-Next \[99\] 50.93 52.46 64.58 66.08 56.83 59.33 67.57
71.82 69.32 67.07 44.75 45.63 51.44 43.20 51.09 59.72 44.72 54.46 70.00
66.67 0.00 0.83 47.56 49.05 Table 11\|Quantitative results on
PRISM-Bench \[19\] evaluated by Qwen2.5-VL-72B \[2\]. Rank
ModelImagination Entity Text rendering Style Affection Composition Long
text Overallâ†‘ Ali. Aes. Avg. Ali. Aes. Avg. Ali. Aes. Avg. Ali. Aes.
Avg. Ali. Aes. Avg. Ali. Aes. Avg. Ali. Aes. Avg. Ali. Aes. Avg. 1
GPT-Image-1 \[High\] \[55\] 79.853.3 66.6 87.381.084.166.786.876.8 87.3
87.8 87.5 88.1 79.8 84.0 92.2 84.9 88.577.277.5 77.4 82.7 78.780.7 2
Gemini 2.5-Flash-Image \[25\] 84.738.1 61.4 86.0 76.7 81.372.884.378.5
89.587.888.6 94.374.8 84.5 91.2 88.289.776.380.6 78.4 85.075.8 80.4
3Z-Image-T urbo 65.7 50.1 57.9 75.782.379.0 59.6 84.9 72.2 76.7 88.2
82.4 85.187.4 86.289.090.289.6 69.8 79.0 74.4 74.580.377.4 4 Seedream
3.0 \[21\] 75.8 38.0 56.9 81.3 74.2 77.7 58.8 74.0 66.4 84.4 84.1 84.2
90.5 74.6 82.5 93.6 85.1 89.3 76.2 76.4 76.3 80.1 72.3 76.2 5Z-Image
68.0 47.3 57.6 75.0 74.4 74.7 59.3 81.6 70.4 78.089.083.5 84.3 80.1 82.2
89.1 85.1 87.1 70.6 76.6 73.6 74.9 76.2 75.6 6 Qwen-Image \[76\] 75.5
37.4 56.5 79.5 64.5 72.0 57.9 71.2 64.5 86.6 84.4 85.5 89.9 70.4
80.193.979.5 86.7 76.8 70.9 73.8 80.0 68.3 74.1 7 FLUX.1-Krea-dev \[37\]
69.6 43.1 56.3 72.2 70.7 71.4 51.7 76.1 63.9 80.0 86.6 83.3 82.6 78.7
80.6 90.8 87.1 88.9 73.6 73.4 73.5 74.4 73.7 74.0 8 HiDream-I1-Full
\[7\] 73.0 44.0 58.5 76.3 72.8 74.5 60.5 76.4 68.4 81.4 81.5 81.4 90.0
76.6 83.3 88.5 80.3 84.4 66.3 48.6 57.4 76.6 68.6 72.6 9 SD3.5-Large
\[18\] 66.7 43.4 55.0 76.8 72.7 74.8 53.6 73.1 63.3 77.3 78.2 77.7 85.6
73.9 79.7 87.8 80.9 84.3 65.8 52.2 59.0 73.4 67.8 70.6 10 HiDream-I1-Dev
\[7\] 68.8 45.8 57.3 73.5 68.1 70.8 56.7 75.7 66.2 70.2 77.4 73.8 88.2
74.3 81.2 84.7 78.5 81.6 64.0 49.3 56.6 72.3 67.0 69.6 11 FLUX.1-dev
\[37\] 65.5 42.9 54.2 70.6 61.9 66.2 52.3 73.0 62.6 72.6 74.2 73.4 86.0
72.9 79.4 87.4 75.8 81.6 70.5 53.8 62.1 72.1 64.9 68.5 12 SD3.5-Medium
\[18\] 65.1 34.7 49.9 72.5 70.9 71.7 36.6 64.5 50.5 75.5 80.0 77.7 81.8
73.9 77.9 85.4 81.0 83.2 63.5 50.6 57.0 68.6 65.1 66.8 13 SD3-Medium
\[18\] 64.3 37.7 51.0 69.4 63.3 66.3 38.5 63.3 50.9 74.6 79.5 77.0 80.5
75.5 78.0 85.6 79.5 82.5 63.4 50.3 56.8 68.0 64.2 66.1 14 FLUX.1-schnell
\[37\] 62.8 35.6 49.2 64.8 56.8 60.8 54.3 68.1 61.2 70.3 71.5 70.9 75.4
65.9 70.6 81.7 75.6 78.6 68.7 54.4 61.5 68.3 61.1 64.7 15 Janus-Pro-7B
\[14\] 65.0 38.8 51.9 68.6 63.5 66.0 23.1 50.3 36.7 70.7 75.2 72.9 80.7
68.0 74.3 82.4 71.1 76.7 63.9 49.0 56.4 64.9 59.4 62.1 16 Bagel \[15\]
68.0 45.0 56.5 67.6 53.4 60.5 29.4 42.3 35.8 69.0 69.7 69.3 87.1 66.7
76.9 86.6 69.2 77.9 64.5 50.2 57.3 67.5 56.6 62.0 17 Bagel-CoT \[15\]
68.0 44.1 56.0 67.6 53.4 60.5 29.4 42.3 35.8 69.0 69.7 69.3 87.1 66.7
76.9 86.6 69.2 77.9 64.5 50.2 57.3 67.5 56.5 62.0 18 Playground \[39\]
59.0 39.0 49.0 69.4 56.7 63.0 15.3 31.9 23.6 74.6 74.6 74.6 88.8 66.0
77.4 72.2 61.3 66.7 56.0 35.3 45.6 62.2 52.1 57.1 19 SDXL \[57\] 54.5
34.1 44.3 71.1 65.0 68.0 18.6 37.3 27.9 71.7 72.6 72.1 78.7 66.5 72.6
72.2 67.8 70.0 54.1 34.5 44.3 60.1 54.0 57.0 20 SD2.1 \[63\] 48.9 28.4
38.6 66.0 57.6 61.8 16.7 31.4 24.0 62.7 66.5 64.6 68.5 62.1 65.3 64.8
58.3 61.5 50.7 29.8 40.2 54.0 47.7 50.8 21 SD1.5 \[63\] 40.7 23.7 32.2
61.2 52.7 56.9 11.4 24.1 17.8 56.7 61.5 59.1 66.9 60.7 63.8 57.5 53.4
55.4 47.3 26.8 37.0 48.8 43.3 46.0 5.2.2. Instruction-based Image
Editing ImgEdit.Table 13 shows the evaluation of Z-Image-Edit on the
ImgEdit Benchmark \[ 87\], where the metric combines instruction
completion and visual quality. Across 9 common editing tasks,
Z-Image-Edit 28

Table 12\|Quantitative results on PRISM-Bench-ZH \[19\] evaluated by
Qwen2.5-VL-72B \[2\]. Rank ModelImagination Entity Text rendering Style
Affection Composition Long text Overallâ†‘ Ali. Aes. Avg. Ali. Aes. Avg.
Ali. Aes. Avg. Ali. Aes. Avg. Ali. Aes. Avg. Ali. Aes. Avg. Ali. Aes.
Avg. Ali. Aes. Avg. 1 GPT-Image-1 \[High\] \[55\] 73.0 37.6 55.3
80.482.181.373.1 89.9 81.577.1 92.4 84.878.0 77.877.9
91.985.788.872.476.3 74.4 78.0 77.4 77.7 2Z-Image 69.5 34.1 51.6 70.6
73.7 72.276.8 90.0 83.474.1 88.2 81.2 77.6 73.5 75.5 89.388.088.6 71.6
75.6 73.6 75.7 74.9 75.3 3Z-Image-T urbo 64.1 37.2 50.7 72.982.477.6
69.4 89.7 79.6 72.9 89.2 81.0 74.080.977.5 87.2 85.8 86.5 71.7 74.8 73.3
73.1 77.1 75.1 4 Seedream 3.0 \[21\] 71.4 36.6 54.0 74.8 73.8 74.3 70.7
88.0 79.4 74.1 88.0 81.179.071.4 75.2 90.3 83.2 86.873.071.2 72.1 76.2
73.2 74.7 5 Qwen-Image \[76\] 71.4 29.9 50.7 74.7 67.8 71.3 64.3 73.1
68.7 75.2 83.2 79.2 77.3 64.5 70.9 89.8 74.1 82.0 72.6 65.8 69.2 75.0
65.5 70.3 6 Bagel-CoT \[15\] 64.4 36.6 50.5 62.6 53.8 58.2 25.2 51.9
38.6 65.4 76.7 71.1 74.0 65.0 69.5 81.3 71.3 76.3 61.4 46.6 54.0 62.0
57.4 59.7 7 Bagel \[15\] 64.6 36.3 50.5 62.7 55.5 59.1 18.6 26.3 22.5
66.0 76.6 71.3 74.9 66.2 70.6 81.3 72.2 76.8 62.4 47.3 54.9 61.5 54.3
57.9 8 HiDream-I1-Full \[7\] 51.2 30.8 41.0 60.1 61.3 60.7 20.7 40.6
30.7 64.5 73.8 69.2 65.2 69.1 67.2 72.4 69.0 70.7 57.1 42.8 50.0 55.9
55.3 55.6 9 HiDream-I1-Dev \[7\] 48.3 24.6 36.5 52.6 54.1 53.4 18.6 35.3
27.0 59.0 68.3 63.7 65.9 62.3 64.1 66.5 64.6 65.6 54.2 38.6 46.4 52.2
49.7 50.9 Table 13\|Quantitative Evaluation results on ImgEdit \[87\].
Rank Model Add Adjust Extract Replace Remove Background Style Hybrid
Action Overallâ†‘ 1 UniWorld-V2 \[43\] 4.294.44 4.32 4.69 4.724.41 4.91
3.83 4.83 4.49 2 Qwen-Image-Edit \[2509\] \[76\] 4.32 4.36 4.04 4.64
4.52 4.37 4.84 3.39 4.71 4.35 3Z-Image-Edit 4.40 4.14 4.30 4.57 4.13
4.14 4.85 3.63 4.50 4.30 4 Qwen-Image-Edit \[76\] 4.38 4.16 3.43 4.66
4.14 4.38 4.81 3.82 4.69 4.27 5 GPT-Image-1 \[High\] \[55\] 4.614.33
2.90 4.35 3.664.57 4.93 3.96 4.89 4.20 6 FLUX.1 Kontext \[Pro\] \[37\]
4.25 4.15 2.35 4.56 3.57 4.26 4.57 3.68 4.63 4.00 7 OmniGen2 \[78\] 3.57
3.06 1.77 3.74 3.20 3.57 4.81 2.52 4.68 3.44 8 UniWorld-V1 \[44\] 3.82
3.64 2.27 3.47 3.24 2.99 4.21 2.96 2.74 3.26 9 BAGEL \[15\] 3.56 3.31
1.70 3.30 2.62 3.24 4.49 2.38 4.17 3.20 10 Step1X-Edit \[47\] 3.88 3.14
1.76 3.40 2.41 3.16 4.63 2.64 2.52 3.06 11 ICEdit \[94\] 3.58 3.39 1.73
3.15 2.93 3.08 3.84 2.04 3.68 3.05 12 OmniGen \[80\] 3.47 3.04 1.71 2.94
2.43 3.21 4.19 2.24 3.38 2.96 13 UltraEdit \[95\] 3.44 2.81 2.13 2.96
1.45 2.83 3.76 1.91 2.98 2.70 14 AnyEdit \[90\] 3.18 2.95 1.88 2.47 2.23
2.24 2.85 1.56 2.65 2.45 15 MagicBrush \[92\] 2.84 1.58 1.51 1.97 1.58
1.75 2.38 1.62 1.22 1.90 16 Instruct-Pix2Pix \[5\] 2.45 1.83 1.44 2.01
1.50 1.44 3.55 1.20 1.46 1.88 Table 14\|Quantitative Evaluation results
on GEdit-Bench \[47\]. Rank ModelGEdit-Bench-EN GEdit-Bench-CN G_SC G_PQ
G_Oâ†‘ G_SC G_PQ G_Oâ†‘ 1 UniWorld-V2 \[43\] 8.39 8.02 7.83 - - - 2
Qwen-Image-Edit \[2509\] \[76\] 8.15 7.86 7.54 8.08 7.89 7.54
3Z-Image-Edit 8.11 7.72 7.57 8.03 7.807.54 4 Qwen-Image-Edit \[76\] 8.00
7.86 7.56 7.82 7.79 7.52 5 GPT-Image-1 \[High\] \[55\] 7.85 7.62 7.53
7.67 7.56 7.30 6 Step1X-Edit \[47\] 7.66 7.35 6.97 7.20 6.87 6.86 7
BAGEL \[15\] 7.36 6.83 6.52 7.34 6.85 6.50 8 OmniGen2 \[78\] 7.16 6.77
6.41 - - - 9 FLUX.1 Kontext \[Pro\] \[37\] 7.02 7.60 6.56 1.11 7.36 1.23
10 FLUX.1 Kontext \[Dev\] \[37\] 6.52 7.38 6.00 - - - 11 OmniGen \[80\]
5.96 5.89 5.06 - - - 12 UniWorld-V1 \[44\] 4.93 7.43 4.85 - - - 13
MagicBrush \[92\] 4.68 5.66 4.52 - - - 14 Instruct-Pix2Pix \[5\] 3.58
5.49 3.68 - - - 15 AnyEdit \[90\] 3.18 5.82 3.21 - - - shows competitive
editing performance with leading models , especially object addition and
extraction. GEdit.We also evaluate Z-Image-Edit on the GEdit-Bench \[
47\], which evaluates visual naturalness (G_PQ) and bilingual
instruction following (G_SC). GEdit-Bench-EN abd GEdit-Bench-CN adopt
English and Chinese instructions in the evaluation, respectively. As
shown in Table 14, Z-Image-Edit achieves 3rd rank, demonstrating robust
bilingual editing capabilities. 5.3. Qualitative Evaluation To further
demonstrate the visual generation capacity of Z-Image4, we first give
the qualitative com- parison against state-of-the-art open-source models
(Lumina-Image 2.0 \[ 58\], Qwen-Image \[ 76\], Hun- 4In the section, all
results of Z-Image are generated by our Turbo version. 29

yuanImage 3.0 \[ 8\], and FLUX 2 dev \[ 36\]) and close-source models
(Imagen 4 Ultra \[ 26\], Seedream 4.0 \[ 64\] and Nano Banana Pro \[
27\]). We then show the editing capacity of our Z-Image-Edit. We next
show the examples of how reasoning capacity and world knowledge are
injected by our prompt enhancer. We finally show that the emerging
multi-lingual and multi-cultural understanding capacity of our Z-Image.
5.3.1. Superior Photorealistic Generation As shown in Figure 16 and 17,
Z-Image-Turbo shows excellent character close-up generation (e.g., the
skin details on a man's face and a girl's tears). When asked to generate
multi-expression portraits of one person (Figure 18), Z-Image-Turbo can
produce images that are more aesthetically pleasing and have more
realistic expressions, while Qwen-Image, HunyuanImage3.0, FLUX 2 dev,
and Seedream 4.0 would sometimes generate exaggerated and unrealistic
expressions, thus lacking authenticity and beauty. Moreover, when
generating a scene captured by a mobile phone (Figure 19 and 20),
Z-Image-Turbo shows strong performance in the authenticity of both the
person and the background, as well as the aesthetic appeal of layout and
posture. while Qwen-Image, HunyuanImage3.0, and FLUX 2 dev would
generate unrealistic things (e.g., clothes that remain completely
unsoaked in the heavy rain). 5.3.2. Outstanding Bilingual Text Rendering
Figure 21 and Figure 22 show the qualitative comparison of Chinese and
English text rendering. As shown in Figure 21 and Figure 22,
Z-Image-Turbo accurately rendered the required text while maintaining
the aesthetic appeal and authenticity of other parts (e.g., the
authenticity of the human face in Figure 21 and the layout of the scene
in Figure 22). Note that this is comparable to the leading closed-source
model Nano Banana Pro, and surpasses other candidates. When rendering
text in poster design (Figure 23 and Figure 24), Z-Image-Turbo not only
presents correct text rendering, but also designs a more aesthetically
pleasing and realistic poster. For example, as shown in Figure 24),
Qwen-Image, HunyuanImage3.0, FLUX 2 dev, and Imagen 4 Ultra make errors
when rendering very small characters, Seedream4.0 and Nano Banana Pro
make errors of repeatedly rendering the text, while Z-Image-Turbo gets
the poster with the right rendered text and satisfactory design. 5.3.3.
Instruction-following Editing The first two columns of Figure 25
demonstrates the ability of Z-Image-Edit to handle complex composite
prompts. For example, the top simultaneously switches the background to
the Sydney Opera House, inserting a specific object (a sign reading
"Z-Image"), and removing the character's backpack. The bottom row also
illustrates precise control over multiple subjects. The last two columns
of Figure 25 also illustrates that Z-Image-Edit can accurately modify
textual content according to a bounding-box--based location constraint
(left) and keep characters consistent when transforming the image
(right). 5.3.4. Enhanced Reasoning Capacity and World Knowledge through
Prompt Enhancer As demonstrated in Figure 15 and Figures 26-27, our
prompt enhancer leverages a structured reasoning chain -- comprising
core subject analysis, problem solving/world knowledge injection,
aesthetic enhance- ment, and comprehensive description -- to equip the
model with logical reasoning and world knowledge capabilities. This
allows the model to handle diverse tasks, ranging from solving complex
logical puzzles (e.g., the chicken-and-rabbit problem) and interpreting
user intent (e.g., visualizing classical poetry or inferring scenes from
coordinates) to performing text rendering and question answering. In the
context of image editing, prompt enhancer is also crucial for addressing
ambiguous or unclear intentions, as well as for injecting world
knowledge and enabling reasoning, similar to how it functions in
text-to-image generation, as shown in Figure 28-29. For example, in
Figure 29, the wrong dish is made because of a lack of reasoning about
the relationship between the ingredients and the dish, while prompt
enhancer can make up for this. 30

5.3.5. Emerging Multi-lingual and Multi-cultural Understanding Capacity
After trained with bilingual data, we are surprised to find that Z-Image
has initially emerged with the ability to handle multilingual input. As
shown in Figure 30, Z-Image can not only understand prompts in multiple
languages but also generate images that align with local cultures and
landmarks. Input promptå¥³ç”Ÿå¤§å“­ã€‚(The girl is crying.) Imagen4 Ultra
HunyuanImage3.0Qwen-Image Seedream4.0 NanoBanana Pro Z-Image-Turbo
Seedream3.0 FLUX 2 \[dev\] Lumina-Image 2.0 Figure 16\|Comparison of
close-up portrait generation, which indicates that Z-Image exhibits
strong capabilities in character emotion and skin texture rendering.
Better to zoom in to check the subtle expressions and the texture of the
skin. 31

Qwen-Image HunyuanImage3.0 Imagen4 Ultra Seedream4.0 NanoBanana Pro
Z-Image-Turbo Input prompt Seedream3.0 FLUX 2 \[dev\] Lumina-Image
2.0ç”·äººæ²‰æ€ã€‚(The man is lost in thought.)Figure 17\|Comparison of
close-up portrait generation, which indicates that Z-Image exhibits
strong capabilities in character emotion and skin texture rendering.
Better to zoom in to check the subtle expressions and the texture of the
skin. 32

ä¸€ä¸ªä¸‰ä¹˜ä¸‰çš„ç½‘æ ¼æ‹¼è´´ç”»ï¼Œåœ¨çº¯ç™½è‰²çš„èƒŒæ™¯ä¸‹ï¼Œå±•ç¤ºäº†åŒä¸€ä½å¹´è½»äºšæ´²å¥³æ€§çš„ä¹å¹…å·¥ä½œå®¤è‚–åƒï¼Œå¥¹æœ‰ç€é»‘è‰²çš„é•¿å‘å’Œç»†ç»†çš„åˆ˜æµ·ã€‚è¿™äº›ç…§ç‰‡æ•æ‰äº†å„ç§å„æ ·
æœ‰è¶£è€Œæ„Ÿæ€§çš„é¢éƒ¨è¡¨æƒ…ã€‚ä»Žæœ€ä¸Šé¢ä¸€æŽ’å¼€å§‹ï¼Œä»Žå·¦åˆ°å³ï¼šå¥¹ç”¨å³çœ¼çœ¨çœ¼ï¼ŒåŒæ—¶è°ƒçš®åœ°ä¼¸å‡ºèˆŒå¤´ã€‚å¥¹çƒ­æƒ…åœ°ç¬‘ç€ï¼Œçœ¼ç›é—­å¾—ç´§ç´§çš„ï¼Œä¸€ç§å®³ç¾žæˆ–é«˜å…´çš„æ ·å­ã€‚
å¥¹åŒæ‰‹æ‚ç€å˜´ï¼Œçå¤§çœ¼ç›ç›´è§†é•œå¤´ï¼Œä¼¼ä¹Žæ˜¯æƒŠè®¶æˆ–å®³ç¾žï¼Œè„¸é¢Šæ˜Žæ˜¾æ³›çº¢ã€‚ä¸­é—´ä¸€æŽ’ï¼šå¥¹ç›´è§†é•œå¤´ï¼Œè¡¨æƒ…ä¸­æ€§ï¼Œå¾®å¾®æ’…ç€å˜´ã€‚å¥¹ç¬‘åˆ°ä¸€åŠï¼Œå¤´å¾®å¾®å‘åŽä»°ï¼Œçœ¼
ç›çœ¯ç€ï¼Œå˜´å·´å¼ å¾—å¤§å¤§çš„ï¼Œéœ²å‡ºäº†ç‰™é½¿ã€‚å¥¹ç”¨å·¦çœ¼çœ¨äº†çœ¨çœ¼ç›ï¼ŒåŒæ—¶å¾®å¾®çš±äº†çš±çœ‰ã€‚æœ€ä¸‹é¢ä¸€æŽ’ï¼šå¥¹å·¦æ‰‹ä¸¾ç€å’Œå¹³æ‰‹åŠ¿ï¼ˆvå­—ï¼‰ï¼Œçœ¼ç›çå¾—å¤§å¤§çš„ï¼ŒèˆŒå¤´å¾®å¾®
ä¼¸å‡ºã€‚å¥¹çš„ä¸¤åªæ‰‹åž‚ç›´åœ°æ”¾åœ¨é¼»å­å’Œçœ¼ç›æ—è¾¹ï¼Œå˜´å·´å¼ å¼€æˆä¸€ä¸ªåœ†å½¢çš„"O"å½¢ï¼Œçœ¼ç›é—­ç€ï¼Œå¥½åƒåœ¨å–Šå«æˆ–å·çœ‹ã€‚å¥¹ç”¨æ‰‹æ‰˜ç€å·¦è„¸é¢Šï¼Œå¸¦ç€ä¸€ç§æ¸©æŸ”ã€æ²‰æ€çš„
è¡¨æƒ…ï¼Œå¾®å¾®ç¦»å¼€é•œå¤´å‘å³çœ‹ã€‚æ‰€æœ‰é•œå¤´çš„ç¯å…‰éƒ½æ˜¯æ˜Žäº®è€Œå‡åŒ€çš„ã€‚Input prompt
Qwen-Image HunyuanImage3.0 Imagen4 Ultra Seedream4.0 Nano Banana Pro
Z-Image-TurboSeedream3.0 FLUX 2 \[dev\] Lumina-Image 2.0Figure
18\|Comparison of complex close-up portrait generation, which indicates
that Z-Image-Turbo has a strong ability in rendering character
expressions and skin textures, as well as generating aesthetic images.
Better to zoom in to check the subtle expressions. 33

ä¸€ä½ç©¿ç€ç™½è¡¬è¡«çš„å¥³äººåœ¨é›¨å¤œç‹¬è‡ªèµ°åœ¨è¡—å¤´ã€‚å¥¹çš„ç¥žæƒ…ä¸­é€éœ²å‡ºå¤±è½ï¼Œå¾®å¾®ä½Žç€
å¤´ï¼Œé›¨ä¸è½»è½»è½ä¸‹ï¼Œåœ¨æ¹¿æ¼‰æ¼‰çš„è·¯é¢ä¸Šå½¢æˆäº†å°æ°´æ´¼ã€‚è¡—ç¯æ´’ä¸‹æŸ”å’Œçš„å…‰èŠ’ï¼Œæ˜ 
ç…§å‡ºå¥¹å¤´å‘å’Œè¡£æœä¸Šçš„é›¨æ»´ã€‚èƒŒæ™¯ä¸­æ¨¡ç³Šçš„åŸŽå¸‚å»ºç­‘å¢žæ·»äº†å¿§éƒå­¤ç‹¬çš„æ°›å›´ã€‚(A
woman dressed in a white shirt walked alone along the street on a rainy
night. Her expression conveyed a sense of loss as she slightly lowered
her head. Raindrops fell gently, forming small puddles on the wet
pavement. The streetlights emitted a soft glow, reflecting off the
raindrops that clung to her hair and clothing. In the background,
blurred city buildings contributed to an atmosphere of melancholy and
solitude. Qwen-Image HunyuanImage3.0 FLUX 2 \[dev\] Seedream4.0 Nano
Banana Pro Z-Image-Turbo Seedream3.0 Imagen4 Ultra Input promp
Lumina-Image 2.0tFigure 19\|Comparison of scene shooting, which
indicates that Z-Image-Turbo shows strong performance in the
authenticity of both the person and the background, as well as the
aesthetic appeal of layout and posture. Better to zoom in to check the
texture of the clothes and hair. 34

An Asian chef is stir-frying at a roadside stall in an old-fashioned
black iron pot, creating a vibrant and lively atmosphere.
(ä¸€ä½äºšæ´²åŽ¨å¸ˆæ­£åœ¨è·¯è¾¹å°æ‘Šä¸Šç”¨è€å¼çš„é»‘
é“é”…ç‚’èœï¼Œè¥é€ å‡ºå……æ»¡æ´»åŠ›å’Œæ´»åŠ›çš„æ°›å›´ã€‚) Qwen-Image HunyuanImage3.0 FLUX
2 \[dev\] Seedream4.0 Nano Banana Pro Z-Image-TurboInput prompt Imagen4
UltraSeedream3.0 Lumina-Image 2.0Figure 20\|Comparison of scene
shooting, which indicates that Z-Image-Turbo shows strong performance in
the authenticity of both the person and the background, as well as the
aesthetic appeal of layout and posture. Better to zoom in to check the
details. 35

é•œå¤´å‰ï¼Œä¸€ä½èº«ç€å°æœ‰ "Z-Image: è®©åˆ›æ„å³åˆ»è½»è£…ä¸Šé˜µ"ç™½ T
æ¤çš„ä¸­å›½å¥³æ•™å¸ˆç¬‘æ„ç›ˆ ç›ˆï¼Œæ‰‹æŒç™½è‰²ç²‰ç¬”ï¼Œå¥¹èº«åŽçš„é»‘æ¿ä¸Šï¼Œå†™ç€: "
Z-Imageæ¨¡åž‹äº®ç‚¹ï¼š1. æ–‡å­—è‰ºæœ¯å®¶ï¼šä¸­ è‹±åŒè¯­ç§’çº§æ¸²æŸ“ï¼ŒæŽ’ç‰ˆè‡ªåŠ¨å¤§ç‰‡çº§ã€‚2.
çœŸå®žæ„Ÿé­”æœ¯å¸ˆï¼šæ‘„å½±çº§åœºæ™¯å¸ƒå±€ï¼Œè¶…çœŸå®žè´¨æ„Ÿï¼Œ ä¸€çœ¼å‘Šåˆ« AI å‘³ã€‚3.
é€Ÿåº¦ç‹‚é­”ï¼šåˆ›æ–°S3-DiTæž¶æž„, 8 æ­¥æŽ¨ç†ï¼Œ6B å‚æ•°ï¼Œå¿«åˆ°è®©ä½ æ€€ç–‘ GPU
å¼€äº†æŒ‚ã€‚"Input prompt Qwen-Image HunyuanImage3.0 Imagen 4 Ultra
Seedream4.0 Nano Banana Pro Z-Image-TurboSeedream3.0 FLUX 2
\[dev\]Figure 21\|Comparison of complex Chinese text rendering. It shows
that only Z-Image-Turbo and Nano Banana Pro can accurately generates the
expected Chinese couplet. Better to zoom in to check the correctness of
the rendered text and the authenticity of the person. 36

A contemporary loft interior with feature wall displaying large
mixed-font typography "Every moment presents an opportunity to create
something meaningful, to inspire others, to leave a legacy that
transcends time and touches the hearts of those around us." against
geometric patterns, industrial aesthetic, natural window lighting.Input
prompt Qwen-Image HunyuanImage3.0 Imagen4 Ultra Seedream4.0 NanoBanana
Pro Z-Image-Turbo Seedream3.0 FLUX 2 \[dev\]Figure 22\|Comparison of
complex English text rendering. It shows that only Z-Image-Turbo and
Nano Banana Pro can accurately generates the expected English couplet.
Better to zoom in to check the correctness of the rendered text and the
layout of the scene. 37

è®¾è®¡ä¸€å¼ åä¸º"é€ ç›¸ç¾Žé£Ÿ"çš„çƒ˜ç„™åº—å•†ä¸šå¹¿å‘Šæµ·æŠ¥ï¼Œä»¥åŒè‰²è°ƒèƒŒæ™¯ï¼ˆä¸Šéƒ¨ä¸ºæµ…æ¡ƒè‰²ï¼Œä¸‹éƒ¨ä¸ºç°çŽ«ç‘°è‰²ï¼‰å±•ç¤º
äº†ä¸‰æ¬¾ä¸åŒçš„æ°´æžœè›‹ç³•ã€‚åœ¨ä¸ŠåŠéƒ¨åˆ†ï¼Œç”¨çº¢æ£•è‰²å¤§å·ä¸­æ–‡å­—ä½“å†™ç€"è½»ç”œ&è½»è´Ÿæ‹…"ã€‚å…¶ä¸‹æ–¹æ˜¯è¾ƒå°çš„ç™½è‰²æ–‡å­—
"(å½“å­£æ–°é²œæ°´æžœ)"ã€‚ä¸»æ ‡é¢˜ä¸‹æ–¹å°æœ‰çº¢æ£•è‰²æ— è¡¬çº¿å­—ä½“çš„è‹±æ–‡"Z-Image
Cake"ã€‚å·¦ä¾§ï¼Œä¸€ä¸ªè£…é¥°ç€çº¢è‰²è¦†ç›†å­ã€
çŸ³æ¦´ç±½å’Œç»¿è‰²å«©æžçš„è›‹ç³•æ”¾åœ¨ä¸€ä¸ªæµ…è‰²æœ¨è´¨åº•åº§ã€‚å³ä¾§ï¼Œå¦ä¸€ä¸ªæ”¾åœ¨æ›´é«˜æœ¨è´¨åº•åº§ä¸Šçš„è›‹ç³•é¡¶éƒ¨æœ‰æ— èŠ±æžœã€è‘¡è„å’Œ
è“èŽ“ã€‚å³ä¸‹æ–¹ï¼Œç¬¬ä¸‰ä¸ªè›‹ç³•æ”¾åœ¨ä¸€ä¸ªç™½è‰²ç›˜å­ä¸Šï¼Œè¦†ç›–ç€æ©™è‰²æŸ¿å­é…±ï¼Œé¡¶éƒ¨æœ‰æŸ¿å­å’Œä¸€æœµå¹²èŠ±ã€‚åœ¨è›‹ç³•ä¹‹é—´ï¼Œä¸€æ¯
çŸ³æ¦´ç±½æ´’è½åœ¨å°é¢ä¸Šã€‚å·¦ä¸‹è§’æœ‰ä¸¤ä¸ªæ–°é²œçš„æŸ¿å­ã€‚æµ·æŠ¥ä¸ºæ¯ä¸ªè›‹ç³•éƒ½é™„æœ‰æè¿°æ€§æ–‡å­—ï¼š"ç•™çº¢çŸ³
Â¥119"ï¼Œé…æ–™ä¸º "é…¸å¥¶èŠå£«å¥¶æ²¹"ï¼Œè§„æ ¼ä¸º"è§„æ ¼: 10.5cm*10.5cm"ï¼›"æœ‰èŠ±é¦™
Â¥139"ï¼Œé…æ–™ä¸º"é…¸å¥¶èŠå£«å¥¶æ²¹Â·æ— èŠ±æžœé…±"ï¼Œ è§„æ ¼ä¸º"è§„æ ¼:
10.5cm*10.5cm"ï¼›ä»¥åŠ"ç§‹æŸ¿æ—© Â¥129"ï¼Œé…æ–™ä¸º"æŸ¿å­å¥¶æ²¹Â·æŸ¿å­é…±"ï¼Œè§„æ ¼ä¸º"è§„æ ¼:
10.5cm\*10.5cm"ã€‚å“ç‰Œå"é€ ç›¸ç¾Žé£Ÿ"ä½äºŽå·¦ä¸‹è§’ã€‚å³ä¸‹è§’å†™ç€"NEW
TASTE"å’Œ"(ç§‹å†¬å°é²œ)"ã€‚è¯¥å›¾åƒæ˜¯ä¸€å¼ 
é«˜è´¨é‡çš„å½±æ£šæ‘„å½±ä½œå“ï¼Œå…‰çº¿æŸ”å’Œæ¸©æš–ï¼Œè¥é€ å‡ºè¯±äººè€Œæ´å‡€çš„ç¾Žæ„Ÿã€‚ Qwen-Image
HunyuanImage3.0 Imagen 4 Ultra Seedream4.0 Z-Image-Turbo Nano Banana
ProInput prompt Seedream3.0 Seedream4.0 FLUX 2 \[dev\]Figure
23\|Comparison of Chinese text rendering in poster design. Z-Image-Turbo
not only presents correct text rendering, but also designs a more
aesthetically pleasing and realistic poster. Better to zoom in to check
the correctness of the rendered text and the fidelity of the food. 38

A vertical graphic design poster for a music event titled "Poly Music
Roaming Plan". The illustration features a surreal scene against a
gradient background that transitions from dark blue at the top to a warm
orange-peach at the bottom, suggesting a night sky over a horizon. In
the lower half, a large, stylized blue bowl-like structure sits on a
dark, wavy surface resembling water. This structure has two small legs
and two blue rings hanging from its sides. On top of the bowl rests a
blue DJ controller with glowing orange and purple lights, and two small
speakers. A blue cable snakes from the controller down towards the left.
The upper half of the poster is dominated by abstract, swirling shapes
in shades of blue, white, and pale orange, creating a sense of movement
and energy. Two bright white circles, representing moons or stars, are
visible in the sky. The entire image has a subtle grainy texture.
Numerous text elements are present. At the top left, the text reads
"Poly Music" in a white pixelated font, followed by "Poly Music" in a
white serif-style font. Below this is "Roaming Plan" in a larger white
serif-style font. To the right, "Roaming Plan" is written in a large,
elegant white script font. On the left side, vertically aligned text
announces the Friday schedule: "Ganboi (Fenkee) 22:00-00:00", "Friday DJ
(kaiwei) 00:00-02:00". On the right side, vertically aligned text
announces the Saturday schedule: "Saturday (Laneta) 22:00-00:00",
"Saturday DJ (Daniel) 00:00- 02:00". In the center of the bowl, there is
a small circular logo containing the word "SCO". At the bottom, contact
information is provided: (Poly Center) Poly Center Store: 18, 3 Jinxiu
Road, Wuhou District, Chengdu â€¢ Call 199-3816-3315. Below this, the word
"Address" is specified. In the bottom left corner is the logo for
"Thousand Cups GrillÂ®". In the bottom right, "Friday Saturday" is
written in a white pixelated font, preceded by a white circle containing
the character "Sun". The core entities identified are the brand Poly
Music, the venue Poly Center, and the DJs Fenkee, kaiwei, Laneta, and
Daniel. Input prompt Qwen-Image HunyuanImage3.0 Imagen 4 Ultra
Seedream4.0 Z-Image-Turbo Nano Banana Pro FLUX 2 \[dev\]
Seedream3.0Figure 24\|Comparison of English text rendering in poster
design. Only Z-Image-Turbo presents correct text rendering with a
pleasing and realistic poster. Better to zoom in to check the
correctness of the rendered text and the details of the poster. 39

èƒŒæ™¯åˆ‡æ¢ä¸ºæ‚‰å°¼æ­Œå‰§é™¢ï¼Œæ‰‹ä¸Š æ‹¿ç€ä¸€ä¸ªç‰Œå­å†™ç€"Z-Image"ï¼Œ åˆ é™¤èƒŒåŒ…ã€‚ The
background changes to the Sydney Opera House, and the person is holding
a sign that reads "Z-Image" while removing his backpack. Input Image
Output Image å·¦è¾¹çš„è¡£æœæ”¹æˆè“è‰²ï¼Œä¸­é—´çš„ è¡£æœæ”¹æˆç»¿è‰²ï¼Œå³è¾¹çš„è¡£æœæ”¹
æˆé»„è‰²ï¼Œä¸‰ä¸ªäººçš„è¡¨æƒ…æ”¹ä¸ºä¸­ æ€§ï¼ŒåŒæ—¶åˆ é™¤æ‰€æœ‰å‘å¡ã€‚ Change the clothes on
the left to blue, the clothes in the middle to green, and the clothes on
the right to yellow. Change the expressions of all three people to
neutral and delete all hair clips. Input Image Output Image
çº¢è‰²æ¡†é‡Œçš„æ–‡å­—ä¿®æ”¹ä¸º"ç”±åœ°"ã€‚ å¤©ç©ºæ”¹ä¸ºæ›´æ™´æœ—çš„è“å¤©ç™½äº‘ï¼Œé˜³ å…‰ç¿çƒ‚ã€‚ The
text in the red box should be changed to "ç”±åœ°". Change it to a clearer
picture of blue sky, white clouds, and bright sunshine. Input Image
Output Image å¥¹çŽ°åœ¨è¹²åœ¨è‰åœ°ä¸Šï¼Œè½»è½»åœ°æŠš æ‘¸ç€ä¸€åªå“ˆå£«å¥‡çŠ¬ï¼ŒèƒŒæ™¯æ˜¯å¼€
é˜”çš„æˆ·å¤–çŽ¯å¢ƒå’ŒæŸ”å’Œçš„å¤©ç©ºï¼Œ åŒæ—¶ä¿æŒç€åŒæ ·çš„è¡£ç€ã€èº«ä»½ã€
ä¸ªæ€§ã€ç§æ—ã€æ€§åˆ«å’Œé¢å®¹ã€‚ She is now crouching on a grassy field, gently
petting a husky dog, with an open outdoor environment and a soft sky in
the background, while keeping the same clothing, identity, personality,
race, gender, and face.Input Image Output ImageFigure 25\|The first two
columns: Mixed-instruction editing across various tasks in
Z-Image-Edit.The last two columns: Text editing (with bounding box) and
identity-preservation editing in Z-Image-Edit. 40

Input: Given that chickens and rabbits are in the same cage, there are a
total of 35 heads and 94 feet. Find the number of chickens and
rabbits.Summarized Reasoning: 1. Analysis & Language Locking: Identified
the user input as a mathematical word problem ("chickens and rabbits")
in English, determining that the output prompt must also be in English.
Recognized the need for "Generative Reasoning" to visualize the solution
rather than just the problem.2. Solution Logic (Generative Reasoning) :
Solved the math problem step-by-step using a system of linear equations
to find the specific values (23 chickens, 12 rabbits). Decided to
visualize this process as a "blackboard derivation" in a classroom
setting, which serves as the logical visual carrier.3. Aesthetic & Scene
Construction: Designed a realistic classroom atmosphere with natural
lighting, specific textures (chalk dust, wood grain), and a shallow
depth of field to focus on the content.4. Text Precision: Transcribed
the exact mathematical steps and final answer (e.g., "2C + 4R = 94", "R
= 12") into explicit text instructions, ensuring all content was
enclosed in double quotes for the image generation model.Output: A
classroom setting with a large green chalkboard on a wooden frame,
illuminated by soft morning light from a window on the left. On the
chalkboard, written in clear white chalk handwriting: "Let C be the
number of chickens", "Let R be the number of rabbits", "C + R = 35",
"2C + 4R = 94", "C = 35 -R", "2(35 -R) + 4R = 94", "70 -2R + 4R = 94",
"70 + 2R = 94", "2R = 24", "R = 12", "C = 35 -12 = 23", "Number of
chickens: 23", "Number of rabbits: 12". A small hand-drawn sketch of a
chicken and a rabbit appears in the bottom right corner. Foreground
features an empty oak desk and chair, slightly out of focus. Dust
particles float in the light beam. Textured chalkboard surface shows
subtle chalk dust residue. Wooden frame has visible grain. Color scheme:
muted greens, warm browns, and crisp white. Shallow depth of field
emphasizes the chalkboard.Before PEAfter PEFigure 26\|Showcases of
prompt enhancer for logical reasoning. 41

Input: å¸®æˆ‘ç»™ã€Šç™»ç§‘åŽã€‹é…å›¾ï¼Œæœ€å‡ºåçš„ä¸¤å¥Summarized Reasoning: 1.
è¯­è¨€ä¸Žæ ¸å¿ƒé”å®š(Language & Core
Locking)ï¼šç¡®è®¤è¾“å…¥ä¸ºä¸­æ–‡ï¼Œé”å®šè¾“å‡ºå¿…é¡»ä¸ºä¸­æ–‡ã€‚æ˜Žç¡®æ ¸å¿ƒé¢˜æä¸ºå”è¯—ã€Šç™»ç§‘åŽã€‹ï¼Œå¹¶é”å®šæœ€è‘—åçš„ä¸¤å¥è¯—æ–‡å†…å®¹ï¼ˆ"æ˜¥é£Žå¾—æ„é©¬è¹„ç–¾ï¼Œä¸€æ—¥çœ‹å°½é•¿å®‰èŠ±"ï¼‰ã€‚2.
ç”Ÿæˆå¼æŽ¨ç†ä¸Žç”»é¢è½¬åŒ–(Generative
Reasoning)ï¼šå°†æŠ½è±¡çš„è¯—æ„ï¼ˆå¾—æ„ã€å–œæ‚¦ï¼‰è½¬åŒ–ä¸ºå…·è±¡çš„è§†è§‰åœºæ™¯ï¼šä¸€ä½èº«ç€çº¢è¢çš„æ–°ç§‘è¿›å£«ï¼Œåœ¨æ˜¥æ—¥é˜³å…‰ä¸‹éª‘é©¬ç–¾é©°ç©¿è¿‡é•¿å®‰è¡—é“ï¼Œå‘¨å›´èŠ±å›¢é”¦ç°‡ã€‚3.
ç¾Žå­¦ä¸Žç»†èŠ‚è®¾å®š(Aesthetic & Detail
Injection)ï¼šè®¾å®šæ°›å›´ä¸ºæ˜Žäº®ã€æ¬¢å¿«çš„æ˜¥æ—¥åˆåŽã€‚ç»†åŒ–å…ƒç´ åŒ…æ‹¬å”ä»£æœ¨è´¨å»ºç­‘ã€ä¸ç»¸æœé¥°è´¨æ„Ÿã€ç²‰ç™½èŠ±æœµä¸Žé£žæ‰¬çš„å°˜åœŸï¼Œæž„å»ºå¯Œæœ‰å±‚æ¬¡çš„ç©ºé—´æ„Ÿã€‚4.
æ–‡å­—å…ƒç´ çš„ç²¾ç¡®æ¤å…¥(Text
Transcription)ï¼šä¸ºäº†å¿ å®žè¿˜åŽŸè¯—æ„ï¼Œå†³å®šå°†è¯—å¥ä½œä¸ºè§†è§‰å…ƒç´ ï¼ˆä¹¦æ³•å·è½´ï¼‰ç½®äºŽç”»é¢åº•éƒ¨ã€‚ä¸¥æ ¼éµå®ˆæŒ‡ä»¤ï¼Œå°†è¯—å¥å†…å®¹ä¸€å­—ä¸å·®åœ°è½¬å½•å¹¶ä½¿ç”¨è‹±æ–‡åŒå¼•å·åŒ…è£¹ï¼ˆ""æ˜¥é£Žå¾—æ„é©¬è¹„ç–¾ï¼Œä¸€æ—¥çœ‹å°½é•¿å®‰èŠ±ã€‚""ï¼‰ã€‚Output:
ä¸€ä½å¹´è½»ç”·å­èº«ç€å”ä»£çº¢è‰²ä¸ç»¸å®˜è¢ï¼Œéª‘åœ¨ä¸€åŒ¹æ£•è‰²éªé©¬ä¸Šï¼Œé©¬è¹„è½»å¿«è¸è¿‡é•¿å®‰åŸŽè¡—é“çŸ³æ¿è·¯ï¼Œç”·å­é¢å®¹æ¬£å–œï¼Œç›®å…‰å‰è§†ï¼Œæ˜¥é£Žæ‹‚åŠ¨å…¶è¡£è¢–ä¸Žé©¬é¬ƒã€‚èƒŒæ™¯ä¸ºå¤ä»£é•¿å®‰è¡—æ™¯ï¼Œä¸¤æ—æŽ’åˆ—ç²‰ç™½è‰²ç››å¼€çš„æ¡ƒæ ‘ï¼ŒèŠ±ç“£éšé£Žé£˜è½ï¼Œè¿œå¤„å¯è§é’ç“¦æœ¨æž„å»ºç­‘ã€åŸŽæ¥¼ä¸Žå•†é“ºæ——å¸œï¼Œè¡—è¾¹æœ‰æ•°åå”ä»£æœé¥°çš„æ°‘ä¼—é©»è¶³è§‚çœ‹ã€‚æ˜¥æ—¥åˆåŽé˜³å…‰æ˜Žäº®ï¼ŒæŸ”å’Œé˜´å½±æŠ•åœ¨åœ°é¢ï¼Œæ•´ä½“è‰²å½©ä»¥æš–è‰²è°ƒä¸ºä¸»ï¼šè“å¤©ã€å«©ç»¿æŸ³æžã€çº¢è¢ä¸Žæ£•é©¬å½¢æˆå¯¹æ¯”ï¼Œæè´¨ç»†èŠ‚åŒ…æ‹¬ä¸ç»¸å…‰æ³½ã€é©¬åŒ¹æ¯›å‘çº¹ç†ã€æœ¨è´¨å»ºç­‘ç²—ç³™è¡¨é¢åŠèŠ±ç“£æŸ”è½¯è´¨æ„Ÿã€‚ç”»é¢åº•éƒ¨ä¸­å¤®ï¼Œä¸€ä¸ªå±•å¼€çš„ç±³ç™½è‰²å®£çº¸å·è½´ï¼Œå®½åº¦å ç”»é¢ä¸‰åˆ†ä¹‹ä¸€ï¼Œä¸Šé¢ä»¥é»‘è‰²æ¯›ç¬”ä¹¦æ³•å­—ä½“æ¸…æ™°å†™ç€ï¼š"æ˜¥é£Žå¾—æ„é©¬è¹„ç–¾ï¼Œä¸€æ—¥çœ‹å°½é•¿å®‰èŠ±ã€‚"å·è½´è¾¹ç¼˜æœ‰ç«¹åˆ¶è½´æ†ï¼Œå›ºå®šäºŽåœ°é¢ã€‚æž„å›¾é‡‡ç”¨ä¸­æ™¯è§†è§’ï¼Œç„¦ç‚¹åœ¨éª‘æ‰‹ä¸Žé©¬åŒ¹ï¼Œç©ºé—´å±‚æ¬¡åˆ†æ˜Žï¼Œå‰æ™¯ä¸ºé£˜è½èŠ±ç“£ï¼Œä¸­æ™¯è¡—é“ï¼ŒèƒŒæ™¯å»ºç­‘æ¸éšã€‚Before
PEAfter PEFigure 27\|Showcases of prompt enhancer for world knowledge
injection. Given the poem title"After Passing the Imperial Examination"
( ã€Šç™»ç§‘åŽã€‹ ), the baseline (Left) lacks cultural context. Our method
(Right) leverages LLM priors to retrieve specific historical details
(e.g., the galloping horse, red official robe) and the famous couplet: "
æ˜¥é£Žå¾—æ„é©¬è¹„ç–¾ï¼Œä¸€æ—¥çœ‹å°½é•¿å®‰èŠ±ã€‚ ", the reasoning module (center)
translates these literary semantics into visual cues, ensuring a
culturally faithful rendering with precise text transcription. 42

Input: è¡¨æƒ…åŒ…ï¼ˆEmoticons ï¼‰Reasoning:1.Source Analysis: A little girl
stands in an outdoor park, with blurred trees and green vegetation in
the background. She has long, black hair with bangs neatly covering her
forehead, and a silver butterfly hair clip on the left side of her hair.
She wears a light-colored striped sundress with small button
decorations. Her expression is calm, her eyes look directly at the
camera, her facial features are clear, and her skin is smooth. The
overall image is realistic, using natural light that is soft and even,
creating a tranquil and refreshing atmosphere. The composition is
centrally symmetrical, focusing on the little girl's face, with a
blurred background that emphasizes the subject. The image has a cool
color tone, mainly gray-green and light blue, creating a quiet visual
experience.2.Edit Plan: Preserve the little girl's overall appearance,
hairstyle, clothing, and background environment, maintaining the
original natural lighting and composition. Modify the little girl's
expression to an exaggerated, humorous one, such as opening her mouth
wide, staring, or making a comical tongue-out gesture, to conform to
common emoji styles. Ensure the new expression seamlessly integrates
with the original character features, without altering other details
such as hair accessories, clothing textures, and the background.Output:
The little girl's facial expressions transform into exaggerated, comical
gestures, such as a wide-open mouth, bulging eyes, or sticking out her
tongue, conveying a humorous and playful mood. Despite the change in
expression, her original long hair, butterfly hair clip, striped
sundress, and the trees and greenery in the background are retained. The
overall style shifts from the original realistic tranquility to a
comedic, meme-like style, but the character's posture, clothing details,
and background environment remain unchanged. The image continues to use
natural lighting and maintains a cool color palette, but the dynamic
changes in facial expression become the visual focus, highlighting its
fun and viral potential as a meme. Input ImageOutput Image (Before
PE)Output Image (After PE)Figure 28\|Showcases of prompt enhancer in
image editing for handling ambiguous and unclear instruc- tions. 43

Input: åšä¸€é“èœï¼ˆMake a dish ï¼‰Reasoning:1.Source Analysis: The kitchen
countertop is laden with ingredients, including a cracked egg, the yolk
still intact in half the shell, the other shell lying empty beside it.
Next to it is a bright red, plump tomato, glistening with moisture, its
green stem clearly visible. Several scallions are neatly arranged in the
background, their stems vibrant green and leaves slender. The countertop
is made of wood with a warm brown hue. In the background, a stainless
steelstovetop, white cabinets, and the sink area are visible. The
overall environment is clean and tidy, with bright, natural lighting.
Sidelight from the window creates a soft glow on the food, creating a
fresh, pre-cooking atmosphere.2.Edit Plan: Remove the tomatoes,
scallions, and eggs (including shells and yolks) from the original
image, while preserving the kitchen background, wooden countertop, and
overall lighting effects. Add a cooked dish, specifically scrambled eggs
with tomatoes, to the original ingredients' locations, placed in the
center of a white ceramic plate. The new dish should have a texture of
golden-yellow egg pieces mixed with red tomato chunks, garnished with
scallions, and slightly glossy to convey a steaming appearance.Output:
The original ingredients (tomatoes, scallions, and eggs) have been
removed, leaving only the original kitchen background, the fragrant
countertop, and the natural, pristine conditions. In the center of the
foreground is a white ceramic plate containing steaming hot scrambled
eggs with tomatoes. The tomato chunks are golden yellow, the diced
tomatoes are red and juicy, and the scallions are bright green on the
countertop. The surface is slightly glistening with oil, giving the
impression that the dish was just cooked. Input ImageOutput Image
(Before PE)Output Image (After PE)Figure 29\|Showcases of prompt
enhancer in image editing for world knowledge injection and reasoning.
44

Prompt: This is a portrait of a cheerful young Australian woman standing
on the edge of the Sydney Opera House. She is positioned in the center
right of the frame, a genuine, friendly, and radiant smile on her face,
her eyes filled with the relaxed warmth characteristic of Australians.
Her long, golden-brown hair is gently blowing in the sea breeze, and she
wears a simple white summer dress. She holds a small flag with the word
"G'day" casually handwritten in white paint. Prompt: Retrato de uma
jovem angolana de pele escura, cabelo curto e crespo, vestindo uma blusa
colorida de padrÃ£o africano e Ã³culos de sol empurrados para cima. Ela
estÃ¡ na Avenida 4 de Fevereiro, em Luanda, segurando com as duas mÃ£os
uma placa de cortiÃ§a com moldura de madeira rÃºstica. Com giz branco,
escreveu "OlÃ¡", letras grandes e um pouco tortas, como se risse ao
escrever. Ao fundo, o oceano AtlÃ¢ntico reflete o cÃ©u azul claro e barcos
de pesca distantes. Seus olhos brilham, sorriso largo e autÃªntico.
Prompt: ÐœÐ¾Ð»Ð¾Ð´Ð°Ñ Ñ€ÑƒÑÑÐºÐ°Ñ Ð¶ÐµÐ½Ñ‰Ð¸Ð½Ð° Ñ Ð´Ð»Ð¸Ð½Ð½Ñ‹Ð¼Ð¸ ÑÐ²ÐµÑ‚Ð»Ñ‹Ð¼Ð¸ Ð²Ð¾Ð»Ð¾ÑÐ°Ð¼Ð¸ Ð² Ð±ÐµÐ»Ð¾Ð¹
ÑˆÐ°Ð¿ÐºÐµ-ÑƒÑˆÐ°Ð½ÐºÐµ ÑÑ‚Ð¾Ð¸Ñ‚ Ð½Ð° ÐšÑ€Ð°ÑÐ½Ð¾Ð¹ Ð¿Ð»Ð¾Ñ‰Ð°Ð´Ð¸ Ð² Ð»ÐµÐ³ÐºÐ¾Ð¼ ÑÐ½ÐµÐ³Ð¾Ð¿Ð°Ð´Ðµ. ÐžÐ½Ð° ÑƒÐ»Ñ‹Ð±Ð°ÐµÑ‚ÑÑ
Ñ‚ÐµÐ¿Ð»Ð¾, Ð´ÐµÑ€Ð¶Ð° Ð¿ÐµÑ€ÐµÐ´ ÑÐ¾Ð±Ð¾Ð¹ Ð´ÐµÑ€ÐµÐ²ÑÐ½Ð½ÑƒÑŽ Ñ€Ð°Ð¼Ñƒ Ñ Ð¿Ñ€Ð¾Ð±ÐºÐ¾Ð²Ð¾Ð¹ Ð´Ð¾ÑÐºÐ¾Ð¹, Ð½Ð° ÐºÐ¾Ñ‚Ð¾Ñ€Ð¾Ð¹
Ð±ÐµÐ»Ñ‹Ð¼ Ð¼ÐµÐ»Ð¾Ð¼ Ð²Ñ‹Ð²ÐµÐ´ÐµÐ½Ð¾ Â«Ð—Ð´Ñ€Ð°Ð²ÑÑ‚Ð²ÑƒÐ¹Ñ‚ÐµÂ». Ð‘ÑƒÐºÐ²Ñ‹ ÐºÑ€ÑƒÐ¿Ð½Ñ‹Ðµ, Ð½ÐµÐ¼Ð½Ð¾Ð³Ð¾ Ñ€Ð°Ð·Ð¼Ñ‹Ñ‚Ñ‹Ðµ ---
ÐºÐ°Ðº Ð¾Ñ‚ Ñ‚ÐµÐ¿Ð»Ñ‹Ñ… Ñ€ÑƒÐº. Ð¡Ð¾Ð±Ð¾Ñ€ Ð’Ð°ÑÐ¸Ð»Ð¸Ñ Ð‘Ð»Ð°Ð¶ÐµÐ½Ð½Ð¾Ð³Ð¾ Ð¿Ð¾Ð·Ð°Ð´Ð¸ Ð½ÐµÑ‘ ÑÑ€ÐºÐ¾ Ð²Ñ‹Ð´ÐµÐ»ÑÐµÑ‚ÑÑ
Ð½Ð° Ñ„Ð¾Ð½Ðµ ÑÐµÑ€Ð¾Ð³Ð¾ Ð·Ð¸Ð¼Ð½ÐµÐ³Ð¾ Ð½ÐµÐ±Ð°. Ð•Ðµ Ð²Ð·Ð³Ð»ÑÐ´ Ð´Ñ€ÑƒÐ¶ÐµÐ»ÑŽÐ±Ð½Ñ‹Ð¹, Ð¸ÑÐºÑ€ÐµÐ½Ð½Ð¸Ð¹. Prompt:
ì„œìš¸íƒ€ì›Œ ì „ë§ëŒ€ì—ì„œ ì Šì€ í•œêµ­ ì—¬ì„±ì´ ë°”ëžŒì— ë‚ ë¦¬ëŠ” ì•žë¨¸ë¦¬ ë¥¼ ë¶€ë“œëŸ½ê²Œ
ë§¤ë§Œì§€ë©° í™˜í•˜ê²Œ ì›ƒê³  ìžˆì—ˆë‹¤. ê·¸ë…€ëŠ” "ì•ˆë…•í•˜ì„¸ìš”"ë¼ê³  ì“°ì¸ íŒ»ë§ì„ ë“¤ê³ 
ìžˆì—ˆëŠ”ë°, ê¸€ì”¨ëŠ” ì•½ê°„ ê¸°ìš¸ì–´ì¡Œì§€ë§Œ ìƒë™ê°ì´ ë„˜ì³¤ ë‹¤. ê·¸ë…€ ë’¤ë¡œ ë„ì‹œê°€
íŽ¼ì³ì¡Œê³ , ê·¸ ë…€ì˜ ëˆˆì€ ë°˜ì§ì´ë©°, ê·¸ë…€ì˜ í‘œì •ì€ í™˜í•˜ê³ 
ë§¤í˜¹ì ì´ì—ˆë‹¤.Prompt: A young Black American woman with curly dark brown
hair tied in a high puff stands on Liberty Island, slightly right of
center, smiling warmly at the camera. She holds up a vintage-style
illuminated sign with glowing white LED letters spelling "Hello" inside
a black metal frame. Behind her, the Statue of Liberty rises against a
soft blue sky, sunlight glinting off its copper-green torch and crown.
Red brick pathways lead toward the pedestal, and small American flags
flutter on poles nearby. The calm waters of New York Harbor stretch into
the distance, reflecting clouds and skyline. Shallow depth of field
keeps her face and the lit sign in sharp focus, while the background
softly blurs. Portrait photography, natural daylight, joyful expression,
cultural diversity, iconic landmark. Prompt: Una joven espaÃ±ola con
rizos castaÃ±os y abrigo rojo oscuro rÃ­e con espontaneidad, mostrando sus
dientes blancos, mientras levanta una pequeÃ±a pizarra de corcho con la
palabra "Hola" escrita en tiza blanca. EstÃ¡ de pie frente a la Plaza de
Cibeles, el sol del atardecer ilumina su perfil. Una mano sostiene la
pizarra, la otra se acerca a su boca como si contuviera una risa.
ExpresiÃ³n alegre, natural, llena de energÃ­a mediterrÃ¡nea.Prompt: Eine
frÃ¶hliche junge Deutsche mit kurzen blonden Haaren und einer grauen
WollmÃ¼tze hÃ¤lt lÃ¤ssig ein Holzschild hoch, auf dem in ordentlicher, aber
nicht ganz perfekter Handschrift â€žHallo" steht. Im Sonnenlicht ist
hinter ihr das Brandenburger Tor deutlich zu sehen. Ihre Wangen sind
leicht gerÃ¶tet, ihre Augen weit geÃ¶ffnet, und ihr LÃ¤cheln wirkt
natÃ¼rlich, als begrÃ¼ÃŸe sie eine alte Freundin. Prompt: A young British
woman with light brown hair in a low ponytail stands on the South Bank
of the Thames, laughing lightly as she holds up a small rectangular cork
board framed in natural wood. On it, "Hello" is written in bold white
chalk, slightly smudged at the edges. Tower Bridge glows behind her in
golden hour light. Her eyes crinkle with joy, one hand gripping the
board, the other gently touching her cheek. Medium close-up, shallow
depth of field, tourism-poster warmth. Prompt: ä¸€ä½å¹´è½»æ±‰æ—å¥³æ€§èº«ç©¿
çº¢ç™½ç›¸é—´ã€é¥°æœ‰é‡‘è‰²èŠ±å‰åˆºç»£çš„ æ±‰æœï¼ŒèƒŒåŽè¿œæ™¯æ˜¯æ•…å®«ã€‚ä¹Œé»‘é•¿
å‘æŒ½æˆç²¾è‡´å‘é«»ï¼Œç‚¹ç¼€çç å‘ç°ª ä¸Žé‡‘é¥°ã€‚å¥¹é¢å¸¦æ¸©æš–å¾®ç¬‘æœ›å‘é•œ
å¤´ï¼ŒåŒæ‰‹ä¸¾èµ·çº¸å·ï¼Œä¸Šé¢ç”¨æ¯›ç¬” å†™ç€"ä½ å¥½"ã€‚å¥¹èº«åŽæ˜¯æœ±çº¢è‰²
å®«å¢™ä¸Žé‡‘é»„è‰²ç‰ç’ƒç“¦é¡¶ï¼Œä¼ ç»Ÿçº¢ ç¯ç¬¼æ‚¬æŒ‚åœ¨çŸ³æŸ±å»Šä¸‹ï¼Œæ—¥å…‰æŸ”å’Œã€‚
å…‰æ»‘çš„é’çŸ³åœ°é¢å¾®å¾®åå…‰ï¼Œå¢žæ·» ç©ºé—´å±‚æ¬¡ã€‚æµ…æ™¯æ·±ç¡®ä¿äººç‰©é¢éƒ¨
ä¸Žæ–‡å­—æ¸…æ™°å¯¹ç„¦ã€‚ Prompt: Une jeune femme parisienne aux boucles chÃ¢tains
se tient sur le Champ de Mars, souriant avec sincÃ©ritÃ© tout en levant
une ardoise en liÃ¨ge encadrÃ©e de bois clair. Ã€ la craie blanche, on peut
lire Â« Bonjour Â», Ã©crit en lettres cursives Ã©lÃ©gantes mais lÃ©gÃ¨rement
irrÃ©guliÃ¨res. La Tour Eiffel brille derriÃ¨re elle sous un ciel bleu
pÃ¢le. Elle incline lÃ©gÃ¨rement la tÃªte, une main tenant fermement le
cadre, l'Å“il pÃ©tillant. Portrait naturel, lumiÃ¨re douce du matin.Figure
30\|Emerging Multi-lingual and Multi-cultural Understanding Capacity of
Z-Image-Turbo. It shows that Z-Image-Turbo can not only understand
prompts in multiple languages but also leverage its world knowledge to
generate images that align with local cultures and landmarks. 45

6.  Conclusion In this report, we introduce theZ-Image series, a suite
    of high-performance 6B-parameter models built upon a Scalable
    Single-Stream Diffusion Transformer (S3-DiT). Challenging the
    prevailing "scale-at- all-costs" paradigm, we propose a holistic
    end-to-end solution anchored by four strategic pillars: (1) a
    curated, efficient data infrastructure; (2) a scalable single-stream
    architecture; (3) a streamlined training strategy; and (4) advanced
    optimization techniques for high-quality and efficient inference,
    encompassing PE- aware supervised fine-tuning, few-step
    distillation, and reward post-training. This synergy allows us to
    complete the entire workflowwithin 314K H800 GPU hoursat a total
    cost ofunder \$630K, delivering top-tier photorealistic synthesis
    and bilingual text rendering. Beyond the robust base model, our
    pipeline yieldsZ-Image-T urbo, which enablessub-second inference
    (\<1s)on an enterprise-grade H800 GPU and fitscomfortably within 16G
    VRAM consumer-grade hardware. Additionally, we developZ-Image-Edit,
    an editing modelefficiently derivedvia our omni-pretraining
    paradigm. Through this pipeline, we provide the community with a
    blueprint for developing accessible, budget-friendly, yet
    state-of-the-art generative models.
7.  Authors 7.1. Core Contributors5 Huanqia Cai, Sihan Cao, Ruoyi Du,
    Peng Gao, Steven Hoi, Shijie Huang, Zhaohui Hou, Dengyang Jiang, Xin
    Jin, Liangchen Li, Zhen Li, Zhong-Yu Li, David Liu, Dongyang Liu,
    Junhan Shi, Qilong Wu, Feng Yu, Chi Zhang, Shifeng Zhang, Shilin
    Zhou 7.2. Contributors6 Chenglin Cai, Yujing Dou, Yan Gao, Minghao
    Guo, Songzhi Han, Wei Hu, Yuyan Huang, Xu Li, Zefu Li, Heng Lin,
    Linhong Luo, Qingqing Mao, Jingyuan Ni, Chuan Qin, Lin Qu, Jinghua
    Sun, Peng Wang, Ping Wang, Shanshan Wang, Xuecong Wang, Yi Wang, Yue
    Wang, Tingkun Wen, Junde Wu, Minggang Wu, Xiongwei Wu, Yi Xin, Haibo
    Xing, Xiaoxiao Xu, Ze Xu, Xunliang Yang, Shuting Yu, Yucheng Zhao,
    Jianan Zhang, Jianfeng Zhang, Jiawei Zhang, Qiang Zhang, Xudong
    Zhao, Yu Zheng, Haijian Zhou, Hanzhang Zhou 5Core Contributors are
    listed in alphabetical order of the last name. 6Contributors are
    listed in alphabetical order of the last name. 46

References \[1\]Jason Ansel, Edward Yang, Horace He, Natalia Gimelshein,
Animesh Jain, Michael Voznesensky, Bin Bao, Peter Bell, David Berard,
Evgeni Burovski, et al.Â Pytorch 2: Faster machine learning through
dynamic python bytecode transformation and graph compilation.
InProceedings of the 29th ACM International Conference on Architectural
Support for Programming Languages and Operating Systems, Volume 2, pages
929--947, 2024. \[2\]Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang,
Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et
al.Â Qwen2.5-vl technical report.arXiv preprint arXiv:2502.13923, 2025.
\[3\]Jason Baldridge, Jakob Bauer, Mukul Bhutani, Nicole Brichtova,
Andrew Bunner, Lluis Castre- jon, Kelvin Chan, Yichang Chen, Sander
Dieleman, Yuqing Du, et al.Â Imagen 3.arXiv preprint arXiv:2408.07009,
2024. \[4\]James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng
Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et
al.Â Improving image generation with better captions.Computer Science.
https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. \[5\]Tim
Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix:
Learning to follow im- age editing instructions. InProceedings of the
IEEE/CVF conference on computer vision and pattern recognition, pages
18392--18402, 2023. \[6\]Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav
Shyam, Girish Sastry, Amanda Askell, et al.Â Language models are few-shot
learners.Advances in neural information processing systems,
33:1877--1901, 2020. \[7\]Qi Cai, Yehao Li, Yingwei Pan, Ting Yao, and
Tao Mei. Hidream-i1: An open-source high-efficient image generative
foundation model. InProceedings of the 33rd ACM International Conference
on Multimedia, pages 13636--13639, 2025. \[8\]Siyu Cao, Hangting Chen,
Peng Chen, Yiji Cheng, Yutao Cui, Xinchi Deng, Ying Dong, Kipper Gong,
Tianpeng Gu, Xiusen Gu, et al.Â Hunyuanimage 3.0 technical report.arXiv
preprint arXiv:2509.23951, 2025. \[9\]Jingjing Chang, Yixiao Fang, Peng
Xing, Shuhan Wu, Wei Cheng, Rui Wang, Xianfang Zeng, Gang Yu, and
Hai-Bao Chen. Oneig-bench: Omni-dimensional nuanced evaluation for image
generation. arXiv preprint arXiv:2506.07977, 2025. \[10\] Jingye Chen,
Yupan Huang, Tengchao Lv, Lei Cui, Qifeng Chen, and Furu Wei.
Textdiffuser-2: Unleashing the power of language models for text
rendering. InEuropean Conference on Computer Vision, pages 386--402.
Springer, 2024. \[11\] Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu,
Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio
Savarese, et al.Â Blip3-o: A family of fully open unified multimodal
models-architecture, training and dataset.arXiv preprint
arXiv:2505.09568, 2025. \[12\] Junsong Chen, Chongjian Ge, Enze Xie, Yue
Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and
Zhenguo Li. Pixart- ðœŽ: Weak-to-strong training of diffusion transformer
for 4k text-to-image generation. InEuropean Conference on Computer
Vision, pages 74--91. Springer, 2024. \[13\] Junsong Chen, YU Jincheng,
GE Chongjian, Lewei Yao, Enze Xie, Zhongdao Wang, James Kwok, Ping Luo,
Huchuan Lu, and Zhenguo Li. Pixart- ð›¼: Fast training of diffusion
transformer for photo- realistic text-to-image synthesis. InThe Twelfth
International Conference on Learning Representations. \[14\] Xiaokang
Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai
Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and
generation with data and model scaling.arXiv preprint arXiv:2501.17811,
2025. \[15\] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li,
Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et
al.Â Emerging properties in unified multimodal pretraining, 2025. URL
https://arxiv. org/abs/2505.14683. 47

\[16\] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da
Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, et al.Â Cogview:
Mastering text-to-image generation via transformers. Advances in neural
information processing systems, 34:19822--19835, 2021. \[17\] Nikai Du,
Zhennan Chen, Zhizhou Chen, Shan Gao, Xi Chen, Zhengkai Jiang, Jian
Yang, and Ying Tai. Textcrafter: Accurately rendering multiple texts in
complex visual scenes.arXiv preprint arXiv:2503.23461, 2025. \[18\]
Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas
MÃ¼ller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic
Boesel, et al.Â Scaling rectified flow transformers for high-resolution
image synthesis. InProceedings of the International Conference on
Machine Learning (ICML), 2024. \[19\] Rongyao Fang, Aldrich Yu, Chengqi
Duan, Linjiang Huang, Shuai Bai, Yuxuan Cai, Kun Wang, Si Liu, Xihui
Liu, and Hongsheng Li. Flux-reason-6m and prism-bench: A million-scale
text-to- image reasoning dataset and comprehensive benchmark.arXiv
preprint arXiv:2509.09680, 2025. \[20\] Peng Gao, Le Zhuo, Chris Liu, ,
Ruoyi Du, Xu Luo, Longtian Qiu, Yuhang Zhang, et al.Â Lumina- t2x:
Transforming text into any modality, resolution, and duration via
flow-based large diffusion transformers.arXiv preprint arXiv:2405.05945,
2024. \[21\] Yu Gao, Lixue Gong, Qiushan Guo, Xiaoxia Hou, Zhichao Lai,
Fanshi Li, Liang Li, Xiaochen Lian, Chao Liao, Liyang Liu, et
al.Â Seedream 3.0 technical report.arXiv preprint arXiv:2504.11346, 2025.
\[22\] Zigang Geng, Yibing Wang, Yeyao Ma, Chen Li, Yongming Rao,
Shuyang Gu, Zhao Zhong, Qinglin Lu, Han Hu, Xiaosong Zhang, et
al.Â X-omni: Reinforcement learning makes discrete autoregressive image
generative models great again.arXiv preprint arXiv:2507.22058, 2025.
\[23\] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval:
An object-focused framework for evaluating text-to-image
alignment.Advances in Neural Information Processing Systems, 36:52132--
52152, 2023. \[24\] Spyros Gidaris and Nikos Komodakis. Dynamic few-shot
visual learning without forgetting. In Proceedings of the IEEE
conference on computer vision and pattern recognition, pages 4367--4375,
2018. \[25\] Google. Gemini 2.5 flash & 2.5 flash image model card.
https://storage.googleapis.com/d
eepmind-media/Model-Cards/Gemini-2-5-Flash-Model-Card.pdf, 2025. \[26\]
Google. Imagen 4 model card.
https://storage.googleapis.com/deepmind-media/Mod
el-Cards/Imagen-4-Model-Card.pdf, 2025. \[27\] Google. Nano banana pro.
https://storage.googleapis.com/deepmind-media/Model-C
ards/Gemini-3-Pro-Image-Model-Card.pdf, 2025. \[28\] Jian Han, Jinlai
Liu, Yi Jiang, Bin Yan, Yuqi Zhang, Zehuan Yuan, Bingyue Peng, and
Xiaobing Liu. Infinity: Scaling bitwise autoregressive modeling for
high-resolution image synthesis. In Proceedings of the Computer Vision
and Pattern Recognition Conference, pages 15733--15744, 2025. \[29\]
Jonathan Ho and Tim Salimans. Classifier-free diffusion
guidance.Advances in Neural Information Processing Systems Workshops
(NeurIPS Workshops), 2021. \[30\] Xiwei Hu, Rui Wang, Yixiao Fang, Bin
Fu, Pei Cheng, and Gang Yu. Ella: Equip diffusion models with llm for
enhanced semantic alignment.arXiv preprint arXiv:2403.05135, 2024.
\[31\] Ideogram. Ideogram v3.https://ideogram.ai, 2025. \[32\] Dengyang
Jiang, Dongyang Liu, Zanyi Wang, Qilong Wu, Xin Jin, David Liu, Zhen Li,
Mengmeng Wang, Peng Gao, and Harry Yang. Distribution matching
distillation meets reinforcement learning. arXiv preprint
arXiv:2511.13649, 2025. \[33\] Tero Karras, Miika Aittala, Jaakko
Lehtinen, Janne Hellsten, Timo Aila, and Samuli Laine. Analyzing and
improving the training dynamics of diffusion models. InProceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pages 24174--24184, 2024. 48

\[34\] Kuaishou Kolors Team. Kolors 2.0.https://app.klingai.com/cn/,
2025. \[35\] Black Forest Labs.
Flux.https://github.com/black-forest-labs/flux, 2023. \[36\] Black
Forest Labs. FLUX.2: State-of-the-Art Visual Intelligence.
https://bfl.ai/blog/flux-2 , 2025. \[37\] Black Forest Labs, Stephen
Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril
Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, et
al.Â Flux. 1 kontext: Flow matching for in-context image generation and
editing in latent space.arXiv preprint arXiv:2506.15742, 2025. \[38\]
LeaderGPU. Gpu server rental pricing. https://www.leadergpu.com/ , 2025.
Accessed: November 2025. \[39\] Daiqing Li, Aleks Kamko, Ehsan Akhgari,
Ali Sabet, Linmiao Xu, and Suhail Doshi. Playground v2. 5: Three
insights towards enhancing aesthetic quality in text-to-image
generation.arXiv preprint arXiv:2402.17245, 2024. \[40\] Yuhan Li,
Xianfeng Tan, Wenxiang Shang, Yubo Wu, Jian Wang, Xuanhong Chen, Yi
Zhang, Hangcheng Zhu, and Bingbing Ni. Ragdiffusion: Faithful cloth
generation via external knowledge assimilation. InProceedings of the
IEEE/CVF International Conference on Computer Vision, pages
17485--17495, 2025. \[41\] Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng
Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao Liu, Minbin
Huang, Zedong Xiao, et al.Â Hunyuan-dit: A powerful multi-resolution
diffusion transformer with fine-grained chinese understanding.arXiv
preprint arXiv:2405.08748, 2024. \[42\] Zhong-Yu Li, Ruoyi Du, Juncheng
Yan, Le Zhuo, Zhen Li, Peng Gao, Zhanyu Ma, and Ming-Ming Cheng.
Visualcloze: A universal image generation framework via visual
in-context learning. In Proceedings of the IEEE International Conference
on Computer Vision (ICCV), 2025. \[43\] Zongjian Li, Zheyuan Liu, Qihui
Zhang, Bin Lin, Shenghai Yuan, Zhiyuan Yan, Yang Ye, Wangbo Yu, Yuwei
Niu, and Li Yuan. Uniworld-v2: Reinforce image editing with diffusion
negative-aware finetuning and mllm implicit feedback.arXiv preprint
arXiv:2510.16888, 2025. \[44\] Bin Lin, Zongjian Li, Xinhua Cheng, Yuwei
Niu, Yang Ye, Xianyi He, Shenghai Yuan, Wangbo Yu, Shaodong Wang,
Yunyang Ge, et al.Â Uniworld: High-resolution semantic encoders for
unified visual understanding and generation.arXiv preprint
arXiv:2506.03147, 2025. \[45\] Yaron Lipman, Ricky TQ Chen, Heli
Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative
modeling.arXiv preprint arXiv:2210.02747, 2022. \[46\] Dongyang Liu,
David Liu, Peng Gao, Ruoyi Du, Zhen Li, Qilong Wu, Xin Jin, Sihan Cao,
Shifeng Zhang, Hongsheng Li, and Steven Hoi. Decoupled dmd: Cfg
augmentation as the spear, distribu- tion matching as the shield.arXiv
preprint, 2025. \[47\] Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui
Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, et
al.Â Step1x-edit: A practical framework for general image editing. arXiv
preprint arXiv:2504.17761, 2025. \[48\] Xingchao Liu, Chengyue Gong, and
Qiang Liu. Flow straight and fast: Learning to generate and transfer
data with rectified flow.arXiv preprint arXiv:2209.03003, 2022. \[49\]
Yiting Lu, Jiakang Yuan, Zhen Li, Shitian Zhao, Qi Qin, Xinyue Li, Le
Zhuo, Licheng Wen, Dongyang Liu, Yuewen Cao, et al.Â Omnicaptioner: One
captioner to rule them all.arXiv preprint arXiv:2504.07089, 2025. \[50\]
Chunjie Luo, Jianfeng Zhan, Xiaohe Xue, Lei Wang, Rui Ren, and Qiang
Yang. Cosine normalization: Using cosine similarity instead of dot
product in neural networks. InInternational conference on artificial
neural networks, pages 382--391. Springer, 2018. 49

\[51\] Yiyang Ma, Xingchao Liu, Xiaokang Chen, Wen Liu, Chengyue Wu,
Zhiyu Wu, Zizheng Pan, Zhenda Xie, Haowei Zhang, Liang Zhao, et
al.Â Janusflow: Harmonizing autoregression and rectified flow for unified
multimodal understanding and generation.arXiv preprint arXiv:2411.07975,
2024. \[52\] Midjourney. Midjourney v7.https://www.midjourney.com/home,
2025. \[53\] Quang-Huy Nguyen, Cuong Q Nguyen, Dung D Le, and Hieu H
Pham. Enhancing few-shot image classification with cosine
transformer.IEEE Access, 11:79659--79672, 2023. \[54\] Hiroyuki Ootomo,
Akira Naruse, Corey Nolet, Ray Wang, Tamas Feher, and Yong Wang. Cagra:
Highly parallel graph construction and approximate nearest neighbor
search for gpus. In2024 IEEE 40th International Conference on Data
Engineering (ICDE), pages 4236--4247. IEEE, 2024. \[55\] OpenAI.
Gpt-image-1. https://openai.com/zh-Hans-CN/index/introducing-4o-image
-generation/, 2025. \[56\] Lawrence Page, Sergey Brin, Rajeev Motwani,
and Terry Winograd. The pagerank citation ranking: Bringing order to the
web. Technical report, Stanford infolab, 1999. \[57\] Dustin Podell,
Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas MÃ¼ller,
Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models
for high-resolution image synthesis.arXiv preprint arXiv:2307.01952,
2023. \[58\] Qi Qin, Le Zhuo, Yi Xin, Ruoyi Du, Zhen Li, Bin Fu, Yiting
Lu, Jiakang Yuan, Xinyue Li, Dongyang Liu, et al.Â Lumina-image 2.0: A
unified and efficient image generative framework.arXiv preprint
arXiv:2503.21758, 2025. \[59\] Rafael Rafailov, Archit Sharma, Eric
Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct
preference optimization: Your language model is secretly a reward
model.Advances in neural information processing systems,
36:53728--53741, 2023. \[60\] rapidsai. cuGraph - RAPIDS Graph Analytics
Library. https://github.com/rapidsai/cugr aph, 2018. Accessed:
2025-11-12. \[61\] Recraft. Recraft
v3.https://www.recraft.ai/docs/recraft-models/recraft-V3, 2024. \[62\]
Stephen Robertson, Hugo Zaragoza, et al.Â The probabilistic relevance
framework: Bm25 and beyond.Foundations and TrendsÂ® in Information
Retrieval, 3(4):333--389, 2009. \[63\] Robin Rombach, Andreas Blattmann,
Dominik Lorenz, Patrick Esser, and BjÃ¶rn Ommer. High- resolution image
synthesis with latent diffusion models. InProceedings of the IEEE/CVF
conference on computer vision and pattern recognition, pages
10684--10695, 2022. \[64\] Team Seedream, Yunpeng Chen, Yu Gao, Lixue
Gong, Meng Guo, Qiushan Guo, Zhiyao Guo, Xi- aoxia Hou, Weilin Huang,
Yixuan Huang, et al.Â Seedream 4.0: Toward next-generation multimodal
image generation.arXiv preprint arXiv:2509.20427, 2025. \[65\] Jay Shah,
Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, and Tri
Dao. Flashattention-3: Fast and accurate attention with asynchrony and
low-precision.Advances in Neural Information Processing Systems,
37:68658--68685, 2024. \[66\] Zhihong Shao, Peiyi Wang, Qihao Zhu,
Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K.
Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathemat-
ical reasoning in open language models, 2024. \[67\] FLUX-Krea Team.
Flux.1 krea \[dev\].https://github.com/krea-ai/flux-krea, 2025. \[68\]
Vincent A Traag, Ludo Waltman, and Nees Jan Van Eck. From louvain to
leiden: guaranteeing well-connected communities.Scientific reports,
9(1):1--12, 2019. 50

\[69\] Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad
Naeem, Ibrahim Alabdul- mohsin, Nikhil Parthasarathy, Talfan Evans,
Lucas Beyer, Ye Xia, Basil Mustafa, et al.Â Siglip 2: Multilingual
vision-language encoders with improved semantic understanding,
localization, and dense features.arXiv preprint arXiv:2502.14786, 2025.
\[70\] Yuxiang Tuo, Wangmeng Xiang, Jun-Yan He, Yifeng Geng, and
Xuansong Xie. Anytext: Multilingual visual text generation and editing.
2023. \[71\] Huy V Vo, Vasil Khalidov, TimothÃ©e Darcet, ThÃ©o Moutakanni,
Nikita Smetanin, Marc Szafraniec, Hugo Touvron, Camille Couprie, Maxime
Oquab, Armand Joulin, et al.Â Automatic data curation for self-supervised
learning: A clustering-based approach.arXiv preprint arXiv:2405.15613,
2024. \[72\] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun,
Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et
al.Â Emu3: Next-token prediction is all you need.arXiv preprint
arXiv:2409.18869, 2024. \[73\] Jason Wei, Xuezhi Wang, Dale Schuurmans,
Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et
al.Â Chain-of-thought prompting elicits reasoning in large language
models.Advances in neural information processing systems,
35:24824--24837, 2022. \[74\] Xinyu Wei, Jinrui Zhang, Zeqing Wang,
Hongyang Wei, Zhen Guo, and Lei Zhang. Tiif-bench: How does your t2i
model follow your instructions?arXiv preprint arXiv:2506.02161, 2025.
\[75\] Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca
Roelofs, Raphael Gontijo-Lopes, Ari S Morcos, Hongseok Namkoong, Ali
Farhadi, Yair Carmon, Simon Kornblith, et al.Â Model soups: averaging
weights of multiple fine-tuned models improves accuracy without
increasing inference time. InInternational conference on machine
learning, pages 23965--23998. PMLR, 2022. \[76\] Chenfei Wu, Jiahao Li,
Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng ming Yin, Shuai
Bai, Xiao Xu, Yilei Chen, Yuxiang Chen, Zecheng Tang, Zekai Zhang,
Zhengyi Wang, An Yang, Bowen Yu, Chen Cheng, Dayiheng Liu, Deqing Li,
Hang Zhang, Hao Meng, Hu Wei, Jingyuan Ni, Kai Chen, Kuan Cao, Liang
Peng, Lin Qu, Minggang Wu, Peng Wang, Shuting Yu, Tingkun Wen, Wensen
Feng, Xiaoxiao Xu, Yi Wang, Yichang Zhang, Yongqiang Zhu, Yujia Wu,
Yuxuan Cai, and Zenan Liu. Qwen-image technical report.arXiv preprint
arXiv:2508.02324, 2025. \[77\] Chengyue Wu, Xiaokang Chen, Zhiyu Wu,
Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu,
Chong Ruan, et al.Â Janus: Decoupling visual encoding for unified
multimodal understanding and generation. InProceedings of the Computer
Vision and Pattern Recognition Conference, pages 12966--12977, 2025.
\[78\] Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo,
Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, et
al.Â Omnigen2: Exploration to advanced multimodal generation. arXiv
preprint arXiv:2506.18871, 2025. \[79\] Xianfeng Wu, Yajing Bai, Haoze
Zheng, Harold Haodong Chen, Yexin Liu, Zihao Wang, Xuran Ma, Wen-Jie
Shu, Xianzu Wu, Harry Yang, et al.Â Lightgen: Efficient image generation
through knowledge distillation and direct preference optimization.arXiv
preprint arXiv:2503.08619, 2025. \[80\] Shitao Xiao, Yueze Wang, Junjie
Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang,
Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation.
InProceedings of the Computer Vision and Pattern Recognition Conference,
pages 13294--13304, 2025. \[81\] Enze Xie, Junsong Chen, Yuyang Zhao,
Jincheng YU, Ligeng Zhu, Yujun Lin, Zhekai Zhang, Muyang Li, Junyu Chen,
Han Cai, et al.Â Sana 1.5: Efficient scaling of training-time and
inference- time compute in linear diffusion transformer. InForty-second
International Conference on Machine Learning. \[82\] Jinheng Xie, Weijia
Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin,
Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One
single transformer to unify multimodal understanding and generation.
InThe Thirteenth International Conference on Learning Representations.
51

\[83\] Jinheng Xie, Zhenheng Yang, and Mike Zheng Shou. Show-o2:
Improved native unified multimodal models.arXiv preprint
arXiv:2506.15564, 2025. \[84\] Jiazheng Xu, Yu Huang, Jiale Cheng,
Yuanming Yang, Jiajun Xu, Yuan Wang, Wenbo Duan, Shen Yang, Qunlin Jin,
Shurun Li, et al.Â Visionreward: Fine-grained multi-dimensional human
preference learning for image and video generation.arXiv preprint
arXiv:2412.21059, 2024. \[85\] An Yang, Anfeng Li, Baosong Yang, Beichen
Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu
Lv, et al.Â Qwen3 technical report.arXiv preprint arXiv:2505.09388, 2025.
\[86\] An Yang, Junshu Pan, Junyang Lin, Rui Men, Yichang Zhang, Jingren
Zhou, and Chang Zhou. Chinese clip: Contrastive vision-language
pretraining in chinese.arXiv preprint arXiv:2211.01335, 2022. \[87\]
Yang Ye, Xianyi He, Zongjian Li, Bin Lin, Shenghai Yuan, Zhiyuan Yan,
Bohan Hou, and Li Yuan. Imgedit: A unified image editing dataset and
benchmark.arXiv preprint arXiv:2505.20275, 2025. \[88\] Tianwei Yin,
MichaÃ«l Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo
Durand, and William T Freeman. Improved distribution matching
distillation for fast image synthesis. In NeurIPS, 2024. \[89\] Tianwei
Yin, MichaÃ«l Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William
T Freeman, and Taesung Park. One-step diffusion with distribution
matching distillation. InProceedings of the IEEE/CVF conference on
computer vision and pattern recognition, pages 6613--6623, 2024. \[90\]
Qifan Yu, Wei Chow, Zhongqi Yue, Kaihang Pan, Yang Wu, Xiaoyang Wan,
Juncheng Li, Siliang Tang, Hanwang Zhang, and Yueting Zhuang. Anyedit:
Mastering unified high-quality image editing for any idea. InProceedings
of the Computer Vision and Pattern Recognition Conference, pages
26125--26135, 2025. \[91\] Biao Zhang and Rico Sennrich. Root mean
square layer normalization.Advances in Neural Informa- tion Processing
Systems, 32, 2019. \[92\] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun,
and Yu Su. Magicbrush: A manually annotated dataset for
instruction-guided image editing.Advances in Neural Information
Processing Systems, 36:31428--31449, 2023. \[93\] Yifu Zhang, Hao Yang,
Yuqi Zhang, Yifei Hu, Fengda Zhu, Chuang Lin, Xiaofeng Mei, Yi Jiang,
Bingyue Peng, and Zehuan Yuan. Waver: Wave your way to lifelike video
generation.arXiv preprint arXiv:2508.15761, 2025. \[94\] Zechuan Zhang,
Ji Xie, Yu Lu, Zongxin Yang, and Yi Yang. In-context edit: Enabling
instructional image editing with in-context generation in large scale
diffusion transformer.arXiv preprint arXiv:2504.20690, 2025. \[95\]
Haozhe Zhao, Xiaojian Shawn Ma, Liang Chen, Shuzheng Si, Rujie Wu,
Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, and Baobao Chang. Ultraedit:
Instruction-based fine-grained image editing at scale.Advances in Neural
Information Processing Systems, 37:3058--3093, 2024. \[96\] Yanli Zhao,
Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less
Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, et al.Â Pytorch fsdp:
experiences on scaling fully sharded data parallel.arXiv preprint
arXiv:2304.11277, 2023. \[97\] Wendi Zheng, Jiayan Teng, Zhuoyi Yang,
Weihan Wang, Jidong Chen, Xiaotao Gu, Yuxiao Dong, Ming Ding, and Jie
Tang. Cogview3: Finer and faster text-to-image generation via relay
diffusion. arXiv preprint arXiv:2403.05121, 2024. \[98\] Dewei Zhou, Ji
Xie, Zongxin Yang, and Yi Yang. 3dis-flux: simple and efficient
multi-instance generation with dit rendering.arXiv preprint
arXiv:2501.05131, 2025. 52

\[99\] Le Zhuo, Ruoyi Du, Xiao Han, Yangguang Li, Dongyang Liu, Rongjie
Huang, Wenze Liu, et al. Lumina-next: Making lumina-t2x stronger and
faster with next-dit.Advances in Neural Information Processing Systems
(NeurIPS), 2024. \[100\] Le Zhuo, Songhao Han, Yuandong Pu, Boxiang Qiu,
Sayak Paul, Yue Liao, Yihao Liu, Jie Shao, Xi Chen, Si Liu, et
al.Â Factuality matters: When image generation and editing meet
structured visuals.arXiv preprint arXiv:2510.05091, 2025. A. Prompts
Used in the Report Here we summarize the prompts/instructions used in
Figure 1-3, which can be directly input into Z-Image-Turbo (with PE
disabled) to reproduce our generation results. A.1. Figure 1 Column #1 â€¢
Case #1:
ä¸€å¼ ä¸­æ™¯æ‰‹æœºè‡ªæ‹ç…§ç‰‡æ‹æ‘„äº†ä¸€ä½ç•™ç€é•¿é»‘å‘çš„å¹´è½»ä¸œäºšå¥³å­åœ¨ç¯å…‰æ˜Žäº®çš„ç”µæ¢¯å†…å¯¹ç€é•œå­è‡ªæ‹ã€‚
å¥¹ç©¿ç€ä¸€ä»¶å¸¦æœ‰ç™½è‰²èŠ±æœµå›¾æ¡ˆçš„é»‘è‰²éœ²è‚©çŸ­ä¸Šè¡£å’Œæ·±è‰²ç‰›ä»”è£¤ã€‚å¥¹çš„å¤´å¾®å¾®å€¾æ–œï¼Œå˜´å”‡å˜Ÿèµ·åšäº²å»
çŠ¶ï¼Œéžå¸¸å¯çˆ±ä¿çš®ã€‚å¥¹å³æ‰‹æ‹¿ç€ä¸€éƒ¨æ·±ç°è‰²æ™ºèƒ½æ‰‹æœºï¼Œé®ä½äº†éƒ¨åˆ†è„¸ï¼ŒåŽç½®æ‘„åƒå¤´é•œå¤´å¯¹ç€é•œå­ã€‚
ç”µæ¢¯å¢™å£ç”±æŠ›å…‰ä¸é”ˆé’¢åˆ¶æˆï¼Œåå°„ç€å¤´é¡¶çš„è§å…‰ç¯å’Œä¸»ä½“ã€‚å·¦ä¾§å¢™ä¸Šæœ‰ä¸€ä¸ªå¸¦æœ‰è®¸å¤šåœ†å½¢æŒ‰é’®å’Œå°
åž‹æ•°å­—æ˜¾ç¤ºå±çš„åž‚ç›´é¢æ¿ã€‚åœ¨æŒ‰é’®ä¸‹æ–¹ï¼Œå¯ä»¥çœ‹åˆ°ä¸€ä¸ªé‡‘å±žæ‰¶æ‰‹ã€‚åŽå¢™ä¸Šè´´ç€å¸¦æœ‰æ–‡å­—çš„é•¿æ–¹å½¢æ ‡
å¿—ã€‚åœ°é¢é“ºç€å¸¦æœ‰ç™½è‰²çº¹ç†çš„æ·±è‰²å¤§ç†çŸ³ç“·ç –ã€‚æ•´ä½“ç…§æ˜Žä¸ºäººé€ å…‰ï¼Œæ˜Žäº®ï¼Œå…·æœ‰ç”µæ¢¯å†…éƒ¨çš„ç‰¹å¾ã€‚
(Translation: A mid-range phone selfie captured a young East Asian woman
with long black hair taking a selfie in front of a mirror in a brightly
lit elevator. She was wearing a black off shoulder short top with a
white floral pattern and dark jeans. Her head tilted slightly, and her
lips curled up in a kiss, very cute and playful. She held a dark gray
smartphone in her right hand, covering part of her face, with the rear
camera facing the mirror. The elevator walls are made of polished
stainless steel, reflecting the fluorescent lights and main body above
the head. There is a vertical panel on the left wall with many circular
buttons and a small digital display screen. Below the button, you can
see a metal armrest. There is a rectangular sign with text on the back
wall. The ground is covered with dark marble tiles with white texture.
The overall lighting is artificial light, bright, and has the
characteristics of an elevator interior.) â€¢ Case #2:
ä¸€å¼ å……æ»¡åŠ¨æ„Ÿçš„è¿åŠ¨æ‘„å½±ç…§ç‰‡ï¼Œæ•æ‰åˆ°ä¸€åæ©„æ¦„çƒè¿åŠ¨å‘˜åœ¨æ¯”èµ›ä¸­å¥”è·‘çš„çž¬é—´ï¼Œä»–æ­£å‘å³å†²åˆºï¼Œ
å·¦æ‰‹æŠ±ç€æ©„æ¦„çƒã€‚ä¸­å¿ƒäººç‰©æ˜¯ä¸€å
30å¤šå²çš„é»‘äººç”·æ€§æ©„æ¦„çƒè¿åŠ¨å‘˜ï¼Œèº«ç©¿ç»¿ç™½æ¡çº¹çƒè¡£ï¼Œä¸Šé¢ æœ‰"ISC"å’Œ"POWERDAY"
æ ‡å¿—ï¼Œç™½è‰²çŸ­è£¤å·¦è…¿å°æœ‰ "ISC"ï¼Œç™½è‰²è¢œå­å¸¦æœ‰ç»¿è‰²æ¡çº¹ï¼Œè„šç©¿ç™½è‰²é’‰
éž‹ã€‚ä»–çš„å·¦ä¾§ï¼Œä¸¤åèº«ç©¿é»‘è‰²å’Œé»„è‰²æ¡çº¹çƒè¡£çš„å¯¹æ‰‹æ­£æœç›¸åæ–¹å‘å¥”è·‘ï¼Œçƒè¡£æ­£é¢æœ‰
"ALEX"å­—
æ ·ï¼Œæ­é…é»„è‰²è¢œå­å’Œç™½è‰²é’‰éž‹ï¼Œå‡ç•¥å¾®å¤±ç„¦ã€‚èƒŒæ™¯æ˜¯æ¨¡ç³Šçš„ä½“è‚²åœºè§‚ä¼—å¸­ï¼Œè§‚ä¼—ç©¿ç€å„è‰²æœè£…ï¼Œè¿˜
æœ‰è“è‰²å’Œç™½è‰²çš„ä½“è‚²åœºåº§æ¤…ï¼Œä»¥åŠä¸€å—æ©™è‰²å¹¿å‘Šç‰Œï¼Œä¸Šé¢æœ‰éƒ¨åˆ†å¯è§çš„ç™½è‰²æ–‡å­—
"RAK"å’Œä¸€ä¸ªé»„
è‰²åœ†å½¢æ ‡å¿—ã€‚å‰æ™¯æ˜¯ç»´æŠ¤è‰¯å¥½çš„ç»¿è‰²æ©„æ¦„çƒåœºã€‚åŠ¨ä½œæ‘„å½±ï¼Œä½“è‚²æ‘„å½±ï¼Œæµ…æ™¯æ·±ï¼Œä¸­å¿ƒçƒå‘˜å¯¹ç„¦æ¸…
æ™°ï¼ŒèƒŒæ™¯è™šåŒ–ï¼Œè‡ªç„¶å…‰ç…§æ˜Žï¼Œè‰²å½©é²œè‰³ï¼Œé«˜å¯¹æ¯”åº¦ï¼Œæž„å›¾åŠ¨æ„Ÿï¼Œå¯¹æ‰‹çƒå‘˜æœ‰è¿åŠ¨æ¨¡ç³Šï¼Œå……æ»¡æ´»åŠ›ï¼Œ
ç«žæŠ€æ°›å›´ï¼Œæˆ·å¤–ä½“è‚²åœºåœºæ™¯ã€‚ (Translation: A dynamic sports photography
photo captures the moment of a rugby player running during a game,
sprinting to the right while holding a rugby ball in his left hand. The
central figure is a black male rugby player in his thirties, wearing a
green and white striped jersey with the "ISC" and "POWERDAY" logos on
it, white shorts with "ISC" printed on the left leg, white socks with
green stripes, and white spiked shoes. On his left side, two opponents
wearing black and yellow striped jerseys are running in opposite
directions, with the word "ALEX" on the front of the jerseys, paired
with yellow socks and white spiked shoes, both slightly out of focus.
The background is a blurry stadium audience, dressed in various
clothing, with blue and white stadium seats, as well as an orange
billboard with partially visible white text "RAK" and a yellow circular
logo. The prospect is to maintain a good green rugby field. Action
photography, sports photography, shallow depth of field, clear focus on
the center player, blurred background, natural lighting, bright colors,
high 53

contrast, dynamic composition, blurred motion of the opponent player,
full of vitality, competitive atmosphere, outdoor sports stadium scene.)
â€¢ Case #3:
ä¸€å¼ å¹¿è§’ã€å¹³è§†è§’åº¦çš„ç…§ç‰‡æ•æ‰åˆ°äº†ä¸€ä¸ªå……æ»¡æ´»åŠ›çš„è¡—æ™¯ï¼Œåœ°ç‚¹æ˜¯ä¸€æ¡é“ºç€ä¸å¹³æ•´é¹…åµçŸ³çš„ç‹­çª„å¤
è€å°å··ã€‚å°å··ä¸¤æ—æ˜¯ä¸¤åˆ°ä¸‰å±‚çš„çº¢ç –å»ºç­‘ï¼Œå…·æœ‰ä¼ ç»Ÿå»ºç­‘ç‰¹è‰²çš„æ·±è‰²æœ¨é—¨ã€çª—æ¡†å’Œæ‚¬æŒ‘çš„æ¥¼å±‚ã€‚åœ¨
å·¦è¾¹ï¼Œä¸¤åå¥³å­ç«™åœ¨ä¸€ç‰‡é˜³å…‰ä¸‹äº¤è°ˆã€‚ä¸€åå¥³å­åœ¨æ ¼å­è¡¬è¡«å’Œæ·±è‰²è£¤å­å¤–å¥—ç€çº¢è‰²å›´è£™ï¼Œå¦ä¸€åå¥³
å­åˆ™æŠ«ç€æ·±è‰²æŠ«è‚©ã€‚ä¸€åªé»‘è‰²å°ç‹—èººåœ¨å¥¹ä»¬è„šä¸‹æ¸©æš–çš„çŸ³å¤´ä¸Šã€‚åœ¨å‰æ™¯ä¸­ï¼Œä¸€åªä½“åž‹è¾ƒå¤§ã€æœ‰ç€è“¬
æ¾å·æ›²å°¾å·´çš„é‡‘è‰²ç‹—æ­£åœ¨å—…æŽ¢é¹…åµçŸ³è·¯é¢ã€‚æ²¿ç€å°å··çš„ä¸­å¿ƒå†å¾€å‰ï¼Œä¸€ä¸ªäººæ­£éª‘ç€ä¸€è¾†å°åž‹æ‘©æ‰˜è½¦
è¿œåŽ»ï¼Œå¦ä¸€åªé»‘è‰²å°ç‹—åˆ™ååœ¨è¡—é“çš„å³ä¾§ã€‚æ˜Žäº®çš„é˜³å…‰å’Œæ·±é‚ƒçš„é˜´å½±åœ¨æ•´ä¸ªåœºæ™¯ä¸­å½¢æˆé²œæ˜Žå¯¹æ¯”ï¼Œ
çªæ˜¾äº†ç –å—å’ŒçŸ³å¤´çš„çº¹ç†ã€‚å°å··å°½å¤´å¯è§çš„å¤©ç©ºæ˜¯è‹ç™½çš„é˜´ç™½è‰²ã€‚ (Translation:
A wide-angle, head up photo captures a vibrant street scene in a narrow
ancient alley paved with uneven pebbles. On both sides of the alley are
two to three story red brick buildings with traditional architectural
features such as dark wooden doors, window frames, and cantilevered
floors. On the left, two women are standing in a sunny area talking. A
woman is wearing a red apron over a checkered shirt and dark pants,
while another woman is draped in a dark shawl. A small black dog lay on
the warm stone beneath their feet. In the foreground, a large golden dog
with a fluffy and curly tail is sniffing the cobblestone road surface.
Continuing along the center of the alley, a person is riding a small
motorcycle away, while another black puppy is sitting on the right side
of the street. The bright sunlight and deep shadows create a sharp
contrast throughout the scene, highlighting the texture of the bricks
and stones. The sky visible at the end of the alley is pale and gloomy
white.) â€¢ Case #4:
ä¸€å¼ å®é™çš„ã€å…¨æ™¯æ¨ªå‘ç…§ç‰‡ï¼Œæ•æ‰äº†ä¸€ä¸ªå°å­©ä¾§èº«ç«™åœ¨éƒéƒè‘±è‘±çš„ç»¿è‰²è‰å²¸ä¸Šï¼Œæ—è¾¹æ˜¯å¹³é™çš„æ°´
ä½“ã€‚åœºæ™¯è®¾ç½®åœ¨é»„é‡‘æ—¶åˆ»ï¼Œå¾ˆå¯èƒ½æ˜¯æ—¥è½æ—¶åˆ†ï¼Œè‰²è°ƒæŸ”å’Œè€Œæ¸©æš–ã€‚å­©å­ä½äºŽç”»é¢å·¦ä¾§ï¼Œæˆ´ç€ä¸€é¡¶æµ…
è‰²çš„ç¼–ç»‡è‰å¸½ï¼Œåœ¨æµ…è“ç™½æ ¼çº¹é•¿è¢–è¡¬è¡«å¤–ç©¿ç€ä¸€ä»¶æ©„æ¦„ç»¿è‰²çš„çŸ­è¢–èƒŒå¿ƒï¼Œä¸‹èº«æ˜¯å®½æ¾çš„æ·±è“è‰²ç‰›ä»”
è£¤ï¼Œè£¤è„šå·èµ·ï¼Œéœ²å‡ºæ£•è‰²çš„éž‹å­ã€‚å­©å­çš„å³æ‰‹æ‹¿ç€ä¸€æœµé»„è‰²å°èŠ±çš„èŒŽï¼Œå·¦æ‰‹æç€ä¸€ä¸ªé“¶è‰²çš„å°å·é•€
é”Œé‡‘å±žå–·å£¶ã€‚ä»–
/å¥¹æ­£æœå³è¾¹æœ›åŽ»ï¼Œçœ‹å‘æ°´é¢ã€‚å‰æ™¯æ˜¯ç‚¹ç¼€ç€é»„è‰²å°é‡ŽèŠ±çš„è‰å¡ã€‚ä¸­æ™¯æ˜¯æ²³æµæˆ–æ± 
å¡˜çš„é™æ°´ï¼Œå€’æ˜ ç€å¤©ç©ºæ¸©æš–çš„ç²‰æ©™è‰²è°ƒã€‚å¯¹å²¸æœ‰ç»¿è‰²æ¤è¢«å’Œä¸€å †ç°è‰²å²©çŸ³ã€‚èƒŒæ™¯æ˜¯æŸ”å’Œæ¨¡ç³Šçš„ï¼Œå±•
çŽ°äº†å¹¿é˜”çš„ç»¿è‰²ç”°é‡Žã€è¿œå¤„çš„æ ‘æž—çº¿ä»¥åŠä¸€äº›æ¨¡ç³Šçš„å»ºç­‘è½®å»“ï¼Œè¿™ä¸€åˆ‡éƒ½åœ¨ä¸€ç‰‡å¹¿é˜”çš„å¤©ç©ºä¸‹ï¼Œå¤©
ç©ºå¸ƒæ»¡äº†æŸ”å’Œçš„ã€å¸¦æœ‰ç²‰è‰²å’Œæ©™è‰²æ¸å˜çš„äº‘å½©ã€‚æ‘„å½±é£Žæ ¼çš„ç‰¹ç‚¹æ˜¯æµ…æ™¯æ·±ï¼Œåœ¨èƒŒæ™¯ä¸­åˆ›é€ äº†æ˜¾è‘—çš„
æ•£æ™¯æ•ˆæžœï¼Œä½¿ä¸»ä½“çªå‡ºã€‚å…‰çº¿è‡ªç„¶è€Œæ¼«å°„ï¼Œè¥é€ å‡ºå¹³å’Œã€ç”°å›­è¯—èˆ¬å’Œæ€€æ—§çš„æ°›å›´ã€‚å­©å­çš„è„šè¢«ç”»é¢
çš„åº•éƒ¨è¾¹ç¼˜è½»å¾®æˆªæ–­ã€‚ (Translation: A peaceful, panoramic horizontal
photo captures a child standing sideways on a lush green grassy bank,
with a calm body of water beside it. The scene is set in prime time,
most likely at sunset, with soft and warm tones. The child is located on
the left side of the screen, wearing a light colored woven straw hat, an
olive green short sleeved vest over a light blue and white checkered
long sleeved shirt, and loose dark blue jeans with rolled up hemlines
revealing brown shoes. The child holds a stem of a small yellow flower
in their right hand and a silver small galvanized metal spray can in
their left hand. He/she is looking to the right, towards the water
surface. The prospect is a grassy slope adorned with small yellow
wildflowers. The central view is the still water of a river or pond,
reflecting the warm pink orange tones of the sky. There is green
vegetation and a pile of gray rocks on the opposite bank. The background
is soft and blurry, showing vast green fields, distant forest lines, and
some blurry building contours, all under a vast sky filled with soft,
pink and orange gradient clouds. The characteristic of photography style
is shallow depth of field, which creates a significant bokeh effect in
the background, making the subject stand out. The natural and diffuse
light creates a peaceful, pastoral, and nostalgic atmosphere. The
child's feet were slightly cut off by the bottom edge of the screen.) â€¢
Case #5:
ä¸€å¼ å¹¿è§’é£Žæ™¯ç…§ç‰‡ï¼Œæ‹æ‘„äºŽé˜´å¤©çš„å®‰å¾½å®æ‘å¤æ‘è½ã€‚ç”»é¢è¢«å¹³é™æ°´ä½“çš„å²¸çº¿æ°´å¹³åˆ†å‰²ï¼Œå½¢æˆäº†æ‘åº„
ä¸Žå¤©ç©ºè¿‘ä¹Žå®Œç¾Žçš„é•œé¢å€’å½±ã€‚åœ¨ä¸­æ™¯éƒ¨åˆ†ï¼Œä¸€ç°‡å¯†é›†çš„ä¼ ç»Ÿå¾½æ´¾å»ºç­‘æ²¿æ°´è¾¹æŽ’åˆ—ï¼Œå…·æœ‰ç‹¬ç‰¹çš„ç™½å¢™
å’Œæ·±ç°è‰²ç“¦é¡¶ã€‚å‡ æ ‹å»ºç­‘çš„å±‹æªä¸‹æ‚¬æŒ‚ç€çº¢è‰²çš„çº¸ç¯ç¬¼ï¼Œåœ¨æŸ”å’Œçš„èƒŒæ™¯ä¸­å¢žæ·»äº†é²œè‰³çš„è‰²å½©ç‚¹ç¼€ã€‚
æ°´è¾¹çš„çŸ³æ¿è·¯ä¸Šå’Œæˆ¿å±‹ä¹‹é—´æ•£å¸ƒç€è®¸å¤šå…‰ç§ƒç§ƒçš„è½å¶æ ‘ã€‚ä¸€äº›èº«å½±å¾®å°çš„äººæ²¿ç€è¿™æ¡å°è·¯è¡Œèµ°æˆ–å
ç€ã€‚åœ¨èƒŒæ™¯ä¸­ï¼Œä¸€ç‰‡æœ¦èƒ§çš„è“ç»¿è‰²å±±è„‰åœ¨æ·¡ç°è‰²çš„å¤©ç©ºä¸‹è¿žç»µèµ·ä¼ã€‚å³ä¾§å±±å¡ä¸Šå¯ä»¥çœ‹åˆ°ä¸€ä¸ªå°åž‹
è¾“ç”µå¡”ã€‚åœ¨ç”»é¢ä¸­å¿ƒåå³çš„ä¸€æ ‹å»ºç­‘ä¸Šï¼Œé—¨æ¥£ä¸Šæ–¹æŒ‚ç€ä¸€å—æ¨ªå‘çš„æœ¨åŒ¾ï¼Œä¸Šé¢æœ‰é»‘è‰²çš„æ±‰å­—
"ä¸–å¾·
å ‚"ã€‚è¯¥æ‘„å½±ä½œå“çš„é£Žæ ¼ç‰¹ç‚¹æ˜¯æž„å›¾å¯¹ç§°ï¼Œå…‰çº¿æŸ”å’Œæ¼«å°„ï¼Œæ™¯æ·±è¾ƒå¤§ï¼Œæ•´ä¸ªåœºæ™¯éƒ½æ¸…æ™°é”åˆ©ï¼Œè‰²è°ƒ
æ¸…å†·è€Œå®é™ï¼Œä»¥ç™½è‰²ã€ç°è‰²å’Œè“è‰²ä¸ºä¸»ï¼Œçº¢è‰²ä½œä¸ºå¼ºçƒˆçš„ç‚¹ç¼€è‰²ã€‚æ•´ä½“æ°›å›´å¹³å’Œã€å®‰è¯¦ä¸”å…·æœ‰æ°¸æ’
54

æ„Ÿã€‚ (Translation: A wide-angle landscape photo taken on a cloudy day in
the ancient village of Hongcun, Anhui. The screen is horizontally
divided by the calm shoreline of the water, forming a nearly perfect
mirror reflection of the village and the sky. In the central area, a
dense cluster of traditional Huizhou style buildings are arranged along
the water's edge, featuring unique white walls and dark gray tiled
roofs. Red paper lanterns hang under the eaves of several buildings,
adding vibrant color accents to the soft background. There are many bare
deciduous trees scattered between the stone roads and houses by the
water's edge. Some small figures walked or sat along this path. In the
background, a hazy blue-green mountain range undulates continuously
under a light gray sky. A small transmission tower can be seen on the
right slope. On a building to the right of the center of the screen,
there is a horizontal wooden plaque hanging above the lintel, with black
Chinese characters " ä¸–å¾·å ‚ " on it. The stylistic features of this
photography work are symmetrical composition, soft and diffuse lighting,
large depth of field, clear and sharp entire scene, cool and peaceful
tones, mainly white, gray and blue, with red as a strong accent color.
The overall atmosphere is peaceful, serene, and has a sense of
eternity.) â€¢ Case #6:
ä¸€å¼ å……æ»¡æ´»åŠ›çš„å¹¿è§’å¤œæ™¯ç…§ç‰‡ï¼Œæ•æ‰äº†ä¸­å›½å¹¿å·žçŒŽå¾·å¤§æ¡¥ä¸Šç©ºå£®è§‚çš„çƒŸèŠ±è¡¨æ¼”ã€‚åœºæ™¯è®¾ç½®åœ¨æ¼†é»‘çš„
å¤œç©ºä¸‹ï¼Œè¢«å¤šæœµå·¨å¤§çš„çƒŸèŠ±çˆ†ç‚¸çž¬é—´ç…§äº®ã€‚çƒŸèŠ±ä¸»è¦ä¸ºç™½è‰²å’Œçº¢è‰²
/ç²‰è‰²ï¼Œåœ¨ç”»é¢çš„ä¸ŠåŠéƒ¨åˆ†å½¢æˆ
äº†ç¿çƒ‚çš„ç‰¡ä¸¹èŠ±çŠ¶å›¾æ¡ˆï¼Œå‘¨å›´çŽ¯ç»•ç€æµ“æµ“çš„ç¡çƒŸã€‚çŒŽå¾·å¤§æ¡¥ï¼Œä¸€åº§çŽ°ä»£åŒ–çš„æ–œæ‹‰æ¡¥ï¼Œåœ¨ä¸­æ™¯å¤„æ¨ªè·¨
ç æ±Ÿã€‚å…¶ç‹¬ç‰¹çš„æ‹±å½¢ä¸­å¤®æ¡¥å¡”è¢«æ¸©æš–çš„é»„è‰²ç¯å…‰ç…§äº®ã€‚åœ¨è¿™ä¸ªä¸­å¤®æ¡¥å¡”çš„æ­£é¢ï¼Œå¯ä»¥çœ‹åˆ°ä¸€ä¸ªè¢«éƒ¨
åˆ†é®æŒ¡çš„çº¢è‰²å°æ ‡å¿—ã€‚æ¡¥é¢ä¹Ÿè¢«è·¯ç¯ç…§äº®ã€‚åœ¨å‰æ™¯ä¸­ï¼Œé»‘æš—çš„æ±Ÿæ°´æ˜ ç…§å‡ºçƒŸèŠ±å’Œæ¡¥ç¯çš„ç¼¤çº·å€’å½±ã€‚
å·¦ä¸‹è§’å¯ä»¥çœ‹åˆ°ä¸€è‰˜å°èˆ¹çš„é»‘è‰²å‰ªå½±ï¼Œè¿œå¤„è¿˜æ•£å¸ƒç€å…¶ä»–æ›´å°çš„èˆ¹åªã€‚èƒŒæ™¯æ˜¯é—ªé—ªå‘å…‰çš„çŽ°ä»£åŒ–åŸŽ
å¸‚å¤©é™…çº¿ï¼Œæ‘©å¤©å¤§æ¥¼å’Œå…¶ä»–å»ºç­‘ä¸Šçš„æ— æ•°ç¯å…‰ç‚¹ç¼€å…¶é—´ã€‚è¯¥æ‘„å½±é£Žæ ¼ä»¥é•¿æ›å…‰ä¸ºç‰¹ç‚¹ï¼Œè¿™ä»ŽçƒŸèŠ±çš„
è½¨è¿¹ä¸­å¯ä»¥æ˜Žæ˜¾çœ‹å‡ºï¼Œè¥é€ å‡ºä¸€ç§åŠ¨æ„Ÿå’Œå–œåº†çš„æ°›å›´ã€‚å›¾åƒå¯¹æ¯”åº¦é«˜ï¼Œå¯¹ç„¦æ¸…æ™°ï¼Œåœ¨é»‘æš—çš„çŽ¯å¢ƒä¸­
å‘ˆçŽ°å‡ºé²œè‰³çš„è‰²å½©ã€‚ (Translation: A vibrant wide-angle night view photo
captures the spectacular fireworks display over the Liede Bridge in
Guangzhou, China. The scene is set in the pitch black night sky,
instantly illuminated by multiple huge fireworks explosions. The
fireworks are mainly white and red/pink, forming a brilliant peony
shaped pattern in the upper part of the picture, surrounded by thick
gunpowder smoke. Liede Bridge, a modern cable-stayed bridge, crosses the
the Pearl River in the middle view. Its unique arched central bridge
tower is illuminated by warm yellow lights. On the front of this central
bridge tower, a partially obscured red small sign can be seen. The
bridge deck is also illuminated by streetlights. In the foreground, the
dark river reflects the colorful reflections of fireworks and bridge
lights. In the lower left corner, a black silhouette of a small boat can
be seen, with other smaller boats scattered in the distance. The
background is a sparkling modern city skyline, adorned with countless
lights from skyscrapers and other buildings. This photography style is
characterized by long exposures, which can be clearly seen from the
trajectory of fireworks, creating a dynamic and festive atmosphere. The
image has high contrast, clear focus, and presents bright colors in dark
environments.) Column #2 â€¢ Case #1: A stylish young woman sits casually
on an unmade bed bathed in soft daylight, wearing a pastel yellow
oversized T-shirt with subtle white text and cozy light gray sweatpants.
Her skin glows fresh beneath glossy deep lavender hydrogel under-eye
patches, while her hair is tied back loosely with a scrunchie,
complemented by delicate gold hoop earrings. Nearby, a tube of hand
cream and an open laptop rest casually atop soft, slightly rumpled
sheets. The natural window light gently illuminates her radiant skin and
the subtle sheen of the hydrogel patches, enhancing the cozy textures of
her loungewear and bedding. Shot from a top-down selfie angle, the
framing captures her face, shoulders, and upper torso with realistic
iPhone grain, conveying an authentic, relaxed self-care morning moment
in a softly lit bedroom scene -- skincare selfie, shot on iPhone. â€¢ Case
#2:
ä¸€å¼ é€¼çœŸçš„å¹´è½»ä¸œäºšå¥³æ€§è‚–åƒï¼Œä½äºŽç”»é¢ä¸­å¿ƒåå·¦çš„ä½ç½®ï¼Œå¸¦ç€æµ…æµ…çš„å¾®ç¬‘ç›´è§†è§‚è€…ã€‚å¥¹èº«ç€ä»¥æµ“
éƒçš„çº¢è‰²å’Œé‡‘è‰²ä¸ºä¸»çš„ä¼ ç»Ÿä¸­å¼æœè£…ã€‚å¥¹çš„å¤´å‘è¢«ç²¾å¿ƒç›˜èµ·ï¼Œé¥°æœ‰ç²¾è‡´çš„çº¢è‰²å’Œé‡‘è‰²èŠ±å‰å’Œå¶å½¢å‘
é¥°ã€‚å¥¹çš„çœ‰å¿ƒä¹‹é—´é¢å¤´ä¸Šç»˜æœ‰ä¸€ä¸ªå°å·§ã€åŽä¸½çš„çº¢è‰²èŠ±å‰å›¾æ¡ˆã€‚å¥¹å·¦æ‰‹æŒä¸€æŠŠä»¿å¤æ‰‡å­ï¼Œæ‰‡é¢ä¸Šç»˜
55

æœ‰ä¸€ä½èº«ç€ä¼ ç»Ÿæœé¥°çš„å¥³æ€§ã€ä¸€æ£µæ ‘å’Œä¸€åªé¸Ÿçš„åœºæ™¯ã€‚å¥¹çš„å³æ‰‹å‘å‰ä¼¸å‡ºï¼Œæ‰‹æŽŒå‘ä¸Šï¼Œæ‰˜ç€ä¸€ä¸ªæ‚¬
æµ®çš„å‘å…‰çš„éœ“è™¹é»„è‰²é—ªç”µäºšå…‹åŠ›ç¯ç‰Œï¼Œè¿™æ˜¯ç”»é¢ä¸­æœ€äº®çš„å…ƒç´ ã€‚èƒŒæ™¯æ˜¯æ¨¡ç³Šçš„å¤œæ™¯ï¼Œå¸¦æœ‰æš–è‰²è°ƒçš„
äººå·¥ç¯å…‰ï¼Œä¸€åœºæˆ·å¤–æ–‡åŒ–æ´»åŠ¨æˆ–åº†å…¸ã€‚åœ¨è¿œå¤„çš„èƒŒæ™¯ä¸­ï¼Œå¥¹å¤´éƒ¨çš„å·¦ä¾§ç•¥åï¼Œæ˜¯ä¸€åº§é«˜å¤§ã€å¤šå±‚ã€
è¢«æš–å…‰ç…§äº®çš„è¥¿å®‰å¤§é›å¡”ã€‚ä¸­æ™¯å¯è§å…¶ä»–æ¨¡ç³Šçš„å»ºç­‘å’Œç¯å…‰ï¼Œæš—ç¤ºç€ä¸€ä¸ªç¹åŽçš„åŸŽå¸‚æˆ–æ–‡åŒ–èƒŒæ™¯ã€‚
å…‰çº¿æ˜¯ä½Žè°ƒçš„ï¼Œé—ªç”µç¬¦å·ä¸ºå¥¹çš„è„¸éƒ¨å’Œæ‰‹éƒ¨æä¾›äº†æ˜¾è‘—çš„ç…§æ˜Žã€‚æ•´ä½“æ°›å›´ç¥žç§˜è€Œè¿·äººã€‚äººç‰©çš„å¤´
éƒ¨ã€æ‰‹éƒ¨å’Œä¸ŠåŠèº«å®Œå…¨å¯è§ï¼Œä¸‹åŠèº«è¢«ç”»é¢åº•éƒ¨è¾¹ç¼˜æˆªæ–­ã€‚å›¾åƒå…·æœ‰ä¸­ç­‰æ™¯æ·±ï¼Œä¸»ä½“æ¸…æ™°èšç„¦ï¼ŒèƒŒ
æ™¯æŸ”å’Œæ¨¡ç³Šã€‚è‰²å½©æ–¹æ¡ˆæ¸©æš–ï¼Œä»¥çº¢è‰²ã€é‡‘è‰²å’Œé—ªç”µçš„äº®é»„è‰²ä¸ºä¸»ã€‚ (Translation:
A realistic portrait of a young East Asian woman, located to the left of
the center of the image, looking directly at the viewer with a faint
smile. She was dressed in traditional Chinese clothing dominated by rich
red and gold colors. Her hair was carefully styled, adorned with
delicate red and gold flowers and leaf shaped hair accessories. There is
a small and gorgeous red floral pattern painted on her forehead between
her eyebrows. She held an antique style fan in her left hand, with a
scene of a woman dressed in traditional clothing, a tree, and a bird
painted on the fan surface. Her right hand extended forward, palm up,
holding a suspended glowing neon yellow lightning acrylic light tag,
which was the brightest element in the picture. The background is a
blurry night scene with warm toned artificial lighting, representing an
outdoor cultural event or celebration. In the distant background, to the
left of her head is a tall, multi-layered, warm lit Xi'an Big Wild Goose
Pagoda. Other blurry buildings and lights can be seen in the middle of
the scene, implying a bustling city or cultural background. The light is
low-key, and the lightning symbol provides significant illumination for
her face and hands. The overall atmosphere is mysterious and charming.
The head, hands, and upper body of the character are fully visible,
while the lower body is cut off by the bottom edge of the screen. The
image has a moderate depth of field, the subject is clearly focused, and
the background is soft and blurry. The color scheme is warm, with red,
gold, and bright yellow of lightning as the main colors.) â€¢ Case #3: A
full-body, eye-level photograph of a young, beautiful East Asian woman
posing cheerfully inside a brightly lit LEGO store or brand exhibition
space. The woman, positioned slightly right of center, has long dark
hair and is smiling at the camera. She wears a vibrant yellow ribbed
beanie, a white diamond-quilted puffer jacket over a white t-shirt, and
medium-wash blue jeans with cuffs rolled up at the ankles. She is
wearing white lace-up sneakers and white socks, with a small red heart
visible on her left sock. In her left hand, she holds a black structured
handbag. Her pose is playful, with her left leg kicked up behind her. To
her left is a large, multi-tiered display stand in bright yellow, which
features the official LEGO logo -- white text in a red square with a
black and yellow outline -- in the upper left corner. On this stand are
two large-scale LEGO Minifigure statues: a policeman in a blue uniform
and hat stands in the foreground, and behind him is a Santa Claus figure
in red. The background shows more yellow retail shelving stocked with
various LEGO sets and products. The floor is made of large, light grey
tiles, and a white dome security camera is visible on the ceiling. The
image is a sharp, well-lit snapshot with a vibrant color palette,
dominated by yellow, red, and blue, creating a joyful and commercial
atmosphere. â€¢ Case #4: A candid mid-2010s-style snapshot featuring a
pale young woman with icy platinum hair styled casually loose, seated on
a metal bench inside a monochrome concept store. She wears a huge black
hoodie, sheer tights, and maroon platform creepers, complemented by a
beanie embroidered with "Z-Image Real & Fast" The subject's relaxed
expression gazes off to the side, conveying subtle, ambiguous emotion.
The lighting is cold and matte with soft shadows stretching along a
wooden floor, intentionally exhibiting muted color saturation, softened
contrast, and distinctly cool-toned bluish-gray shadows. Visible
textures include realistic skin details, detailed fabric grain of the
hoodie and tights, individual icy hair strands, and clear accessory
textures. The framing is slightly off-center and casually tilted,
capturing spontaneous intimacy and informal snapshot aesthetics
characteristic of mid-2010s casual youth photography. Column #3 â€¢ Case
#1:
ä¸€ä½ç”·å£«å’Œä»–çš„è´µå®¾çŠ¬ç©¿ç€é…å¥—çš„æœè£…å‚åŠ ç‹—ç‹—ç§€ï¼Œå®¤å†…ç¯å…‰ï¼ŒèƒŒæ™¯ä¸­æœ‰è§‚ä¼—ã€‚
(Translation: A man and his poodle participated in a dog show wearing
matching costumes, with 56

indoor lighting and an audience in the background.) â€¢ Case #2:
ä¸€å¼ ç‰¹å†™ã€é€¼çœŸçš„ä¸œäºšå©´å„¿è‚–åƒï¼Œå©´å„¿ç©¿ç€ä¸€ä»¶å°æœ‰å¿ƒå½¢å›¾æ¡ˆçš„å¥¶æ²¹è‰²è“¬æ¾å†¬å­£è¿žä½“è¡£ï¼Œç›´è§†è§‚
è€…ã€‚å©´å„¿æ‹¥æœ‰æ·±è‰²å¤´å‘å’Œçº¢æ‰‘æ‰‘çš„è„¸é¢Šã€‚å©´å„¿æ‰‹è¾¹éƒ¨åˆ†å¯è§ä¸€ä¸ªè‰²å½©é²œè‰³çš„çŽ©å…·ï¼ŒèƒŒæ™¯æ¨¡ç³Šå¤„æœ‰ä¸€
ä½ç©¿ç€æ ¼å­è¡¬è¡«çš„äººã€‚å®¤å†…å…‰çº¿å…·æœ‰æŸ”å’Œçš„é˜´å½±å’Œé«˜å…‰ï¼Œè¥é€ å‡ºæ¸©æš–çš„è‰²è°ƒï¼Œå©´å„¿è„¸éƒ¨æ¸…æ™°èšç„¦ï¼Œ
èƒŒæ™¯æŸ”å’Œæ¨¡ç³Šã€‚ä½Žé¥±å’Œåº¦ã€é¢—ç²’æ„Ÿã€è€èƒ¶ç‰‡é£Žæ ¼ã€‚ (Translation: A close-up,
realistic portrait of an East Asian baby wearing a creamy fluffy winter
jumpsuit with a heart-shaped pattern, looking straight at the viewer.
Babies have dark hair and rosy cheeks. A brightly colored toy can be
seen near the baby's side, with a person wearing a checkered shirt in a
blurry background. The indoor lighting features soft shadows and
highlights, creating a warm tone. The baby's face is clearly focused,
and the background is soft and blurry. Low saturation, graininess, and
vintage film style.) â€¢ Case #3:
åŒ—äº¬å›½å®¶ä½“è‚²åœºï¼ˆé¸Ÿå·¢ï¼‰çš„ç…§ç‰‡ï¼Œè“å¤©èƒŒæ™¯ä¸‹ï¼Œä½“è‚²åœºçš„å¤–è§‚ç”±å¤æ‚çš„äº¤ç»‡é’¢ç»“æž„å½¢æˆç½‘çŠ¶å›¾æ¡ˆä¸»
å¯¼ã€‚å‰æ™¯ä¸­ä¸€ä¸ªäººç©¿ç€ä¼‘é—²è£…ï¼Œç•¥å¾®åä¸­å¿ƒä½ç½®è¡Œèµ°ã€‚èƒŒæ™¯é€šè¿‡é’¢ç»“æž„å¯ä»¥çœ‹åˆ°ä½“è‚²åœºå†…éƒ¨çš„çº¢
è‰²åº§ä½åŒºã€‚
"A30"ç”¨çº¢è‰²æ ‡è®°åœ¨é’¢ç»“æž„çš„å·¦ä¸‹è§’ã€‚å›¾åƒä»Žä½Žè§’åº¦æ‹æ‘„ï¼Œçªæ˜¾å»ºç­‘çš„å®ä¼Ÿå’Œè§„æ¨¡ã€‚ç…§
ç‰‡ï¼Œé«˜å¯¹æ¯”åº¦ï¼Œæˆå‰§æ€§å…‰çº¿ï¼Œè“å¤©ï¼Œä½Žè§’åº¦è§†è§’ï¼Œå»ºç­‘æ‘„å½±ï¼Œèšç„¦æ¸…æ™°ï¼ŒçŽ°ä»£è®¾è®¡ï¼Œç²¾ç»†é’¢ç»“æž„ï¼Œ
é²œè‰³çº¢è‰²ç‚¹ç¼€ï¼Œè§†è§‰å†²å‡»åŠ›å¼ºï¼Œæž„å›¾å¹³è¡¡ã€‚ (Translation: A photo of the
Beijing National Stadium (Bird's Nest), with a blue sky background, the
appearance of the stadium is dominated by a complex interwoven steel
structure forming a mesh pattern. In the foreground, a person is wearing
casual clothing and walking slightly off center. The background shows
the red seating area inside the stadium through the steel structure.
"A30" is marked in red on the bottom left corner of the steel structure.
The image is taken from a low angle to highlight the grandeur and scale
of the building. Photos, high contrast, dramatic lighting, blue sky, low
angle perspective, architectural photography, clear focus, modern
design, fine steel structure, bright red accents, strong visual impact,
balanced composition.) A.2. Figure 2 Row #1 â€¢ Case #1:
æ‚å¿—å°é¢è®¾è®¡ã€‚æ–‡æ¡ˆï¼šå¤§æ ‡é¢˜ "ã€Œé€ ç›¸ã€ Z-Image" ã€‚å°æ ‡é¢˜ï¼š "Winter
Release. Spring for Gener- ative Art." ã€‚ç‰ˆæœ¬å·ï¼š " VOL 1.0"
ã€‚ä¸­é—´åº•éƒ¨æžå°å­— "é€šä¹‰å¤šæ¨¡æ€äº¤äº’å‡ºç‰ˆç¤¾"ã€‚æ‹‰å¼€ä¸€ç‰‡ç™½é›ªèŒ«
èŒ«ä¸‹çš„æ‹‰é“¾ï¼Œæ‹‰é“¾ä¸‹æ¼å‡ºç»¿è‰é²œèŠ±çš„æ˜¥å¤©ï¼Œç§»è½´å¾®è·ï¼Œæ‹‰é“¾æ˜¯ä¸€ä¸ªå†’ç€ç™½çƒŸè¿œåŽ»çš„ç«è½¦å¤´ï¼Œç²¾ç¾Žæž„
å›¾ï¼Œå¤¸å¼ çš„ä¿¯è§†è§†è§’ï¼Œè§†è§‰å†²å‡»åŠ›ï¼Œé«˜å¯¹æ¯”åº¦ï¼Œé«˜é¥±å’Œåº¦ã€‚ (Translation:
Magazine cover design. Copy: Headline " ã€Œé€ ç›¸ã€ Z-Image". Subtitle:
"Winter Release. Spring for Generative Art. Version number:"VOL 1.0".
The extremely small font at the bottom of the middle reads '
é€šä¹‰å¤šæ¨¡æ€äº¤äº’å‡ºç‰ˆç¤¾ '. Pulling open a zipper under a vast expanse of
white snow, the spring of green grass and flowers peeks out from under
the zipper. Moving the axis macro, the zipper is a locomotive emitting
white smoke far away, with exquisite composition, exaggerated top-down
perspective, visual impact, high contrast, and high saturation.) â€¢ Case
#2:
ä¸€å¹…åž‚ç›´æž„å›¾ã€é£Žæ ¼åŒ–çš„æ•°å­—æ’ç”»ï¼Œè®¾è®¡ä¸ºä¸€å¼ åŠ±å¿—æµ·æŠ¥ã€‚åœºæ™¯æç»˜äº†å¤œé—´çš„æ²™æ¼ æ™¯è§‚ï¼Œå¤´é¡¶æ˜¯å¹¿
é˜”æ— åž ã€ç¹æ˜Ÿå¯†å¸ƒçš„å¤©ç©ºï¼Œå…¶ä¸­é“¶æ²³æ¸…æ™°å¯è§ã€‚å‰æ™¯å’Œä¸­æ™¯ä»¥æ·±è“è‰²è¿‘ä¹Žé»‘è‰²çš„å‰ªå½±ä¸ºç‰¹è‰²ã€‚å·¦
ä¾§ï¼Œä¸€æ£µå·¨å¤§è€Œç»†èŠ‚ä¸°å¯Œçš„çº¦ä¹¦äºšæ ‘å‰ªå½±å æ®äº†ç”»é¢ä¸»å¯¼ã€‚æ›´è¿œå¤„å¯ä»¥çœ‹åˆ°ä¸¤æ£µè¾ƒå°çš„çº¦ä¹¦äºšæ ‘ã€‚
å³ä¾§ï¼Œä¸¤ä¸ªäººçš„å‰ªå½±ç«™åœ¨ä¸€ä¸ªå°å±±ä¸˜ä¸Šï¼Œä»°æœ›ç€å¤©ç©ºã€‚å¤©ç©ºä»Žåº•éƒ¨çš„æ·±æµ·å†›è“è¿‡æ¸¡åˆ°é¡¶éƒ¨çš„æµ…è“
è‰²ï¼Œå¸ƒæ»¡æ˜Ÿè¾°ï¼Œæ˜Žäº®çš„é“¶æ²³å¸¦ä»¥æŸ”å’Œçš„ç™½è‰²ã€ç´«è‰²å’Œè“è‰²è°ƒï¼Œä»Žå³ä¸Šè§’åˆ’è¿‡ã€‚å›¾åƒä¸Šè¦†ç›–æœ‰äº”å¤„ç‹¬
ç«‹çš„æ¸²æŸ“ä¸­æ–‡å­—æ ·ã€‚é¡¶éƒ¨æ˜¯ç™½è‰²å¤§å·å­—ä½“ï¼Œå†…å®¹ä¸º
"äºŽæ— åž é»‘æš—ä¸­ï¼Œå¯»è§ä½ çš„å¾®å…‰"ã€‚åœ¨ä¸­é—´é è¿‘äºº
ç‰©çš„ä½ç½®ï¼Œæœ‰è¾ƒå°çš„é»‘è‰²å­—ä½“å†™ç€
"å¿ƒä¹‹æ‰€å‘ï¼Œå®‡å®™å›žå“"ã€‚åœ¨æœ€åº•éƒ¨ï¼Œæ˜¯ç™½è‰²å¤§å·è‰ºæœ¯å­—çš„ä¸»æ ‡ é¢˜"ä»°æœ› Â·é€æ¢¦
"ï¼Œå…¶ä¸‹æ–¹æ˜¯ç¨å°çš„ç™½è‰²å­—ä½“
"å¿ƒçš„æ—…ç¨‹ç”±æ­¤å¼€å§‹"ã€‚åœ¨å±±ä¸˜ä¸Šé è¿‘äººç‰©çš„åœ°æ–¹ï¼Œæœ‰ä¸€
ä¸ªéžå¸¸å°ã€å‡ ä¹Žéšè—çš„é»‘è‰²ç­¾å
"è§‚æ˜Ÿè€…"ã€‚æ•´ä½“é£Žæ ¼å›¾å½¢åŒ–ä¸”ç®€çº¦ï¼Œå°†æ‰å¹³çš„å‰ªå½±ä¸Žç»†èŠ‚æ›´ä¸°å¯Œã€
å¯Œæœ‰ç»˜ç”»æ„Ÿçš„å¤©ç©ºç›¸ç»“åˆï¼Œè¥é€ å‡ºä¸€ç§æ·±æ²‰ã€å¼•äººæ·±æ€ä¸”å……æ»¡å¸Œæœ›çš„æ°›å›´ã€‚
(Translation: A vertically composed and stylized digital illustration
designed as an inspirational poster. The scene depicts a desert
landscape at night, with a vast and starry sky overhead, among which the
Milky Way is clearly visible. The foreground and middle ground are
characterized by 57

deep blue and almost black silhouettes. On the left, a large and
detailed silhouette of a Joshua tree dominates the scene. Two smaller
Joshua trees can be seen further away. On the right, silhouettes of two
people stand on a small hill, looking up at the sky. The sky transitions
from deep sea navy blue at the bottom to light blue at the top, filled
with stars, and the bright Milky Way streaks across in soft white,
purple, and blue tones from the top right corner. The image is covered
with five independent rendered Chinese characters. At the top is a large
white font that reads ' äºŽæ— åž é»‘æš— ä¸­ï¼Œå¯»è§ä½ çš„å¾®å…‰ '. In the middle,
near the character, there is a small black font that reads ' å¿ƒä¹‹
æ‰€å‘ï¼Œå®‡å®™å›žå“ '. At the bottom, there is the main title " ä»°æœ› Â·é€æ¢¦ "
in large white artistic font, and below it is a slightly smaller white
font " å¿ƒçš„æ—…ç¨‹ç”±æ­¤å¼€å§‹ ". Near the character on the hill, there is a
very small, almost hidden black signature called ' è§‚æ˜Ÿè€… '. The overall
style is graphical and minimalist, combining flat silhouettes with a
more detailed and picturesque sky, creating a deep, thought-provoking,
and hopeful atmosphere.) â€¢ Case #3:
ä¸€å¼ å……æ»¡æ´»åŠ›çš„è§†è§‰ä½œå“é›†å¹³é¢è®¾è®¡æµ·æŠ¥ï¼Œæ•´å¼ å›¾ç‰‡ä»¥éžå¸¸å°çš„é€æ˜Žæ£‹ç›˜æ ¼ä¸ºèƒŒæ™¯ï¼Œå±•ç¤ºäº†ä¸€
ä¸ª3Dæ¸²æŸ“çš„å¡é€šäººç‰©ã€‚ç”»é¢å·¦ä¾§æ˜¯ä¸€ä½å¹´è½»å¥³æ€§çš„åŠèº«åƒï¼Œå¥¹çš®è‚¤ç™½çš™ï¼Œç•™ç€æ·±æ£•è‰²é•¿å·å‘ï¼Œæˆ´
ç€ç²‰è‰²è¾¹æ¡†çš„çœ¼é•œï¼Œçœ¼é•œåŽæ˜¯æ£•è‰²çš„å¤§çœ¼ç›ã€‚å¥¹ç¬‘å®¹ç¿çƒ‚ï¼Œéœ²å‡ºç‰™é½¿ï¼Œæˆ´ç€å°å·§çš„é“¶è‰²è€³é’‰ã€‚å¥¹çš„
ç€è£…åŒ…æ‹¬ä¸€ä»¶æµ…ç°è‰²è¥¿è£…å¤–å¥—ã€ä¸€ä»¶ç™½è‰²ç¿»é¢†è¡¬è¡«å’Œä¸€æ¡çº¢è‰²é¢†å¸¦ã€‚å¥¹æ‰‹ä¸­æ§ç€ä¸€æŸç”±å››æœµé²œè‰³
çš„é»„è‰²å‘æ—¥è‘µç»„æˆçš„èŠ±æŸï¼ŒèŠ±èŒŽä¸ºç»¿è‰²ã€‚è¯¥è§’è‰²è¢«ä¸€åœˆç²—ç™½çš„è½®å»“çº¿åŒ…å›´ï¼Œä½¿å…¶ä»ŽèƒŒæ™¯ä¸­å‡¸æ˜¾å‡º
æ¥ã€‚æµ·æŠ¥çš„å³ä¾§ä¸»è¦æ˜¯å¤§åž‹è‰ºæœ¯å­—ã€‚ä¸»æ ‡é¢˜
"è§†è§‰ä½œå“é›†"é‡‡ç”¨ç²—å¤§çš„é»„è‰²ç¬”åˆ·é£Žæ ¼å­—ä½“ã€‚å…¶ä¸Šå 
åŠ ç€ä¸€è¡Œçº¤ç»†çš„çº¢è‰²è‰ä¹¦è‹±æ–‡ "Personalization"
ã€‚ä¸‹æ–¹æ˜¯åœ†æ¶¦æ°”æ³¡çŠ¶çš„é»„è‰²å°ä¸€å·å­—ä½“ "VISUAL PORTFOLIO"
ã€‚å…¶ä¸‹å†™å‡ºäº†ä¸‰ä¸ªäº®ç‚¹ï¼š "Â·ä¸­è‹±æ¸²æŸ“ï¼Œå­—å­—å¦‚åˆ»â— Bilingual Rendering" "Â·
ä¸æ­¢çœŸ å®žï¼Œæ›´æ‡‚ç¾Žå­¦â— Realism & Aesthetic" "Â· è¯»æ‡‚å¤æ‚ï¼Œç”Ÿæˆç²¾å¦™â—
Complexity & Elegance" è¿™é‡Œ
ä¸­æ–‡æ˜¯ç™½è‰²æ‰‹å†™ä½“å¤§å­—ï¼Œè‹±æ–‡æ˜¯åŠé€æ˜Žçš„å°åˆ·ä½“å°å­—ã€‚æµ·æŠ¥åŒ…å«å¤šä¸ªæ–‡æœ¬å—å’Œæ ‡å¿—ã€‚ä¸­ä¸Šéƒ¨å…ˆæ˜¯
é»„è‰²çš„æ–‡å­— "Z-Image x"
ï¼Œä¸­é—´æ˜¯ä¸€ä¸ªæˆ´ç€è€³æœºçš„å¡é€šå¤´åƒçš„é»„è‰²çº¿æ¡ç”»æ ‡å¿—ï¼ŒåŽé¢è·Ÿç€æ–‡å­— "x
Design"ã€‚åœ¨å³ä¸‹è§’æœ‰ä¸€ä¸ªå¯çˆ±çš„æ‹ŸäººåŒ–æ‰©éŸ³å™¨ï¼Œå®ƒæœ‰ä¸¤åªå¤§å¤§çš„çœ¼ç›ï¼Œé¢œè‰²ä¸ºæµ…ç»¿è‰²å’Œå¥¶æ²¹è‰²ï¼Œ
åº•éƒ¨æœ‰ä¸€æœµå°é›èŠã€‚æ•´ä½“é£Žæ ¼æ˜¯
3Dè§’è‰²æ¸²æŸ“å’Œå¹³é¢è®¾è®¡çš„ç»“åˆï¼Œç‰¹ç‚¹æ˜¯æ°›å›´æ„‰å¿«ã€å¯¹æ¯”åº¦é«˜ï¼Œå¹¶
é‡‡ç”¨äº†é»„ã€é»‘ã€ç™½ä¸ºä¸»çš„é…è‰²æ–¹æ¡ˆã€‚ (Translation: A vibrant visual
portfolio graphic design poster, with a very small transparent
checkerboard pattern as the background, showcasing a 3D rendered cartoon
character. On the left side of the screen is a half body portrait of a
young woman with fair skin, long curly dark brown hair, wearing pink
framed glasses, and brown big eyes behind the glasses. She had a bright
smile, revealing her teeth and wearing small silver earrings. Her attire
includes a light gray suit jacket, a white collared shirt, and a red
tie. She held a bouquet of four bright yellow sunflowers in her hand,
with green stems. The character is surrounded by a thick white outline,
making it stand out from the background. On the right side of the poster
are mainly large artistic characters. The main title "Visual Works
Collection" adopts a thick yellow brush style font. On top of it is a
thin line of red cursive English word "Personalization". Below is a
round, bubble shaped yellow font with one size smaller reading 'VISUAL
PORTFOLIO'. Below are three highlights: "Â·ä¸­è‹±æ¸²æŸ“ï¼Œå­—å­—å¦‚åˆ»â— Bilingual
Rendering" "Â· ä¸æ­¢çœŸå®žï¼Œæ›´æ‡‚ç¾Žå­¦â— Realism & Aesthetic" "Â·
è¯»æ‡‚å¤æ‚ï¼Œç”Ÿæˆç²¾å¦™â— Complexity & Elegance" The Chinese characters here
are white handwritten large characters, while the English characters are
semi transparent printed small characters. The poster contains multiple
text blocks and logos. The upper part is first marked with yellow text
"Z-Image x", followed by a yellow line drawn logo of a cartoon avatar
wearing headphones, and then the text "x Design". In the bottom right
corner, there is a cute anthropomorphic amplifier with two big eyes in
light green and cream colors, and a small daisy at the bottom. The
overall style is a combination of 3D character rendering and graphic
design, characterized by a pleasant atmosphere, high contrast, and
predominantly yellow, black, and white color schemes.) Row #2 â€¢ Case #1:
ä¸€å¼ è™šæž„çš„è‹±è¯­ç”µå½±ã€Šå›žå¿†ä¹‹å‘³ã€‹ï¼ˆ The Taste of Memory
ï¼‰çš„ç”µå½±æµ·æŠ¥ã€‚åœºæ™¯è®¾ç½®åœ¨ä¸€ä¸ªè´¨ æœ´çš„
19ä¸–çºªé£Žæ ¼åŽ¨æˆ¿é‡Œã€‚ç”»é¢ä¸­å¤®ï¼Œä¸€ä½çº¢æ£•è‰²å¤´å‘ã€ç•™ç€å°èƒ¡å­çš„ä¸­å¹´ç”·å­ï¼ˆæ¼”å‘˜é˜¿ç‘Ÿ
Â·å½­å“ˆ
åˆ©æ ¹é¥°ï¼‰ç«™åœ¨ä¸€å¼ æœ¨æ¡ŒåŽï¼Œä»–èº«ç©¿ç™½è‰²è¡¬è¡«ã€é»‘è‰²é©¬ç”²å’Œç±³è‰²å›´è£™ï¼Œæ­£çœ‹ç€ä¸€ä½å¥³å£«ï¼Œæ‰‹ä¸­æ‹¿
ç€ä¸€å¤§å—ç”Ÿçº¢è‚‰ï¼Œä¸‹æ–¹æ˜¯ä¸€ä¸ªæœ¨åˆ¶åˆ‡èœæ¿ã€‚åœ¨ä»–çš„å³è¾¹ï¼Œä¸€ä½æ¢³ç€é«˜é«»çš„é»‘å‘å¥³å­ï¼ˆæ¼”å‘˜åŸƒèŽ‰
è¯ºÂ·ä¸‡æ–¯é¥°ï¼‰å€šé åœ¨æ¡Œå­ä¸Šï¼Œæ¸©æŸ”åœ°å¯¹ä»–å¾®ç¬‘ã€‚å¥¹ç©¿ç€æµ…è‰²è¡¬è¡«å’Œä¸€æ¡ä¸Šç™½ä¸‹è“çš„é•¿è£™ã€‚æ¡Œä¸Šé™¤
58

äº†æ”¾æœ‰åˆ‡ç¢Žçš„è‘±å’Œå·å¿ƒèœä¸çš„åˆ‡èœæ¿å¤–ï¼Œè¿˜æœ‰ä¸€ä¸ªç™½è‰²é™¶ç“·ç›˜ã€æ–°é²œé¦™è‰ï¼Œå·¦ä¾§ä¸€ä¸ªæœ¨ç®±ä¸Šæ”¾
ç€ä¸€ä¸²æ·±è‰²è‘¡è„ã€‚èƒŒæ™¯æ˜¯ä¸€é¢ç²—ç³™çš„ç°ç™½è‰²æŠ¹ç°å¢™ï¼Œå¢™ä¸ŠæŒ‚ç€ä¸€å¹…é£Žæ™¯ç”»ã€‚æœ€å³è¾¹çš„ä¸€ä¸ªå°é¢ä¸Š
æ”¾ç€ä¸€ç›å¤å¤æ²¹ç¯ã€‚æµ·æŠ¥ä¸Šæœ‰å¤§é‡çš„æ–‡å­—ä¿¡æ¯ã€‚å·¦ä¸Šè§’æ˜¯ç™½è‰²çš„æ— è¡¬çº¿å­—ä½“
"ARTISAN FILMS PRESENTS" ï¼Œå…¶ä¸‹æ–¹æ˜¯ "ELEANOR VANCE" å’Œ"ACADEMY AWARDÂ®
WINNER" ã€‚å³ä¸Šè§’å†™ ç€"ARTHUR PENHALIGON" å’Œ"GOLDEN GLOBEÂ® AWARD WINNER"
ã€‚é¡¶éƒ¨ä¸­å¤®æ˜¯åœ£ä¸¹æ–¯ ç”µå½±èŠ‚çš„æ¡‚å† æ ‡å¿—ï¼Œä¸‹æ–¹å†™ç€ "SUNDANCE FILM FESTIVAL
GRAND JURY PRIZE 2024" ã€‚ä¸» æ ‡é¢˜ "THE TASTE OF MEMORY"
ä»¥ç™½è‰²çš„å¤§å·è¡¬çº¿å­—ä½“é†’ç›®åœ°æ˜¾ç¤ºåœ¨ä¸‹åŠéƒ¨åˆ†ã€‚æ ‡é¢˜ä¸‹æ–¹æ³¨ æ˜Žäº† "A FILM BY
Tongyi Interaction Lab" ã€‚åº•éƒ¨åŒºåŸŸç”¨ç™½è‰²å°å­—åˆ—å‡ºäº†å®Œæ•´çš„æ¼”èŒå‘˜åå•ï¼ŒåŒ…
æ‹¬"SCREENPLAY BY ANNA REID" ã€"CULINARY DIRECTION BY JAMES CARTER" ä»¥åŠ
Artisan Filmsã€Riverstone Pictures å’ŒHeritage
Mediaç­‰ä¼—å¤šå‡ºå“å…¬å¸æ ‡å¿—ã€‚æ•´ä½“é£Žæ ¼æ˜¯å†™å®žä¸»ä¹‰ï¼Œé‡‡ç”¨æ¸©
æš–æŸ”å’Œçš„ç¯å…‰æ–¹æ¡ˆï¼Œè¥é€ å‡ºä¸€ç§äº²å¯†çš„æ°›å›´ã€‚è‰²è°ƒä»¥æ£•è‰²ã€ç±³è‰²å’ŒæŸ”å’Œçš„ç»¿è‰²ç­‰å¤§åœ°è‰²ç³»ä¸ºä¸»ã€‚ä¸¤
ä½æ¼”å‘˜çš„èº«ä½“éƒ½åœ¨è…°éƒ¨è¢«æˆªæ–­ã€‚ (Translation: A movie poster for the
fictional English movie 'The Taste of Memory'. The scene is set in a
rustic 19th century style kitchen. In the center of the screen, a
middle-aged man with reddish brown hair and a small beard (played by
actor Arthur Penhaligan) stands behind a wooden table. He is wearing a
white shirt, black vest, and beige apron, looking at a woman holding a
large piece of raw red meat with a wooden cutting board below. On his
right, a black haired woman with a high bun (played by actress Eleanor
Vance) leaned against the table and smiled gently at him. She was
wearing a light colored shirt and a long skirt with white on top and
blue on the bottom. On the table, in addition to a chopping board with
chopped onions and shredded cabbage, there is also a white ceramic plate
and fresh herbs. On the left side, there is a wooden box with a string
of dark grapes. The background is a rough gray white plaster wall with a
landscape painting hanging on it. On the far right countertop is a
vintage oil lamp. There is a lot of textual information on the poster.
The white sans serif font "ARTISAN FILMS PRESS" is located in the upper
left corner, with "ELEANOR VANCE" and "ACADEMY AWARD" below it Â®
WINNER". In the upper right corner are written "ARTHUR PENHALIGON" and
"GOLDEN GLOBE" Â® AWARD WINNER". At the top center is the crown emblem of
Sundance Film Festival, with the words' SUNDANCE FILM FESTIVAL GRAND
JURY PRIZE 2024 'written below. The main title "THE TASTE OF Memory" is
prominently displayed in large white serif font in the lower half. The
title reads 'A FILM BY Tongyi Interaction Lab.'. The bottom area lists
the complete cast and crew list in small white font, including
"SCREENPLAY BY ANNA REID", "CULINARY Directing BY JAMES CARTER", as well
as many production company logos such as Artisan Films, Riverstone
Pictures, and Heritage Media. The overall style is realism, using warm
and soft lighting schemes to create an intimate atmosphere. The color
scheme is dominated by earthy tones such as brown, beige, and soft
green. The bodies of both actors were severed at the waist.) â€¢ Case #2:
ä¸€å¼ ç«–ç‰ˆæ—¥æœ¬è‰ºæœ¯å±•æµ·æŠ¥ï¼ŒèƒŒæ™¯ä¸ºæ·±è“è‰²ã€‚è®¾è®¡ä»¥é†’ç›®çš„é»„è‰²æ–‡å­—å’Œä¸ƒå¹…æ°´å½©ç”»æ‹¼è´´ä¸ºä¸»ã€‚é¡¶éƒ¨æ˜¯
æ—¥æ–‡å’Œè‹±æ–‡æ ‡é¢˜ã€‚æ—¥æ–‡éƒ¨åˆ†ä½¿ç”¨å¤§å·é»„è‰²å®‹ä½“é£Žæ ¼å­—ä½“ï¼Œå†…å®¹ä¸º "è°·å·æ­£å­£
-æ°´å½©ç”»ã®ä¸–ç•Œ -"ã€‚å…¶ ä¸‹æ–¹æ˜¯è¾ƒå°çš„é»„è‰²æ— è¡¬çº¿å­—ä½“ "-The world of
watercolor-" ã€‚ä¸»æ ‡é¢˜ "ä¸­å¤©å ‚"ä»¥éžå¸¸å¤§çš„é£Žæ ¼åŒ–é»„
è‰²å­—ä½“çªå‡ºæ˜¾ç¤ºã€‚å…¶ä¸‹æ˜¯è‹±æ–‡ç¿»è¯‘ "HEAVEN OF DREAM"
ï¼ŒåŒæ ·ä¸ºé»„è‰²æ— è¡¬çº¿å­—ä½“ã€‚å†ä¸‹ä¸€è¡Œæ˜¯ æ—¥æ–‡å‰¯æ ‡é¢˜
"æˆ‘ãŒå¿ƒã®æ¡‚æž—"ï¼Œå­—ä½“è¾ƒå¤§ï¼ŒåŽè·Ÿå…¶è‹±æ–‡ç¿»è¯‘ "GUILIN IN MY MIND"
ï¼Œå­—ä½“è¾ƒå°ã€‚
æµ·æŠ¥ä¸­å¤®æ˜¯ç”±ä¸ƒå¹…æç»˜æ¡‚æž—å–€æ–¯ç‰¹åœ°è²Œä¸åŒåœºæ™¯çš„æ°´å½©ç”»ç»„æˆçš„ç½‘æ ¼ã€‚è¿™äº›ç”»ä½œå±•ç¤ºäº†äº‘é›¾ç¼­ç»•çš„
ç¾¤å±±ã€èœ¿èœ’ç©¿è¿‡å±±è°·çš„æ²³æµã€å€’æ˜ åœ¨æ°´é¢ä¸Šçš„ç»šä¸½æ—¥è½ã€äººä»¬åœ¨èˆ¹ä¸Šæç€ç¯ç¬¼çš„å¤œæ™¯ä»¥åŠå…¶ä»–å¯Œæœ‰
æ°›å›´çš„é£Žæ™¯ã€‚æµ·æŠ¥åº•éƒ¨ä¸‰åˆ†ä¹‹ä¸€å¤„ç”¨è¾ƒå°çš„é»„è‰²æ–‡å­—åˆ—å‡ºäº†æ´»åŠ¨è¯¦æƒ…ï¼ŒåŒ…æ‹¬
"2025.11.11( å…­) 17(äº”) 9:00 20:00"
ï¼Œ"é˜¿é‡Œå·´å·´äº‘è°·å›­åŒº"ï¼Œ"(021)-34567890"
ã€‚æ•´ä½“é£Žæ ¼æ˜¯ä¼˜é›…çš„å¹³é¢è®¾è®¡ï¼Œé‡‡ç”¨äº†æ·±è“è‰² å’Œé»„è‰²çš„é«˜å¯¹æ¯”åº¦åŒè‰²è°ƒè‰²æ¿ã€‚
(Translation: A vertical Japanese art exhibition poster with a dark blue
background. The design mainly features eye-catching yellow text and
seven watercolor collages. At the top are Japanese and English titles.
The Japanese section uses large yellow Song style fonts and the content
is " è°·å·æ­£å­£ -æ°´å½©ç”»ã®ä¸–ç•Œ -". Below it is a smaller yellow sans serif
font that reads' The world of watercolor - '. The main title" ä¸­å¤©å ‚ "
is highlighted in a very large stylized yellow font. Below is the
English translation "HEAVEN OF DREAM", also in yellow sans serif font.
The next line is the Japanese subtitle ' æˆ‘ãŒå¿ƒã®æ¡‚æž— ', with a larger
font size, followed by its English translation 'GUILIN IN MY MIND', with
a smaller font size. In the center of the poster is a grid composed of
seven watercolor paintings depicting different scenes of Guilin's karst
landscape. These paintings showcase misty mountains, winding rivers
through valleys, stunning sunsets reflected on the water, night scenes
of 59

people carrying lanterns on boats, and other atmospheric landscapes. The
activity details are listed in small yellow text at the bottom third of
the poster, including "2025.11.11( å…­) 17(äº”) 9:00 20:00",
"é˜¿é‡Œå·´å·´äº‘è°·å›­åŒº", and "(021) -34567890". The overall style is an
elegant graphic design, featuring a high contrast dual tone palette of
dark blue and yellow.) â€¢ Case #3:
ä¸€å¼ æ–¹å½¢æž„å›¾çš„ç‰¹å†™ç…§ç‰‡ï¼Œä¸»ä½“æ˜¯ä¸€ç‰‡å·¨å¤§çš„ã€é²œç»¿è‰²çš„æ¤ç‰©å¶ç‰‡ï¼Œå¹¶å åŠ äº†æ–‡å­—ï¼Œä½¿å…¶å…·æœ‰æµ·æŠ¥
æˆ–æ‚å¿—å°é¢çš„å¤–è§‚ã€‚ä¸»è¦æ‹æ‘„å¯¹è±¡æ˜¯ä¸€ç‰‡åŽšå®žã€æœ‰èœ¡è´¨æ„Ÿçš„å¶å­ï¼Œä»Žå·¦ä¸‹è§’åˆ°å³ä¸Šè§’å‘ˆå¯¹è§’çº¿å¼¯æ›²
ç©¿è¿‡ç”»é¢ã€‚å…¶è¡¨é¢åå…‰æ€§å¾ˆå¼ºï¼Œæ•æ‰åˆ°ä¸€ä¸ªæ˜Žäº®çš„ç›´å°„å…‰æºï¼Œå½¢æˆäº†ä¸€é“çªå‡ºçš„é«˜å…‰ï¼Œäº®é¢ä¸‹æ˜¾éœ²
å‡ºå¹³è¡Œçš„ç²¾ç»†å¶è„‰ã€‚èƒŒæ™¯ç”±å…¶ä»–æ·±ç»¿è‰²çš„å¶å­ç»„æˆï¼Œè¿™äº›å¶å­è½»å¾®å¤±ç„¦ï¼Œè¥é€ å‡ºæµ…æ™¯æ·±æ•ˆæžœï¼Œçªå‡º
äº†å‰æ™¯çš„ä¸»å¶ç‰‡ã€‚æ•´ä½“é£Žæ ¼æ˜¯å†™å®žæ‘„å½±ï¼Œæ˜Žäº®çš„å¶ç‰‡ä¸Žé»‘æš—çš„é˜´å½±èƒŒæ™¯ä¹‹é—´å½¢æˆé«˜å¯¹æ¯”åº¦ã€‚å›¾åƒ
ä¸Šæœ‰å¤šå¤„æ¸²æŸ“æ–‡å­—ã€‚å·¦ä¸Šè§’æ˜¯ç™½è‰²çš„è¡¬çº¿å­—ä½“æ–‡å­— "PIXEL-PEEPERS GUILD
Presents" ã€‚å³ä¸Šè§’åŒ æ ·æ˜¯ç™½è‰²è¡¬çº¿å­—ä½“çš„æ–‡å­— "\[Instant Noodle\]
æ³¡é¢è°ƒæ–™åŒ…"ã€‚å·¦ä¾§åž‚ç›´æŽ’åˆ—ç€æ ‡é¢˜ "Render Distance:
Max"ï¼Œä¸ºç™½è‰²è¡¬çº¿å­—ä½“ã€‚å·¦ä¸‹è§’æ˜¯äº”ä¸ªç¡•å¤§çš„ç™½è‰²å®‹ä½“æ±‰å­— "æ˜¾å¡åœ¨
...ç‡ƒçƒ§"ã€‚å³ä¸‹è§’æ˜¯è¾ƒå°çš„ç™½ è‰²è¡¬çº¿å­—ä½“æ–‡å­—"Leica Glowâ„¢ Unobtanium
X-1"ï¼Œå…¶æ­£ä¸Šæ–¹æ˜¯ç”¨ç™½è‰²å®‹ä½“å­—ä¹¦å†™çš„åå­—"è”¡å‡ "ã€‚ (Translation: A close-up
photo with a square composition, featuring a large, bright green plant
leaf and overlaid with text to give it the appearance of a poster or
magazine cover. The main subject being photographed is a thick, waxy
leaf that curves diagonally through the frame from the bottom left
corner to the top right corner. Its surface has strong reflectivity,
capturing a bright direct light source and forming a prominent
highlight, revealing parallel fine leaf veins under the bright surface.
The background is composed of other dark green leaves that are slightly
out of focus, creating a shallow depth of field effect and highlighting
the main leaf of the foreground. The overall style is realistic
photography, with high contrast between bright leaves and dark shadow
backgrounds. There are multiple rendered texts on the image. In the
upper left corner is the white serif font text "PIXEL-PEEPERS GUIDE
Gifts". The text in white serif font in the upper right corner reads
'\[Instant Noodle\] æ³¡é¢è°ƒæ–™åŒ… '. The title "Render Distance: Max" is
vertically arranged on the left side in white serif font. In the bottom
left corner are five large white Song typeface Chinese characters that
read 'æ˜¾å¡åœ¨ ...ç‡ƒçƒ§ '. The smaller white serif font text "Leica Glow"
is located in the bottom right corner â„¢ Unobtanium X-1"ï¼ŒAbove it is the
name "è”¡å‡ " written in white Song typeface.) Row #3 â€¢ Case #1: A
vertical digital illustration depicting a serene and majestic Chinese
landscape, rendered in a style reminiscent of traditional Shanshui
painting but with a modern, clean aesthetic. The scene is dominated by
towering, steep cliffs in various shades of blue and teal, which frame a
central valley. In the distance, layers of mountains fade into a light
blue and white mist, creating a strong sense of atmospheric perspective
and depth. A calm, turquoise river flows through the center of the
composition, with a small, traditional Chinese boat, possibly a sampan,
navigating its waters. The boat has a bright yellow canopy and a red
hull, and it leaves a gentle wake behind it. It carries several
indistinct figures of people. Sparse vegetation, including green trees
and some bare-branched trees, clings to the rocky ledges and peaks. The
overall lighting is soft and diffused, casting a tranquil glow over the
entire scene. Centered in the image is overlaid text. At the top of the
text block is a small, red, circular seal-like logo containing stylized
characters. Below it, in a smaller, black, sans-serif font, are the
words 'Zao-Xiang \* East Beauty & West Fashion \* Z-Image'. Directly
beneath this, in a larger, elegant black serif font, is the word 'SHOW &
SHARE CREATIVITY WITH THE WORLD'. Among them, there are "SHOW & SHARE",
"CREATIVITY", and "WITH THE WORLD" â€¢ Case #2: vertical movie poster for
the film "Come Back Home Often." created by Master of Oil painting. The
artwork is a unified digital painting with a heavy impasto texture,
mimicking thick oil paint strokes applied with a palette knife. The
central focus is a massive, abstract figure rendered in thick, textured
white paint, resembling a giant bird or a stylized human form. This
white shape is set against a dark navy blue background that is densely
covered with small, stylized flowers painted in vibrant red and white,
with green stems. In the bottom right corner, two elderly people are
depicted from behind, walking away from the viewer. One person, slightly
ahead, wears a purple jacket and uses a wooden cane. The other, slightly
behind, wears a greyish-blue jacket. Their 60

bodies are truncated at the ankles by the bottom edge of the frame. The
overall style is surreal and symbolic, with a high-contrast color
palette dominated by deep navy, white, and red. Text control: all
lettering is fully integrated into the painted surface with identical
heavy impasto, each character exhibiting raised ridges and knife-scraped
edges that catch ambient light. In the top left corner, in white
sans-serif strokes sculpted with thick, palette-knife ridges, the words
"Z-Image" appear, and directly beneath them, still in raised impasto,
"Visionary Creator." In the bottom left, the Chinese title is rendered
in large, white, cursive calligraphy ( è‰ä¹¦ style) built up from
layered, knife-pressed paint: "å¸¸å›žå®¶çœ‹çœ‹", its down-strokes showing
visible paint peaks. Below this, in a smaller white serif font whose
letterforms are similarly embossed with raised impasto, reads the
English title: "Come Back Home Often." â€¢ Case #3:
ä¼ ç»Ÿä¸­å›½æ°´å¢¨ç”»ç…§ç‰‡ï¼Œæç»˜äº†è§ç‘Ÿç§‹æ—¥é»„æ˜æ™¯è±¡ï¼Œä½äºŽé¡µé¢å·¦ä¾§ã€‚ç”»ä½œç«–å‘æŽ’åˆ—ï¼Œç”¨æž¯ç¬”å‹¾å‹’ç›˜æ›²
è€è—¤ç¼ ç»•å¤æ ‘ï¼Œæµ“å¢¨ç‚¹æŸ“æ –æ¯æ˜é¸¦ï¼Œæ·¡å¢¨æ™•æŸ“æš®è‰²å¤©ç©ºã€‚æž¯è—¤å¦‚é¾™è›‡èˆ¬æ”€é™„åœ¨è™¬æ›²æ ‘å¹²ä¸Šï¼Œä¸‰ä¸¤åª
ä¹Œé¸¦åœé©»æžå¤´ï¼Œå‰ªå½±èˆ¬çš„è½®å»“ã€‚è¿œå¤„éšçº¦å¯è§å°æ¡¥æµæ°´å’Œå¤æœ´äººå®¶å±‹èˆï¼Œç‚ŠçƒŸè¢…è¢…ã€‚è¿‘æ™¯æœ‰ä¸€æ¡é»„
åœŸå¤é“ï¼Œç˜¦é©¬ä½Žå¤´ç¼“è¡Œã€‚ç”»é¢ä¸Šæ–¹æœ‰å‡ è¡Œé»‘è‰²è¡Œä¹¦ä¹¦æ³•ï¼Œé¢˜å†™å…ƒæ›²åå¥ï¼Œå³ä¸Šè§’é’¤ä¸€æžšæœ±çº¢æ–¹å°ã€‚
åœ¨ç”»ä½œå³ä¾§ï¼Œæœ‰ä¸¤åˆ—ç«–æŽ’ä¸­æ–‡æ–‡å­—ï¼Œçº¯ç™½è‰²èƒŒæ™¯ï¼Œè‰ä¹¦å­—ä½“ï¼Œç¬¬ä¸€åˆ—å†™ç€ "å¤©å‡€æ²™
Â·ç§‹æ€"ï¼Œç¬¬äºŒåˆ— å†™ç€
"æž¯è—¤è€æ ‘æ˜é¸¦ï¼Œå°æ¡¥æµæ°´äººå®¶"ã€‚ä¼ ç»Ÿä¸­å¼æ–‡äººç”»é£Žæ ¼ï¼Œæ°´å¢¨å•è‰²ç³»å¸¦é£žç™½æž¯ç¬”æ•ˆæžœï¼Œç¬”è§¦
è‹åŠ²æœ‰åŠ›ï¼Œæž„å›¾ç–å¯†æœ‰è‡´ï¼Œç•™ç™½å¤„ç†çªå‡ºï¼Œå……æ»¡è§ç‘Ÿè‹å‡‰çš„ç¾Žå­¦æ„è•´ï¼Œæ°›å›´å­¤å¯‚è€Œæ‚ è¿œï¼Œå…·æœ‰æµ“åŽš
çš„å¤å…¸è¯—æ„å’Œæ–‡åŒ–éŸµå‘³ã€‚ (Translation: The traditional Chinese ink
painting photo depicts a bleak autumn dusk scene, located on the left
side of the page. The painting is arranged vertically, using a dry brush
to outline the winding old vines around the ancient trees, with thick
ink coloring the roosting crows, and light ink blending the twilight
sky. The withered vine clung to the winding tree trunk like a dragon or
snake, with three or two crows perched on the branches, forming a
silhouette like silhouette. In the distance, small bridges, flowing
water, and quaint houses can be faintly seen, with smoke rising from
cooking. In the close-up, there is a loess ancient road, and thin horses
are walking slowly with their heads down. There are several lines of
black cursive calligraphy above the screen, inscribed with famous lines
of Yuan opera, and a vermilion square seal is stamped in the upper right
corner. On the right side of the painting, there are two vertical
columns of Chinese characters with a pure white background and cursive
font. The first column reads " å¤©å‡€æ²™ Â·ç§‹æ€ ", and the second column
reads"æž¯è—¤è€æ ‘æ˜é¸¦ï¼Œå°æ¡¥æµæ°´äººå®¶ ". The traditional Chinese literati
painting style features a single color ink wash with a flying white and
withered pen effect. The brushstrokes are vigorous and powerful, the
composition is dense and orderly, and the white space treatment is
prominent. It is full of desolate and desolate aesthetic connotations,
creating a lonely and distant atmosphere with strong classical poetry
and cultural charm.) Row #4 â€¢ Case #1:
ç«–æŽ’è¡Œä¹¦ä¹¦æ³•ä½œå“ç‰¹å†™ï¼Œä»¥ç±³ç™½è‰²å¸¦æµ…æ·¡è‚Œç†çš„åŠç”Ÿç†Ÿå®£çº¸ä¸ºè½½ä½“ï¼Œçº¸å¼ å¸¦æœ‰è‡ªç„¶è½»å¾®è¤¶çš±ï¼Œè´¨æ„Ÿ
æ¸©æ¶¦æŸ”å’Œï¼›é»‘è‰²å¢¨æ±ä¹¦å†™çš„è¡Œä¹¦å­—ä½“ï¼Œç¬”é”‹ç²—ç»†å˜åŒ–çµåŠ¨ï¼Œå¦‚
"äºº"å­—æºç¬”èˆ’å±•åŠ²æŒºã€ "å¿—"å­—è¿žç¬”å©‰
è½¬æµç•…ï¼Œå¢¨è‰²å±‚æ¬¡ä¸°å¯Œï¼Œéƒ¨åˆ†ç¬”ç”»å¸¦è‡ªç„¶é£žç™½æ•ˆæžœï¼Œå°½æ˜¾è‹åŠ²æ´’è„±çš„ä¹¦å†™å¼ åŠ›ï¼›æ–‡å­—æŒ‰ä¼ ç»Ÿä»Žå³è‡³
å·¦ç«–åˆ—æŽ’å¸ƒï¼Œå¯è§ "äººæ‰¶æˆ‘é’å¿—""æˆ‘è‡ªè¸é›ªè‡³å±±å·…"ç­‰è¯å¥ï¼Œå·¦ä¾§é…æœ‰
"é€ ç›¸å¤§å¸ˆ"è½æ¬¾å°å­—ï¼Œçº¸é¢ç‚¹
ç¼€å¤šæžšæœ±çº¢æ–¹å½¢ç¯†åˆ»å°ç« ï¼Œå°æ³¥è‰²æ³½é¥±æ»¡ã€å°æ–‡çº¿æ¡æ¸…æ™°ï¼›å¤šå¼ ä¹¦æ³•çº¸å‘ˆè½»å¾®é‡å çš„é”™è½æ‘†æ”¾ï¼ŒèƒŒ
æ™¯éšçº¦éœ²å‡ºå…¶ä»–çº¸å¼ çš„æ·¡è‰²å­—è¿¹ï¼Œè¥é€ å‡ºéšæ€§çš„åˆ›ä½œæ°›å›´ï¼›å…‰çº¿ä¸ºæŸ”å’Œè‡ªç„¶å…‰ï¼Œå‡åŒ€é“ºæ´’åœ¨çº¸é¢ï¼Œ
å‡¸æ˜¾å¢¨è‰²çš„å…‰æ³½ä¸Žçº¸å¼ çš„çº¹ç†è¤¶çš±ï¼Œé€ æ¢¦å¸ˆçš„è¯—æ„è§†è§‰é£Žæ ¼ï¼Œæ•´ä½“æ°›å›´é›…è‡´å¤æœ´ï¼Œå…¼å…·æ‰‹å†™ä¹¦æ³•çš„
çµåŠ¨éšæ€§ä¸Žä¼ ç»Ÿæ–‡æˆ¿çš„æ²‰é™è´¨æ„Ÿã€‚ (Translation: Close up of vertical
cursive calligraphy works, using semi ripe rice paper with a light
texture and off white color as the carrier. The paper has natural slight
wrinkles and a warm and soft texture; The running script font written in
black ink has dynamic changes in stroke thickness, such as the
stretching and vigorous strokes of the " äºº" character and the smooth
and graceful strokes of the " å¿—" character. The ink layers are rich,
and some strokes have a natural flying white effect, showcasing the
vigorous and free spirited writing tension; The text is arranged
vertically from right to left according to tradition, with phrases such
as " äººæ‰¶æˆ‘é’å¿— " and "æˆ‘è‡ª è¸é›ªè‡³å±±å·…" visible. On the left side, there
is a small signature of " é€ ç›¸å¤§å¸ˆ ", and the paper is decorated with
multiple vermilion square seal seals. The ink color is full and the
lines of the seal 61

are clear; Multiple calligraphy papers are arranged in a slightly
overlapping and staggered manner, with the background faintly revealing
the light colored handwriting of other papers, creating a casual
creative atmosphere; The light is soft natural light, evenly spread on
the paper surface, highlighting the luster of ink color and the texture
wrinkles of the paper. The poetic visual style of the dream maker
creates an elegant and rustic atmosphere, combining the agility and
casualness of handwritten calligraphy with the calm texture of
traditional study rooms.) â€¢ Case #2:
ä¸€å¼ åž‚ç›´æž„å›¾çš„å¹³é¢è®¾è®¡æµ·æŠ¥ï¼ŒèƒŒæ™¯æ˜¯çº¯ç²¹è€Œé²œè‰³çš„å®è“è‰²ã€‚é¡¶éƒ¨çš„å·¨å¤§æ— è¡¬çº¿å­—ä½“ä¸»æ ‡é¢˜ï¼Œä¸Š
åŠéƒ¨åˆ†ä¸ºæµ…ç°è‰²çš„ "Sofa Montain Slummerfest" ï¼Œä¸‹åŠéƒ¨åˆ†ä¸ºç™½è‰²çš„ "Annual
Napping Festival 2025"ã€‚å…¶ä¸‹æ–¹æ˜¯å·¨å¤§çš„é»‘è‰²ä¹¦æ³•å­—ä½“ä¸­æ–‡æ ‡é¢˜
"æ²™å‘å±±æ‰“å‘¼èŠ‚"ã€‚æµ·æŠ¥çš„ä¸‹åŠéƒ¨åˆ†ç”±ä¸€å¹…å·¨å¤§çš„ã€
æ’ç”»é£Žæ ¼çš„è€è™Žæ’ç”»å æ®ï¼Œå®ƒæ­£è¶´ç€é¢å‘è§‚ä¼—ï¼Œçœ¼ç›æ˜¯é»„è‰²çš„ã€‚å…¶çš®æ¯›ç”±æ©™ã€é»‘ã€ç™½ä¸‰è‰²æž„æˆã€‚ä¸€
ä¸ªåŒ…å«çº¢è‰²çˆ±å¿ƒçš„æ€æƒ³æ³¡æ³¡æ¼‚æµ®åœ¨å®ƒçš„å¤´é¡¶ã€‚æµ·æŠ¥ä¸Šå¸ƒæ»¡äº†è¯¦å°½çš„æ´»åŠ¨æ–‡å­—ã€‚å·¦æ ç”¨ç™½è‰²å­—ä½“åˆ—å‡º
äº†ä»¥çŒ«ä¸ºä¸»é¢˜çš„ä¹é˜Ÿåï¼Œå¦‚ "The Fluffy Paws Grumbers ( æ¯›çˆªå’•å™œ )"ã€"DJ
Meow Mix" ã€"ä¹å‘½æ€ª çŒ«(Nine Lives)" ã€"æ¿€å…‰ç¬”è¿½é€è€… (The Laser Dots)"
ã€"çº¸ç®±çˆ±å¥½è€… (Cardbock Box Lovers)" ã€"å‘¼ å™œç¥žæ•™ (The
Purr-fectionists)" ã€"çŒ«è‰æˆç˜¾è€… (The Catnip Junkies)" ã€"DJ Chairman
Meow ( çŒ«ä¸» å¸­)"ä»¥åŠåƒ "Varh Radator Fesidenl Paw-Five"
è¿™æ ·çš„æ— æ„ä¹‰çŸ­è¯­ã€‚å³æ åˆ—å‡ºäº†æ´»åŠ¨ç»†èŠ‚ï¼Œå…¶ä¸­è®¸å¤š
éƒ½å¸¦æœ‰æ»‘ç¨½çš„æ‹¼å†™é”™è¯¯æˆ–æ— æ„ä¹‰å†…å®¹ï¼ŒåŒ…æ‹¬æ—¥æœŸ "4/1 MONDAY SUNL SUNSET"
ã€åœ°ç‚¹ "ä¸Šæµ·å¸‚æµ¦ ä¸œæ–°åŒºçŒ«æŠ“æ¿è·¯ 1å·é¡¶æ¥¼é˜³å°"ä»¥åŠç¥¨åŠ¡ä¿¡æ¯å¦‚ "ADV . 1 CAN
OF TUNA, DOOR 2 CANS, KITTENS
FREE!"ã€‚åœ¨æœ€åº•éƒ¨æ˜¯ä¸€æŽ’è™šæž„çš„èµžåŠ©å•†æ ‡å¿—ï¼Œåç§°åŒ…æ‹¬ "Catberd"
ã€"å¥½ä¸»äººç½ç½æœ‰é™å…¬å¸ (Good Oinar Canned Food Ltd)"å’Œ"iNONEPAWS"ã€‚
(Translation: A vertically composed graphic design poster with a pure
and vibrant navy blue background. The main title features a large sans
serif font at the top, with the upper half in light gray reading "Sofa
Montain Slummerfest" and the lower half in white reading "Annual Napping
Festival 2025". Below it is a huge black calligraphy font with the
Chinese title " æ²™å‘å±±æ‰“å‘¼èŠ‚ ". The lower part of the poster is occupied
by a huge, illustrated tiger illustration, which is lying face down to
the audience with yellow eyes. Its fur is composed of three colors:
orange, black, and white. A thought bubble containing a red heart floats
above its head. The poster is filled with detailed activity text. The
left column lists the names of cat themed bands in white font, such
as"The Fluffy Paws Grumbers ( æ¯›çˆªå’•å™œ )","DJ Meow Mix", " ä¹å‘½æ€ªçŒ«
(Nine Lives)","æ¿€å…‰ç¬”è¿½é€è€… (The Laser Dots)","çº¸ç®±çˆ±å¥½è€… (Cardbock Box
Lovers)"," å‘¼å™œç¥žæ•™ (The Purr-fectionists)"," çŒ«è‰æˆç˜¾è€… (The Catnip
Junkies)","DJ Chairman Meow ( çŒ«ä¸»å¸­ )" and meaningless phrases like
"Varh Radator Feseidel Paw Five. The right column lists the details of
the event, many of which have humorous spelling errors or meaningless
content, including the date"4/1 MONDAY SUNL SUNSET", location "
ä¸Šæµ·å¸‚æµ¦ä¸œ æ–°åŒºçŒ«æŠ“æ¿è·¯ 1å·é¡¶æ¥¼é˜³å° ", and ticketing information such
as"ADV . 1 CAN OF TUNA, DOOR 2 CANS, KITTENS FREE. At the bottom is a
row of fictional sponsor logos, with names including "Cattered,"
"å¥½ä¸»äººç½ç½æœ‰é™å…¬å¸" and "iNONEPAWS".) â€¢ Case #3:
ä¸€å¼ æ¤ç‰©å±•è§ˆçš„å¹³é¢è®¾è®¡æµ·æŠ¥ï¼ŒèƒŒæ™¯ä¸ºç´ å‡€çš„ç±³ç™½è‰²ã€‚æµ·æŠ¥ä¸Šæœ‰å¤šå¹…æ°´å½©æ’ç”»ï¼Œæç»˜äº†å„ç§è‹”è—“å’Œ
è•¨ç±»æ¤ç‰©ï¼Œç”¨è‰²ä»¥ç»¿è‰²ã€æ£•è‰²å’Œé»„è‰²ä¸ºä¸»ï¼Œå¹¶é…æœ‰ç²¾è‡´çš„é»‘è‰²å¢¨æ°´è½®å»“çº¿ã€‚ç”»é¢ä¸­å¤®æ˜¯ä¸€å¹…å·¨å¤§è€Œ
ç²¾ç»†çš„ç»¿è‰²åœ°é’±æ’ç”»ï¼Œä¸Šé¢æœ‰æ£•è‰²çš„å­¢å­ä½“ã€‚å…¶ä»–è¾ƒå°çš„æ’å›¾æ•£å¸ƒåœ¨å‘¨å›´ï¼ŒåŒ…æ‹¬å·¦ä¸Šè§’çš„ç»†å¶æ»¡æ±Ÿ
çº¢ï¼ˆ Fissidens ï¼‰ï¼Œé¡¶éƒ¨ä¸­å¤®çš„å·¨å¶çº¢èŒŽè—“ï¼ˆ Rhodobryum giganteum
ï¼‰ï¼Œå³ä¸Šè§’çš„é’è‹”å±žï¼ˆ Bryum sp.ï¼‰ï¼Œä»¥åŠå³ä¸‹è§’çš„å‡¤å°¾è—“ï¼ˆ Marchantia
formosana ï¼‰ã€‚æµ·æŠ¥å¸ƒå±€å¹²å‡€ã€ç®€çº¦ï¼Œæ–‡å­—åž‚ç›´å’Œæ°´å¹³
æŽ’åˆ—ã€‚å³ä¸Šè§’æ˜¯çºµå‘æŽ’åˆ—çš„é»‘è‰²å®‹ä½“å¤§å­—
"è‹”ç—•"ï¼Œä¸‹è¾¹æ˜¯æ¨ªå‘æŽ’åˆ—çš„æ— è¡¬çº¿å­—ä½“è‹±æ–‡ "Moss Exhibi-
tion"ã€‚å·¦ä¾§æ˜¯é»‘è‰²æ— è¡¬çº¿å­—ä½“"Elkhorn Fern LifeStyle"ã€‚å·¦ä¸‹è§’å†™ç€"Alishan
Moss Ecological"ã€‚ æ—¥æœŸå’Œæ—¶é—´åœ¨åº•éƒ¨çªå‡ºæ˜¾ç¤ºï¼š
"2001"ä¸ºé»‘è‰²å¤§å·è¡¬çº¿å­—ä½“ï¼Œå…¶åŽæ˜¯è¾ƒå°çš„æ— è¡¬çº¿å­—ä½“ "04.22 \[Apr.\] am.
09:00"å’Œ"05.22 \[May\] pm. 17:00"
ã€‚æ¯å¹…æ¤ç‰©æ’å›¾éƒ½é™„æœ‰å…¶å­¦åï¼Œä½¿ç”¨å°å·ç°è‰²æ— è¡¬çº¿å­—ä½“ ä¹¦å†™ï¼Œä¾‹å¦‚
"Fissidens" ã€"Rhodobryum giganteum" ã€"Bryum sp." ã€"Bartramiaceae"
ã€"Alishan Moss Ecological" ã€"Marchantia formosana" å’Œ"Astrocella
yoshinagana" ã€‚æ•´ä½“é£Žæ ¼ä¼˜é›…ä¸”å…·æœ‰æ•™ è‚²æ„ä¹‰ï¼Œå°†ç§‘å­¦æ’ç”»ä¸ŽçŽ°ä»£æŽ’ç‰ˆç›¸ç»“åˆã€‚
(Translation: A graphic design poster for a plant exhibition, with a
plain beige background. The poster features multiple watercolor
illustrations depicting various mosses and ferns, primarily using green,
brown, and yellow colors, with delicate black ink outlines. In the
center is a large and intricate illustration of green liverwort with
brown sporophytes on top. Other smaller illustrations are scattered
around, including Fissidens in the upper left corner, Rhodobryum
giganteum in the top center, Bryum sp. in the upper right corner, and
Marchantia formosana in the lower right corner. The poster layout is
clean and minimalist, with text arranged both vertically and
horizontally. In 62

the upper right corner is the vertically arranged black Song-style
characters " è‹”ç—• ", below which is the horizontally arranged sans-serif
English text"Moss Exhibition". On the left side is the black sans-serif
text "Elkhorn Fern LifeStyle". The lower left corner reads "Alishan Moss
Ecological". The date and time are prominently displayed at the bottom:
"2001" in large black serif font, followed by smaller sans-serif text
"04.22 \[Apr.\] am. 09:00" and "05.22 \[May\] pm. 17:00". Each plant
illustration is accompanied by its scientific name, written in small
gray sans-serif font, such as "Fissidens", "Rhodobryum giganteum",
"Bryum sp.", "Bartramiaceae", "Alishan Moss Ecological", "Marchantia
formosana", and "Astrocella yoshinagana". The overall style is elegant
and educational, combining scientific illustration with modern
typography.) A.3. Figure 3 Row #1 â€¢ Case #1:
å¤´å‘å˜æˆæ·¡ç´«è‰²å·å‘ï¼Œå‘ä¸è¾¹ç¼˜æ•£å‘å‡ºé‡‘å…‰ã€‚ (Translation: The hair becomes
light purple curls, with golden light emanating from the edges of the
hair strands.) â€¢ Case #2:
è®©è¿™ä¸ªå¥³æ€§ç›˜è…¿ååœ¨é…’åº—æˆ¿é—´çš„åœ°æ¯¯ä¸Šï¼Œé¢å¸¦å¾®ç¬‘ï¼Œçœ¼ç¥žæ¸©æŸ”åœ°æ³¨è§†ç€é•œå¤´ï¼ŒèƒŒæ™¯æ˜¯æ•´æ´çš„ç™½è‰²å¤§
åºŠå’Œæ¸©é¦¨çš„åºŠå¤´ç¯ï¼Œæ•´ä½“æ°›å›´å®é™è€Œä¼˜é›…ã€‚ (Translation: Have this woman sit
cross-legged on the hotel room carpet, with a smile on her face and a
gentle gaze looking at the camera. The background features a neat white
bed and warm bedside lamp, with an overall atmosphere that is tranquil
and elegant.) â€¢ Case #3: å˜æˆæ°´å½©é£Žæ ¼ã€‚ (Translation: Transform into
watercolor style.) â€¢ Case #4:
æŠŠè€é¹°å˜æˆçŽ»ç’ƒæè´¨ï¼ŒåŒæ—¶æŠŠå¤©ç©ºå˜æˆæ©™è‰²çš„å¤•é˜³ã€‚ (Translation: Transform
the eagle into glass material, while changing the sky into an orange
sunset.) Row #2 â€¢ Case #1:
å‚è€ƒçŒ«çš„å½¢è±¡ï¼Œç”Ÿæˆä¸‰å®«æ ¼æ•…äº‹ï¼Œç”»é¢åŒ…æ‹¬æµ·æ»©ã€è¿œå±±ã€è½æ—¥ã€å¤•é˜³ï¼Œä¸‰ä¸ªå®«æ ¼ä»Žä¸Šåˆ°ä¸‹ã€‚å®«
æ ¼1ï¼ˆé¡¶éƒ¨ï¼‰ :çŒ«èƒŒå¯¹ç€é•œå¤´ï¼Œååœ¨æ²™æ»©ä¸Šï¼Œå‡æœ›ç€è¿œæ–¹çš„å¤•é˜³å’Œæµ·é¢ï¼Œå­—å¹•
"å±±çš„é‚£è¾¹æ˜¯ä»€ä¹ˆ"ï¼›å®« æ ¼2ï¼ˆä¸­éƒ¨ï¼‰ï¼šçŒ«å’ªè½¬è¿‡èº«æ¥ï¼Œä¾§è„¸å¯¹ç€é•œå¤´ï¼Œå­—å¹•
"ä½ ä¸å¿…è¯´"ï¼›å®«æ ¼ 3ï¼ˆåº•éƒ¨ï¼‰ï¼šçŒ«è„¸ç‰¹å†™ï¼Œæ­£è„¸
ç›´è§†é•œå¤´ï¼Œå­—å¹•"æˆ‘æ ¹æœ¬ä¸æƒ³çŸ¥é“"ã€‚ (Translation: Referencing the cat's
image, generate a three-panel story. The scenes include a beach, distant
mountains, sunset, and evening glow, with three panels arranged from top
to bottom. Panel 1 (top): The cat sits on the beach with its back to the
camera, gazing at the distant sunset and sea, with subtitle "
å±±çš„é‚£è¾¹æ˜¯ä»€ä¹ˆ "; Panel 2 (middle): The cat turns around, showing its
profile to the camera, with subtitle" ä½ ä¸å¿…è¯´ "; Panel 3 (bottom):
Close-up of the cat's face, looking directly at the camera, with
subtitle"æˆ‘æ ¹æœ¬ä¸æƒ³çŸ¥é“".) â€¢ Case #2:
åˆ¶ä½œæµ·æŠ¥ï¼ŒèƒŒæ™¯å˜æˆå…¬è·¯å’Œè“å¤©ç™½äº‘ï¼Œä¸¤ä¾§æ˜¯å¼€é˜”çš„ç”°é‡Žã€‚è‡ªè¡Œè½¦ä¿æŒå§¿æ€ä¸å˜ï¼Œæ”¾åœ¨å…¬è·¯ä¸­å¤®ã€‚
æµ·æŠ¥ä¸Šæ–¹æ˜¯ä¸»æ ‡é¢˜
"çŽ¯çƒéª‘è¡Œæ´¾å¯¹"ï¼Œä½¿ç”¨ç²—çŠ·ã€åŠ¨æ„Ÿçš„é»‘è‰²å­—ä½“ï¼Œå…¶æ­£ä¸‹æ–¹æ˜¯å‰¯æ ‡é¢˜ "From 2.30 to
2.31"ã€‚ (Translation: Create a poster with the background transformed
into a highway with blue sky and white clouds, with open fields on both
sides. The bicycle remains in its original posture, placed in the center
of the road. At the top of the poster is the main title " çŽ¯çƒéª‘è¡Œæ´¾å¯¹ "
in bold, dynamic black font, with the subtitle "From 2.30 to 2.31"
directly below it.) â€¢ Case #3: æŠŠçŒ«æ¢æˆä¸€åªæœ‰ç€ç›¸åŒå§¿åŠ¿çš„å“ˆå£«å¥‡ï¼Œæ–‡å­—
"Love Cat"ä¿®æ”¹ä¸º "Love Dog" ï¼Œä»¥åŠæ–‡å­— "å–µå–µ"æ”¹
ä¸º"æ±ªæ±ª"ã€‚åŒæ—¶åœ¨å·¦ä¸‹è§’çš„å® ç‰©é¤ç›˜é‡ŒåŠ æ»¡ç‹—ç²®ã€‚ 63

(Translation: Replace the cat with a husky in the same pose, change the
text "Love Cat" to "Love Dog", and change the text " å–µå–µ " to "æ±ªæ±ª".
Additionally, fill the pet food bowl in the lower left corner with dog
food.) Row #3 â€¢ Case #1:
è®©çŒ«å’Œç‹—åˆ†åˆ«ç©¿ä¸Šç²‰è‰²å’Œå’Œç»¿è‰²çš„æ¯›è¡£ï¼ŒèƒŒæ™¯æ”¹ä¸ºæµ·è¾¹çš„æˆ·å¤–å’–å•¡åº—ï¼ŒçŒ«å’Œç‹—ååœ¨æ¡Œå­æ—çš„æ¤…å­ä¸Š
å–å’–å•¡ã€‚ (Translation: Have the cat and dog wear pink and green sweaters
respectively, change the back- ground to a seaside outdoor cafÃ©, with
the cat and dog sitting on chairs at the table drinking coffee.) â€¢ Case
#2:
æŠŠè¿™å¼ å›¾å˜æˆä¸€å¹…ç”»ï¼Œé‡‘è‰²è¾¹æ¡†ï¼ŒæŒ‚åœ¨ç”»å±•çš„å¢™ä¸Šï¼Œæ—è¾¹æœ‰ä¸€äº›äººåœ¨æ¬£èµè¿™å¹…ç”»ã€‚
(Translation: Transform this image into a painting with a golden frame,
hanging on the wall of an art exhibition, with some people standing
beside it appreciating the painting.) Row #4 â€¢ Case #1:
è®©ä¸€ä¸ªä¸œæ–¹ç¾Žå¥³ç©¿ä¸Šè¿™ä¸ªè¿žè¡£è£™å’Œé»‘è‰²çš„éž‹ï¼Œå¹¶æˆ´ä¸Šè¿™é¡¶å¸½å­ã€‚ç¾Žå¥³ç«™åœ¨å…¬å›­è‰åœ°ä¸Šï¼ŒèƒŒæ™¯æœ‰å‡ é¢—
æ¡ƒæ ‘ã€‚ (Translation: Have an East Asian woman wear this dress and black
shoes, and put on this hat. The woman stands on the park lawn, with
several peach trees in the background.) Row #5 â€¢ Case #1:
å‚è€ƒå›¾åƒï¼Œç”Ÿæˆä¸€ä¸ªç‹ç‹¸çŽ©å¶çš„å½©è‰²å›¾åƒï¼ŒçŽ©å¶æ”¾åœ¨å…¬å›­è‰åœ°ä¸Šï¼ŒèƒŒæ™¯æœ‰ä¸€äº›æ ‘æœ¨ã€‚
(Translation: Referencing the image, generate a color image of a fox
plush toy, with the toy placed on the park lawn and some trees in the
background.) â€¢ Case #2: æ—‹è½¬çŽ©å¶ï¼Œå±•çŽ°ä»–çš„ä¾§é¢ã€‚ (Translation: Rotate
the toy to show its side profile.) â€¢ Case #3:
ç”Ÿæˆä¸¤ä¸ªè¿™ä¸ªç‹ç‹¸çŽ©å¶åœ¨è¶…å¸‚è´­ç‰©çš„ç”»é¢ï¼Œåƒäººä¸€æ ·æŽ¨ç€è´­ç‰©è½¦è´­ç‰©ï¼Œè´­ç‰©è½¦é‡Œæ”¾æ»¡äº†æ°´æžœã€‚
(Translation: Generate an image of two of these fox plush toys shopping
in a supermarket, pushing shopping carts like humans, with the carts
filled with fruits.) 64
